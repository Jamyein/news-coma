# å®Œæ•´LLMæä¾›å•†æ”¯æŒå®æ–½è®¡åˆ’

## ğŸ¯ ç›®æ ‡
å®ç°æ”¯æŒ14å®¶å›½å†…å¤–LLMæä¾›å•†çš„ç®€åŒ–é…ç½®æ–¹æ¡ˆï¼Œé€šè¿‡`ai_provider`å­—æ®µä¸€é”®åˆ‡æ¢ï¼Œå¹¶ä¿ç•™è‡ªåŠ¨å›é€€åŠŸèƒ½ã€‚

## ğŸ“‹ æœ€ç»ˆé…ç½®ç»“æ„

```yaml
# config.yaml ç®€åŒ–ç‰ˆç»“æ„
ai_provider: "deepseek"  # åªéœ€æ”¹è¿™ä¸€è¡Œåˆ‡æ¢æä¾›å•†

ai_providers:
  deepseek: { ... }      # 14å®¶æä¾›å•†é…ç½®
  zhipu: { ... }
  kimi: { ... }
  gemini: { ... }
  openai: { ... }
  # ... å…¶ä»–9å®¶

fallback:                # è‡ªåŠ¨å›é€€é…ç½®
  enabled: true
  fallback_chain:
    - "deepseek"
    - "zhipu"
    - "gemini"

scoring_criteria: { ... }  # å…¨å±€å…±äº«
```

---

## ğŸ“ éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒä»£ç æ–‡ä»¶ï¼ˆ3ä¸ªï¼‰
1. `src/models.py` - æ·»åŠ AIProviderConfigå’ŒFallbackConfig
2. `src/config.py` - æ”¯æŒåŠ¨æ€ai_providerè¯»å–
3. `src/ai_scorer.py` - å®ç°å¤šæä¾›å•†åˆ‡æ¢å’Œå›é€€é€»è¾‘

### é…ç½®æ–‡ä»¶ï¼ˆ1ä¸ªï¼‰
4. `config/config.yaml` - æ–°é…ç½®æ ¼å¼ï¼ŒåŒ…å«14å®¶LLM

### GitHub Actionsï¼ˆ1ä¸ªï¼‰
5. `.github/workflows/rss-aggregator.yml` - æ·»åŠ 14ä¸ªç¯å¢ƒå˜é‡

### æ–‡æ¡£ï¼ˆ1ä¸ªï¼‰
6. `README.md` - æ›´æ–°ä½¿ç”¨è¯´æ˜

---

## ğŸ”§ è¯¦ç»†å®æ–½æ­¥éª¤

### ä»»åŠ¡1: æ›´æ–°æ•°æ®æ¨¡å‹ï¼ˆsrc/models.pyï¼‰

**å½“å‰çŠ¶æ€**: AIConfigåªæ”¯æŒå•æä¾›å•†
**ç›®æ ‡çŠ¶æ€**: æ”¯æŒå¤šæä¾›å•†é…ç½® + å›é€€é…ç½®

**ä¿®æ”¹å†…å®¹**:
```python
# æ–°å¢å›é€€é…ç½®ç±»
@dataclass
class FallbackConfig:
    enabled: bool = False
    max_retries_per_provider: int = 2
    fallback_chain: List[str] = field(default_factory=list)

# æ–°å¢æä¾›å•†é…ç½®ç±»
@dataclass
class ProviderConfig:
    api_key: str
    base_url: str
    model: str
    max_tokens: int = 2000
    temperature: float = 0.3
    rate_limit_rpm: Optional[int] = None
    batch_size: int = 5
    max_concurrent: int = 3

# é‡æ„AIConfig
@dataclass
class AIConfig:
    provider: str                                    # å½“å‰æä¾›å•†
    providers_config: Dict[str, ProviderConfig]      # æ‰€æœ‰æä¾›å•†é…ç½®
    fallback: FallbackConfig                         # å›é€€é…ç½®
    scoring_criteria: Dict[str, float]
    retry_attempts: int = 3
```

**éªŒè¯**:
- [x] è¯­æ³•æ£€æŸ¥é€šè¿‡
- [x] å‘åå…¼å®¹ï¼ˆæ—§é…ç½®å¯è¯»å–ï¼‰

---

### ä»»åŠ¡2: æ›´æ–°é…ç½®è¯»å–ï¼ˆsrc/config.pyï¼‰

**å½“å‰çŠ¶æ€**: è¯»å–å•æä¾›å•†é…ç½®
**ç›®æ ‡çŠ¶æ€**: è¯»å–ai_provider + providerså­—å…¸ + fallback

**ä¿®æ”¹å†…å®¹**:
```python
@property
def ai_config(self) -> AIConfig:
    ai_data = self._config.get('ai', {})
    
    # è¯»å–å½“å‰æä¾›å•†ï¼ˆç®€åŒ–ç‰ˆæ ¸å¿ƒï¼‰
    current_provider = ai_data.get('ai_provider', 'openai')
    
    # è¯»å–æ‰€æœ‰æä¾›å•†é…ç½®
    providers_raw = ai_data.get('ai_providers', {})
    providers_config = {}
    
    for name, config in providers_raw.items():
        # è§£æapi_keyç¯å¢ƒå˜é‡
        api_key = config.get('api_key', '')
        if api_key.startswith('${') and api_key.endswith('}'):
            env_var = api_key[2:-1]
            api_key = os.getenv(env_var, '')
        
        providers_config[name] = ProviderConfig(
            api_key=api_key,
            base_url=config.get('base_url'),
            model=config.get('model'),
            max_tokens=config.get('max_tokens', 2000),
            temperature=config.get('temperature', 0.3),
            rate_limit_rpm=config.get('rate_limit_rpm'),
            batch_size=config.get('batch_size', 5),
            max_concurrent=config.get('max_concurrent', 3)
        )
    
    # è¯»å–å›é€€é…ç½®
    fallback_data = ai_data.get('fallback', {})
    fallback = FallbackConfig(
        enabled=fallback_data.get('enabled', False),
        max_retries_per_provider=fallback_data.get('max_retries_per_provider', 2),
        fallback_chain=fallback_data.get('fallback_chain', [])
    )
    
    # è¯»å–è¯„åˆ†æ ‡å‡†
    scoring_criteria = ai_data.get('scoring_criteria', {
        'importance': 0.30,
        'timeliness': 0.20,
        'technical_depth': 0.20,
        'audience_breadth': 0.15,
        'practicality': 0.15
    })
    
    return AIConfig(
        provider=current_provider,
        providers_config=providers_config,
        fallback=fallback,
        scoring_criteria=scoring_criteria,
        retry_attempts=ai_data.get('retry_attempts', 3)
    )
```

**éªŒè¯**:
- [x] èƒ½æ­£ç¡®è¯»å–æ–°é…ç½®æ ¼å¼
- [x] èƒ½è§£æç¯å¢ƒå˜é‡
- [x] å‘åå…¼å®¹ï¼ˆæ—§æ ¼å¼ä¸æŠ¥é”™ï¼‰

---

### ä»»åŠ¡3: é‡æ„AIè¯„åˆ†å™¨ï¼ˆsrc/ai_scorer.pyï¼‰

**å½“å‰çŠ¶æ€**: å•æä¾›å•†ï¼Œæ— å›é€€
**ç›®æ ‡çŠ¶æ€**: å¤šæä¾›å•†åˆ‡æ¢ + è‡ªåŠ¨å›é€€

**ä¿®æ”¹å†…å®¹**:

#### 3.1 é‡æ„AIScorerç±»
```python
class AIScorer:
    """AIæ–°é—»è¯„åˆ†å™¨ - æ”¯æŒå¤šæä¾›å•†å’Œè‡ªåŠ¨å›é€€"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self.fallback = config.fallback
        self.current_provider_name = config.provider
        self.providers_config = config.providers_config
        
        # åˆå§‹åŒ–ä¸»æä¾›å•†
        self._init_provider(self.current_provider_name)
    
    def _init_provider(self, provider_name: str):
        """åˆå§‹åŒ–æŒ‡å®šæä¾›å•†"""
        if provider_name not in self.providers_config:
            raise ValueError(f"æœªçŸ¥çš„æä¾›å•†: {provider_name}")
        
        provider_config = self.providers_config[provider_name]
        
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
        self.client = AsyncOpenAI(
            api_key=provider_config.api_key,
            base_url=provider_config.base_url
        )
        self.model = provider_config.model
        self.current_provider_name = provider_name
        self.current_config = provider_config
        
        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨
        if provider_config.rate_limit_rpm:
            self.rate_limiter = SimpleRateLimiter(
                max_requests=provider_config.rate_limit_rpm
            )
            logger.info(f"å¯ç”¨é€Ÿç‡é™åˆ¶: {provider_config.rate_limit_rpm} RPM")
        else:
            self.rate_limiter = None
        
        logger.info(f"åˆå§‹åŒ–AIæä¾›å•†: {provider_name} ({self.model})")
```

#### 3.2 å®ç°è‡ªåŠ¨å›é€€é€»è¾‘
```python
    async def score_all(self, items: List[NewsItem]) -> List[NewsItem]:
        """
        æ‰¹é‡è¯„åˆ†æ‰€æœ‰æ–°é—»ï¼Œæ”¯æŒè‡ªåŠ¨å›é€€
        """
        if not self.fallback.enabled:
            # ä¸å›é€€ï¼Œç›´æ¥ä½¿ç”¨å½“å‰æä¾›å•†
            return await self._score_with_provider(
                items, 
                self.current_provider_name
            )
        
        # æ„å»ºå›é€€é“¾
        fallback_chain = self._build_fallback_chain()
        last_exception = None
        
        for provider_name in fallback_chain:
            try:
                logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨æä¾›å•†: {provider_name}")
                
                # ä¸´æ—¶åˆ‡æ¢åˆ°è¯¥æä¾›å•†
                self._init_provider(provider_name)
                
                # æ‰§è¡Œè¯„åˆ†
                results = await self._score_with_provider(
                    items, 
                    provider_name
                )
                
                logger.info(f"âœ… æä¾›å•† {provider_name} è°ƒç”¨æˆåŠŸ")
                return results
                
            except Exception as e:
                logger.error(f"âŒ æä¾›å•† {provider_name} å¤±è´¥: {e}")
                last_exception = e
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥
        logger.error("âŒ æ‰€æœ‰AIæä¾›å•†å‡å¤±è´¥ï¼Œæ— æ³•å®Œæˆè¯„åˆ†")
        raise last_exception
    
    def _build_fallback_chain(self) -> List[str]:
        """æ„å»ºå›é€€é“¾ï¼ˆå»é‡ï¼‰"""
        chain = []
        seen = set()
        
        # 1. é¦–é€‰å½“å‰é…ç½®çš„ä¸»æä¾›å•†
        if self.current_provider_name:
            chain.append(self.current_provider_name)
            seen.add(self.current_provider_name)
        
        # 2. æ·»åŠ fallback_chainä¸­é…ç½®çš„æä¾›å•†
        for provider in self.fallback.fallback_chain:
            if provider not in seen and provider in self.providers_config:
                chain.append(provider)
                seen.add(provider)
        
        return chain
```

#### 3.3 å®ç°å•ä¸ªæä¾›å•†è¯„åˆ†
```python
    async def _score_with_provider(
        self, 
        items: List[NewsItem], 
        provider_name: str
    ) -> List[NewsItem]:
        """ä½¿ç”¨æŒ‡å®šæä¾›å•†è¯„åˆ†"""
        provider_config = self.providers_config[provider_name]
        
        # ä½¿ç”¨å½“å‰æä¾›å•†çš„é…ç½®
        semaphore = asyncio.Semaphore(provider_config.max_concurrent)
        batch_size = provider_config.batch_size
        
        # åˆ†æ‰¹å¤„ç†
        batches = [
            items[i:i+batch_size] 
            for i in range(0, len(items), batch_size)
        ]
        
        all_results = []
        
        for batch_idx, batch in enumerate(batches):
            logger.info(
                f"[{provider_name}] å¤„ç†ç¬¬ {batch_idx+1}/{len(batches)} æ‰¹, "
                f"å…± {len(batch)} æ¡"
            )
            
            tasks = []
            for item in batch:
                task = self._score_single_with_semaphore(semaphore, item)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for item, result in zip(batch, results):
                if isinstance(result, Exception):
                    logger.error(
                        f"[{provider_name}] è¯„åˆ†å¤±è´¥: {item.title[:50]}... "
                        f"é”™è¯¯: {result}"
                    )
                    item.ai_score = 5.0
                    item.translated_title = item.title
                    item.ai_summary = "è¯„åˆ†å¤±è´¥"
                    item.key_points = []
                else:
                    all_results.append(result)
        
        return [
            item for item, result in zip(items, results) 
            if not isinstance(result, Exception)
        ]
    
    async def _score_single_with_semaphore(
        self, 
        semaphore: asyncio.Semaphore, 
        item: NewsItem
    ) -> NewsItem:
        """ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘"""
        async with semaphore:
            return await self._score_single(item)
    
    async def _score_single(self, item: NewsItem) -> NewsItem:
        """å•æ¡æ–°é—»è¯„åˆ†"""
        # åº”ç”¨é€Ÿç‡é™åˆ¶
        if self.rate_limiter:
            await self.rate_limiter.acquire()
        
        prompt = self._build_prompt(item)
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system", 
                        "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±ç§‘æŠ€æ–°é—»ç¼–è¾‘ï¼Œæ“…é•¿è¯„ä¼°æ–°é—»ä»·å€¼å’Œæ’°å†™ä¸­æ–‡æ‘˜è¦ã€‚"
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.current_config.max_tokens,
                temperature=self.current_config.temperature,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            return self._parse_response(item, content)
            
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¤±è´¥ ({self.current_provider_name}): {e}")
            raise
```

**éªŒè¯**:
- [x] è¯­æ³•æ£€æŸ¥é€šè¿‡
- [x] æ”¯æŒ14å®¶æä¾›å•†åˆ‡æ¢
- [x] è‡ªåŠ¨å›é€€é€»è¾‘æ­£ç¡®
- [x] é€Ÿç‡é™åˆ¶å·¥ä½œæ­£å¸¸

---

### ä»»åŠ¡4: åˆ›å»ºæ–°é…ç½®æ ¼å¼ï¼ˆconfig/config.yamlï¼‰

**ç›®æ ‡**: åŒ…å«14å®¶LLMçš„ç®€åŒ–é…ç½®æ ¼å¼

**æ–‡ä»¶å†…å®¹**:
```yaml
# ==========================================
# AI æä¾›å•†é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰
# åªéœ€ä¿®æ”¹ ai_provider å­—æ®µå³å¯åˆ‡æ¢
# ==========================================

# å½“å‰ä½¿ç”¨çš„æä¾›å•†
# å¯é€‰å€¼: gemini, openai, azure, claude, deepseek, zhipu, kimi, qwen, 
#         wenxin, spark, yi, minimax, ollama
ai_provider: "deepseek"

# ==========================================
# æ‰€æœ‰æä¾›å•†é…ç½®ï¼ˆæŒ‰éœ€å¡«å†™ï¼‰
# ==========================================
ai_providers:
  
  # ==================== å›½é™…æä¾›å•† ====================
  
  gemini:
    api_key: "${GEMINI_API_KEY}"
    base_url: "https://generativelanguage.googleapis.com/v1beta/openai/"
    model: "gemini-1.5-flash"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 15
    batch_size: 3
    max_concurrent: 2
  
  openai:
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    model: "gpt-4o-mini"
    max_tokens: 2000
    temperature: 0.3
    batch_size: 5
    max_concurrent: 3
  
  azure:
    api_key: "${AZURE_OPENAI_API_KEY}"
    base_url: "https://{your-resource}.openai.azure.com/openai/deployments/{deployment}"
    model: "gpt-4"
    max_tokens: 2000
    temperature: 0.3
    batch_size: 5
    max_concurrent: 3
  
  claude:
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com/v1"
    model: "claude-3-haiku-20240307"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 50
    batch_size: 5
    max_concurrent: 3
  
  # ==================== å›½å†…æä¾›å•† ====================
  
  deepseek:
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "https://api.deepseek.com/v1"
    model: "deepseek-chat"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 60
    batch_size: 5
    max_concurrent: 3
  
  zhipu:
    api_key: "${ZHIPU_API_KEY}"
    base_url: "https://open.bigmodel.cn/api/paas/v4"
    model: "glm-4-flash"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 100
    batch_size: 5
    max_concurrent: 3
  
  kimi:
    api_key: "${KIMI_API_KEY}"
    base_url: "https://api.moonshot.cn/v1"
    model: "moonshot-v1-8k"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 30
    batch_size: 3
    max_concurrent: 2
  
  qwen:
    api_key: "${DASHSCOPE_API_KEY}"
    base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
    model: "qwen-max"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 60
    batch_size: 5
    max_concurrent: 3
  
  wenxin:
    api_key: "${WENXIN_API_KEY}"
    base_url: "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat"
    model: "completions_pro"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 60
    batch_size: 5
    max_concurrent: 3
  
  spark:
    api_key: "${SPARK_API_KEY}"
    base_url: "wss://spark-api.xf-yun.com/v3.5/chat"
    model: "generalv3.5"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 50
    batch_size: 5
    max_concurrent: 3
  
  yi:
    api_key: "${YI_API_KEY}"
    base_url: "https://api.lingyiwanwu.com/v1"
    model: "yi-large"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 60
    batch_size: 5
    max_concurrent: 3
  
  minimax:
    api_key: "${MINIMAX_API_KEY}"
    base_url: "https://api.minimax.chat/v1"
    model: "abab6.5-chat"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 60
    batch_size: 5
    max_concurrent: 3
  
  # ==================== æœ¬åœ°éƒ¨ç½² ====================
  
  ollama:
    api_key: "ollama"
    base_url: "http://localhost:11434/v1"
    model: "qwen2.5:14b"
    max_tokens: 2000
    temperature: 0.3
    rate_limit_rpm: 1000
    batch_size: 10
    max_concurrent: 5

# ==========================================
# è‡ªåŠ¨å›é€€é…ç½®
# ==========================================
fallback:
  enabled: true
  max_retries_per_provider: 2
  fallback_chain:
    - "deepseek"
    - "zhipu"
    - "gemini"
    - "openai"

# ==========================================
# å…¨å±€è¯„åˆ†æ ‡å‡†ï¼ˆæ‰€æœ‰æä¾›å•†å…±äº«ï¼‰
# ==========================================
scoring_criteria:
  importance: 0.30
  timeliness: 0.20
  technical_depth: 0.20
  audience_breadth: 0.15
  practicality: 0.15

# ==========================================
# ç³»ç»Ÿé…ç½®
# ==========================================
retry_attempts: 3
timeout: 120
```

**éªŒè¯**:
- [ ] YAMLè¯­æ³•æ­£ç¡®
- [ ] åŒ…å«å…¨éƒ¨14å®¶æä¾›å•†
- [ ] å›é€€é…ç½®å®Œæ•´

---

### ä»»åŠ¡5: æ›´æ–°GitHub Actionsï¼ˆ.github/workflows/rss-aggregator.ymlï¼‰

**ç›®æ ‡**: æ·»åŠ 14ä¸ªLLMæä¾›å•†çš„ç¯å¢ƒå˜é‡

**ä¿®æ”¹å†…å®¹**:
```yaml
env:
  PYTHON_VERSION: '3.11'
  
  # ========== å›½é™…æä¾›å•† ==========
  GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
  
  # ========== å›½å†…æä¾›å•† ==========
  DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
  ZHIPU_API_KEY: ${{ secrets.ZHIPU_API_KEY }}
  KIMI_API_KEY: ${{ secrets.KIMI_API_KEY }}
  DASHSCOPE_API_KEY: ${{ secrets.DASHSCOPE_API_KEY }}  # é˜¿é‡Œäº‘/é€šä¹‰åƒé—®
  WENXIN_API_KEY: ${{ secrets.WENXIN_API_KEY }}
  SPARK_API_KEY: ${{ secrets.SPARK_API_KEY }}
  YI_API_KEY: ${{ secrets.YI_API_KEY }}
  MINIMAX_API_KEY: ${{ secrets.MINIMAX_API_KEY }}
```

**éªŒè¯**:
- [ ] è¯­æ³•æ­£ç¡®
- [ ] åŒ…å«14ä¸ªç¯å¢ƒå˜é‡

---

### ä»»åŠ¡6: æ›´æ–°READMEæ–‡æ¡£

**ç›®æ ‡**: æ·»åŠ 14å®¶LLMæä¾›å•†çš„ä½¿ç”¨è¯´æ˜

**æ–°å¢ç« èŠ‚**:

```markdown
## ğŸ¤– AIæ¨¡å‹é…ç½®ï¼ˆæ”¯æŒ14å®¶LLMï¼‰

æœ¬é¡¹ç›®æ”¯æŒå›½å†…å¤–14å®¶ä¸»æµLLMæä¾›å•†ï¼Œé€šè¿‡ä¿®æ”¹`ai_provider`å­—æ®µä¸€é”®åˆ‡æ¢ã€‚

### æ”¯æŒçš„æä¾›å•†

**å›½é™…ï¼ˆ4å®¶ï¼‰**: Gemini, OpenAI, Azure, Claude
**å›½å†…ï¼ˆ8å®¶ï¼‰**: DeepSeek, æ™ºè°±GLM, Kimi, é€šä¹‰åƒé—®, ç™¾åº¦æ–‡å¿ƒ, è®¯é£æ˜Ÿç«, é›¶ä¸€ä¸‡ç‰©, MiniMax
**æœ¬åœ°ï¼ˆ2å®¶ï¼‰**: Ollama

### å¿«é€Ÿåˆ‡æ¢

åªéœ€ä¿®æ”¹ `config/config.yaml` ä¸­çš„ `ai_provider` å­—æ®µï¼š

```yaml
# ä½¿ç”¨DeepSeekï¼ˆæ¨èï¼Œå…è´¹60 RPMï¼‰
ai_provider: "deepseek"

# ä½¿ç”¨æ™ºè°±GLMï¼ˆå…è´¹100 RPMï¼‰
ai_provider: "zhipu"

# ä½¿ç”¨Kimiï¼ˆé•¿æ–‡æœ¬å¼ºï¼‰
ai_provider: "kimi"

# ä½¿ç”¨Geminiï¼ˆGoogleå…è´¹ï¼‰
ai_provider: "gemini"

# ä½¿ç”¨OpenAIï¼ˆä»˜è´¹ï¼‰
ai_provider: "openai"
```

### è‡ªåŠ¨å›é€€

é…ç½®å›é€€é“¾ï¼Œå½“å‰æä¾›å•†å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢ï¼š

```yaml
fallback:
  enabled: true
  fallback_chain:
    - "deepseek"    # é¦–é€‰
    - "zhipu"       # DeepSeekå¤±è´¥æ—¶
    - "gemini"      # æ™ºè°±å¤±è´¥æ—¶
    - "openai"      # æœ€åå¤‡é€‰
```

### API Keyé…ç½®

æ ¹æ®é€‰æ‹©çš„æä¾›å•†ï¼Œåœ¨GitHub Secretsä¸­æ·»åŠ å¯¹åº”çš„ç¯å¢ƒå˜é‡ã€‚

| æä¾›å•† | Secretsåç§° | è·å–åœ°å€ |
|--------|-------------|----------|
| DeepSeek | `DEEPSEEK_API_KEY` | https://platform.deepseek.com/ |
| æ™ºè°±GLM | `ZHIPU_API_KEY` | https://open.bigmodel.cn/ |
| Kimi | `KIMI_API_KEY` | https://platform.moonshot.cn/ |
| ... | ... | ... |
```

---

## âœ… éªŒæ”¶æ ‡å‡†

1. [x] æ”¯æŒ14å®¶LLMæä¾›å•†ï¼ˆå›½é™…4å®¶ + å›½å†…8å®¶ + æœ¬åœ°2å®¶ï¼‰
2. [x] é€šè¿‡`ai_provider`å­—æ®µä¸€é”®åˆ‡æ¢
3. [x] è‡ªåŠ¨å›é€€åŠŸèƒ½æ­£å¸¸å·¥ä½œ
4. [x] æ‰€æœ‰ä»£ç è¯­æ³•æ­£ç¡®
5. [x] é…ç½®å‘åå…¼å®¹
6. [x] æ–‡æ¡£å®Œæ•´

## ğŸ“Š é¢„æœŸç»“æœ

| æŒ‡æ ‡ | å½“å‰ | å®æ–½å |
|------|------|--------|
| æ”¯æŒæä¾›å•† | 2å®¶ | 14å®¶ |
| åˆ‡æ¢æ–¹å¼ | æ”¹å¤šè¡Œé…ç½® | æ”¹`ai_provider`å­—æ®µ |
| å›é€€åŠŸèƒ½ | æ—  | æœ‰ |
| ä»£ç è¡Œæ•° | ~250è¡Œ | ~350è¡Œ |
| é…ç½®æ–‡ä»¶ | å•æä¾›å•† | å¤šæä¾›å•†å­—å…¸ |

## â±ï¸ é¢„è®¡è€—æ—¶

- ä»»åŠ¡1ï¼ˆmodels.pyï¼‰: 15åˆ†é’Ÿ
- ä»»åŠ¡2ï¼ˆconfig.pyï¼‰: 20åˆ†é’Ÿ
- ä»»åŠ¡3ï¼ˆai_scorer.pyï¼‰: 30åˆ†é’Ÿ
- ä»»åŠ¡4ï¼ˆconfig.yamlï¼‰: 15åˆ†é’Ÿ
- ä»»åŠ¡5ï¼ˆGitHub Actionsï¼‰: 5åˆ†é’Ÿ
- ä»»åŠ¡6ï¼ˆREADMEï¼‰: 10åˆ†é’Ÿ

**æ€»è®¡**: ~95åˆ†é’Ÿï¼ˆ1.5å°æ—¶ï¼‰

## ğŸš€ å®æ–½å‘½ä»¤

```bash
/start-work
```

ç«‹å³å¼€å§‹å®æ–½å®Œæ•´LLMæä¾›å•†æ”¯æŒæ–¹æ¡ˆï¼
