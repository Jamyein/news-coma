"""
å†…å®¹è·å–æ¨¡å—
ä½¿ç”¨ trafilatura åº“ä»URLè·å–æ–‡ç« å…¨æ–‡
æ”¯æŒå¹¶å‘æ§åˆ¶å’Œè¶…æ—¶è®¾ç½®ï¼Œæ— éœ€ç¼“å­˜ï¼ˆRSSå¢é‡è·å–ç¡®ä¿ä¸é‡å¤ï¼‰
"""
import asyncio
import logging
from typing import Optional, List, Dict, Tuple
from functools import partial

# trafilatura 2.0.0 imports
from trafilatura.settings import Extractor

logger = logging.getLogger(__name__)

# å…¨å±€è®¾ç½®
DEFAULT_CONCURRENCY = 5
DEFAULT_TIMEOUT_MIN = 10  # ç§’
DEFAULT_TIMEOUT_MAX = 30  # ç§’


class ContentFetcher:
    """å†…å®¹è·å–å™¨ - ä½¿ç”¨ trafilatura è·å–æ–‡ç« å…¨æ–‡"""
    
    def __init__(
        self, 
        max_concurrent: int = DEFAULT_CONCURRENCY,
        timeout_range: Tuple[int, int] = (DEFAULT_TIMEOUT_MIN, DEFAULT_TIMEOUT_MAX)
    ):
        """
        åˆå§‹åŒ–å†…å®¹è·å–å™¨
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ä¸º5
            timeout_range: è¶…æ—¶èŒƒå›´ï¼ˆæœ€å°ï¼Œæœ€å¤§ï¼‰ç§’ï¼Œé»˜è®¤ä¸º(10, 30)
        """
        self.max_concurrent = max_concurrent
        self.timeout_range = timeout_range
        
        # æ£€æŸ¥ trafilatura åº“æ˜¯å¦å¯ç”¨
        self._trafilatura_available = True
        try:
            import trafilatura
            self._trafilatura = trafilatura
            logger.info("âœ… trafilatura åº“åŠ è½½æˆåŠŸ")
        except ImportError as e:
            logger.warning(f"âš ï¸ trafilatura åº“ä¸å¯ç”¨: {e}")
            self._trafilatura_available = False
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "successful_fetches": 0,
            "failed_fetches": 0,
            "timeout_fetches": 0,
            "total_fetches": 0
        }
    
    async def fetch(self, url: str, timeout: Optional[int] = None) -> Optional[str]:
        """
        è·å–å•ä¸ªURLçš„æ–‡ç« å…¨æ–‡
        
        Args:
            url: æ–‡ç« URL
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤èŒƒå›´
            
        Returns:
            æ–‡ç« å…¨æ–‡å†…å®¹ï¼Œå¦‚æœè·å–å¤±è´¥åˆ™è¿”å›None
        """
        self.stats["total_fetches"] += 1
        
        if timeout is None:
            # ä½¿ç”¨é»˜è®¤èŒƒå›´çš„ä¸­é—´å€¼
            timeout = (self.timeout_range[0] + self.timeout_range[1]) // 2
        
        logger.debug(f"ğŸŒ å¼€å§‹è·å–å…¨æ–‡: {url} (è¶…æ—¶: {timeout}ç§’)")
        
        try:
            # ä½¿ç”¨ asyncio.wait_for è®¾ç½®è¶…æ—¶
            content = await asyncio.wait_for(
                self._fetch_inner(url),
                timeout=timeout
            )
            
            if content:
                self.stats["successful_fetches"] += 1
                logger.info(f"âœ… æˆåŠŸè·å–å…¨æ–‡: {url} (é•¿åº¦: {len(content)} å­—ç¬¦)")
                return content
            else:
                self.stats["failed_fetches"] += 1
                logger.warning(f"âš ï¸ è·å–å…¨æ–‡è¿”å›ç©ºå†…å®¹: {url}")
                return None
                
        except asyncio.TimeoutError:
            self.stats["timeout_fetches"] += 1
            logger.warning(f"â° è·å–å…¨æ–‡è¶…æ—¶: {url} (è¶…æ—¶: {timeout}ç§’)")
            return None
        except Exception as e:
            self.stats["failed_fetches"] += 1
            logger.error(f"âŒ è·å–å…¨æ–‡å¤±è´¥ {url}: {e}")
            return None
    
    async def fetch_multiple(
        self, 
        urls: List[str], 
        max_concurrent: Optional[int] = None,
        timeout: Optional[int] = None
    ) -> Dict[str, Optional[str]]:
        """
        æ‰¹é‡è·å–å¤šä¸ªURLçš„æ–‡ç« å…¨æ–‡
        
        Args:
            urls: URLåˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®ä¾‹çš„max_concurrent
            timeout: æ¯ä¸ªè¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤èŒƒå›´
            
        Returns:
            å­—å…¸æ ¼å¼çš„ç»“æœ {url: content}ï¼Œå¤±è´¥çš„URLå¯¹åº”çš„å€¼ä¸ºNone
        """
        if not urls:
            logger.info("ğŸ“‹ æ‰¹é‡è·å–ï¼šURLåˆ—è¡¨ä¸ºç©º")
            return {}
        
        # ä½¿ç”¨æŒ‡å®šçš„å¹¶å‘æ•°æˆ–å®ä¾‹çš„å¹¶å‘æ•°
        concurrent = max_concurrent or self.max_concurrent
        semaphore = asyncio.Semaphore(concurrent)
        
        logger.info(f"ğŸ“‹ å¼€å§‹æ‰¹é‡è·å– {len(urls)} ç¯‡æ–‡ç« å…¨æ–‡ (å¹¶å‘: {concurrent})")
        
        async def fetch_with_semaphore(url: str) -> Tuple[str, Optional[str]]:
            """å¸¦ä¿¡å·é‡æ§åˆ¶çš„å•ä¸ªè·å–"""
            async with semaphore:
                # æ·»åŠ å°å»¶è¿Ÿé¿å…è¢«å°ç¦
                await asyncio.sleep(0.5)
                content = await self.fetch(url, timeout)
                return url, content
        
        # å¹¶å‘è·å–æ‰€æœ‰URL
        tasks = [fetch_with_semaphore(url) for url in urls]
        results = await asyncio.gather(*tasks)
        
        # è½¬æ¢ä¸ºå­—å…¸
        result_dict = {url: content for url, content in results}
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for content in result_dict.values() if content is not None)
        failure_count = len(urls) - success_count
        
        logger.info(
            f"ğŸ“Š æ‰¹é‡è·å–å®Œæˆ: æˆåŠŸ {success_count}/{len(urls)} ç¯‡, "
            f"å¤±è´¥ {failure_count}/{len(urls)} ç¯‡"
        )
        
        return result_dict
    
    async def fetch_with_timeout(
        self, 
        url: str, 
        min_timeout: Optional[int] = None,
        max_timeout: Optional[int] = None
    ) -> Optional[str]:
        """
        ä½¿ç”¨è‡ªé€‚åº”è¶…æ—¶è·å–æ–‡ç« å…¨æ–‡
        å¦‚æœæœ€å°è¶…æ—¶å¤±è´¥ï¼Œä¼šå°è¯•ä½¿ç”¨æœ€å¤§è¶…æ—¶é‡è¯•
        
        Args:
            url: æ–‡ç« URL
            min_timeout: æœ€å°è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®ä¾‹çš„timeout_range[0]
            max_timeout: æœ€å¤§è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®ä¾‹çš„timeout_range[1]
            
        Returns:
            æ–‡ç« å…¨æ–‡å†…å®¹ï¼Œå¦‚æœè·å–å¤±è´¥åˆ™è¿”å›None
        """
        if min_timeout is None:
            min_timeout = self.timeout_range[0]
        if max_timeout is None:
            max_timeout = self.timeout_range[1]
        
        # å…ˆå°è¯•æœ€å°è¶…æ—¶
        logger.debug(f"âš¡ å°è¯•å¿«é€Ÿè·å– (è¶…æ—¶: {min_timeout}ç§’): {url}")
        content = await self.fetch(url, min_timeout)
        
        if content:
            logger.debug(f"âœ… å¿«é€Ÿè·å–æˆåŠŸ: {url}")
            return content
        
        # å¦‚æœå¿«é€Ÿè·å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æœ€å¤§è¶…æ—¶
        logger.debug(f"ğŸŒ å¿«é€Ÿè·å–å¤±è´¥ï¼Œå°è¯•å»¶é•¿è¶…æ—¶ (è¶…æ—¶: {max_timeout}ç§’): {url}")
        content = await self.fetch(url, max_timeout)
        
        if content:
            logger.info(f"âœ… å»¶é•¿è¶…æ—¶åè·å–æˆåŠŸ: {url}")
            return content
        
        logger.warning(f"âš ï¸ æ‰€æœ‰è¶…æ—¶è®¾ç½®å‡å¤±è´¥: {url}")
        return None
    
    async def _fetch_inner(self, url: str) -> Optional[str]:
        """
        å†…éƒ¨è·å–é€»è¾‘ï¼Œä½¿ç”¨ trafilatura åº“
        éœ€è¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿è¡Œï¼ˆä½¿ç”¨ asyncio.to_threadï¼‰
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            æ–‡ç« å…¨æ–‡å†…å®¹
        """
        if not self._trafilatura_available:
            logger.error(f"âŒ trafilatura åº“ä¸å¯ç”¨ï¼Œæ— æ³•è·å–å…¨æ–‡: {url}")
            return None
        
        try:
            # trafilatura 2.0.0: ä½¿ç”¨ Extractor é…ç½®å¯¹è±¡
            from asyncio import to_thread

            # åˆ›å»º Extractor é…ç½®å¯¹è±¡ï¼ˆtrafilatura 2.0.0 APIï¼‰
            extractor = Extractor(
                comments=False,  # å¯¹åº”åŸ include_comments=False
                tables=True      # å¯¹åº”åŸ include_tables=True
            )

            # ä½¿ç”¨æ–°ç‰ˆ APIï¼šfetch_url(url, no_ssl, options)
            content = await to_thread(
                self._trafilatura.fetch_url,
                url,
                no_ssl=False,
                options=extractor
            )
            
            # æ¸…ç†å’ŒéªŒè¯å†…å®¹
            if content and isinstance(content, str):
                content = content.strip()
                if len(content) < 50:  # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ²¡æœ‰è·å–åˆ°æœ‰æ•ˆå†…å®¹
                    logger.debug(f"âš ï¸ è·å–çš„å†…å®¹è¿‡çŸ­: {url} (é•¿åº¦: {len(content)} å­—ç¬¦)")
                    return None
                return content
            else:
                return None
                
        except Exception as e:
            logger.error(f"âŒ trafilatura è·å–å¤±è´¥ {url}: {e}")
            return None
    
    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "success_rate": (
                self.stats["successful_fetches"] / self.stats["total_fetches"] 
                if self.stats["total_fetches"] > 0 else 0
            )
        }
    
    def reset_stats(self) -> None:
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "successful_fetches": 0,
            "failed_fetches": 0,
            "timeout_fetches": 0,
            "total_fetches": 0
        }


# ä¾¿æ·å‡½æ•°
async def fetch_content(url: str, timeout: int = None) -> Optional[str]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–å•ä¸ªURLçš„æ–‡ç« å…¨æ–‡
    
    Args:
        url: æ–‡ç« URL
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        æ–‡ç« å…¨æ–‡å†…å®¹
    """
    fetcher = ContentFetcher()
    return await fetcher.fetch(url, timeout)


async def fetch_contents(
    urls: List[str], 
    max_concurrent: int = DEFAULT_CONCURRENCY
) -> Dict[str, Optional[str]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ‰¹é‡è·å–å¤šä¸ªURLçš„æ–‡ç« å…¨æ–‡
    
    Args:
        urls: URLåˆ—è¡¨
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        
    Returns:
        å­—å…¸æ ¼å¼çš„ç»“æœ {url: content}
    """
    fetcher = ContentFetcher(max_concurrent=max_concurrent)
    return await fetcher.fetch_multiple(urls)