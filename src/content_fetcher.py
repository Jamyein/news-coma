"""
å†…å®¹è·å–æ¨¡å—
ä½¿ç”¨ trafilatura åº“ä»URLè·å–æ–‡ç« å…¨æ–‡
æ”¯æŒå¹¶å‘æ§åˆ¶å’Œè¶…æ—¶è®¾ç½®ï¼Œæ— éœ€ç¼“å­˜ï¼ˆRSSå¢é‡è·å–ç¡®ä¿ä¸é‡å¤ï¼‰
"""
import asyncio
import logging
from typing import Optional, List, Dict, Tuple

logger = logging.getLogger(__name__)


class ContentFetcher:
    """å†…å®¹è·å–å™¨ - ä½¿ç”¨ trafilatura è·å–æ–‡ç« å…¨æ–‡"""
    
    def __init__(
        self, 
        max_concurrent: int = 5,
        timeout_range: Tuple[int, int] = (10, 30)
    ):
        """åˆå§‹åŒ–å†…å®¹è·å–å™¨"""
        self.max_concurrent = max_concurrent
        self.timeout_range = timeout_range
        
        # å°è¯•å¯¼å…¥ trafilatura
        try:
            import trafilatura
            self._trafilatura = trafilatura
            logger.info("âœ… trafilatura åº“åŠ è½½æˆåŠŸ")
        except ImportError:
            self._trafilatura = None
            logger.warning("âš ï¸ trafilatura åº“ä¸å¯ç”¨")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "successful_fetches": 0,
            "failed_fetches": 0,
            "timeout_fetches": 0,
            "total_fetches": 0
        }
    
    def _handle_fetch_error(self, error: Exception, url: str, context: str = "") -> None:
        """ç»Ÿä¸€å¤„ç†è·å–é”™è¯¯å¹¶æ›´æ–°ç»Ÿè®¡"""
        error_type = type(error).__name__

        if isinstance(error, asyncio.TimeoutError):
            self.stats["timeout_fetches"] += 1
            logger.warning(f"â° è·å–è¶…æ—¶ [{context}]: {url}")
        else:
            self.stats["failed_fetches"] += 1
            logger.error(f"âŒ è·å–å¤±è´¥ [{context}] {url}: {error}")

    async def fetch(self, url: str, timeout: Optional[int] = None) -> Optional[str]:
        """è·å–å•ä¸ªURLçš„æ–‡ç« å…¨æ–‡"""
        self.stats["total_fetches"] += 1
        timeout = timeout or (self.timeout_range[0] + self.timeout_range[1]) // 2

        try:
            content = await asyncio.wait_for(self._fetch_inner(url), timeout=timeout)
            if content:
                self.stats["successful_fetches"] += 1
                logger.debug(f"âœ… è·å–å…¨æ–‡æˆåŠŸ: {url} ({len(content)} å­—ç¬¦)")
                return content
            else:
                self.stats["failed_fetches"] += 1
                return None
        except Exception as e:
            self._handle_fetch_error(e, url)
            return None
    
    async def fetch_multiple(
        self, 
        urls: List[str], 
        max_concurrent: Optional[int] = None,
        timeout: Optional[int] = None
    ) -> Dict[str, Optional[str]]:
        """
        æ‰¹é‡è·å–å¤šä¸ªURLçš„æ–‡ç« å…¨æ–‡
        
        Args:
            urls: URLåˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®ä¾‹çš„max_concurrent
            timeout: æ¯ä¸ªè¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤èŒƒå›´
            
        Returns:
            å­—å…¸æ ¼å¼çš„ç»“æœ {url: content}ï¼Œå¤±è´¥çš„URLå¯¹åº”çš„å€¼ä¸ºNone
        """
        if not urls:
            logger.info("ğŸ“‹ æ‰¹é‡è·å–ï¼šURLåˆ—è¡¨ä¸ºç©º")
            return {}
        
        # ä½¿ç”¨æŒ‡å®šçš„å¹¶å‘æ•°æˆ–å®ä¾‹çš„å¹¶å‘æ•°
        concurrent = max_concurrent or self.max_concurrent
        semaphore = asyncio.Semaphore(concurrent)
        
        logger.info(f"ğŸ“‹ å¼€å§‹æ‰¹é‡è·å– {len(urls)} ç¯‡æ–‡ç« å…¨æ–‡ (å¹¶å‘: {concurrent})")
        
        async def fetch_with_semaphore(url: str) -> Tuple[str, Optional[str]]:
            """å¸¦ä¿¡å·é‡æ§åˆ¶çš„å•ä¸ªè·å–"""
            async with semaphore:
                # æ·»åŠ å°å»¶è¿Ÿé¿å…è¢«å°ç¦
                await asyncio.sleep(0.5)
                content = await self.fetch(url, timeout)
                return url, content
        
        # å¹¶å‘è·å–æ‰€æœ‰URL
        tasks = [fetch_with_semaphore(url) for url in urls]
        results = await asyncio.gather(*tasks)
        
        # è½¬æ¢ä¸ºå­—å…¸
        result_dict = {url: content for url, content in results}
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for content in result_dict.values() if content is not None)
        failure_count = len(urls) - success_count
        
        logger.info(
            f"ğŸ“Š æ‰¹é‡è·å–å®Œæˆ: æˆåŠŸ {success_count}/{len(urls)} ç¯‡, "
            f"å¤±è´¥ {failure_count}/{len(urls)} ç¯‡"
        )
        
        return result_dict
    
    async def fetch_with_timeout(
        self, 
        url: str, 
        min_timeout: Optional[int] = None,
        max_timeout: Optional[int] = None
    ) -> Optional[str]:
        """ä½¿ç”¨è‡ªé€‚åº”è¶…æ—¶è·å–æ–‡ç« å…¨æ–‡ï¼ˆå…ˆå°è¯•å¿«è¶…æ—¶ï¼Œå¤±è´¥åˆ™ç”¨æ…¢è¶…æ—¶ï¼‰"""
        min_timeout = min_timeout or self.timeout_range[0]
        max_timeout = max_timeout or self.timeout_range[1]
        
        # å…ˆå°è¯•æœ€å°è¶…æ—¶
        content = await self.fetch(url, min_timeout)
        if content:
            return content
        
        # å¤±è´¥åˆ™ç”¨æœ€å¤§è¶…æ—¶é‡è¯•
        return await self.fetch(url, max_timeout)
    
    async def _fetch_inner(self, url: str) -> Optional[str]:
        """ä½¿ç”¨ trafilatura è·å–æ–‡ç« å…¨æ–‡"""
        if not self._trafilatura:
            return None
        
        try:
            from asyncio import to_thread
            from trafilatura.settings import Extractor

            extractor = Extractor(comments=False, tables=True)
            content = await to_thread(
                self._trafilatura.fetch_url, url, no_ssl=False, options=extractor
            )
            
            if content and len(content.strip()) >= 50:
                return content.strip()
            return None
                
        except Exception as e:
            logger.debug(f"trafilatura è·å–å¤±è´¥ {url}: {e}")
            return None
    
    def get_stats(self) -> Dict[str, float]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = self.stats["total_fetches"]
        return {
            **self.stats,
            "success_rate": self.stats["successful_fetches"] / total if total > 0 else 0
        }
    
    def reset_stats(self) -> None:
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        for key in self.stats:
            self.stats[key] = 0


# ä¾¿æ·å‡½æ•°
# ä¾¿æ·å‡½æ•°
async def fetch_content(url: str, timeout: int = None) -> Optional[str]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–å•ä¸ªURLçš„æ–‡ç« å…¨æ–‡"""
    return await ContentFetcher().fetch(url, timeout)


async def fetch_contents(urls: List[str], max_concurrent: int = 5) -> Dict[str, Optional[str]]:
    """ä¾¿æ·å‡½æ•°ï¼šæ‰¹é‡è·å–å¤šä¸ªURLçš„æ–‡ç« å…¨æ–‡"""
    return await ContentFetcher(max_concurrent=max_concurrent).fetch_multiple(urls)