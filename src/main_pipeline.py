"""
æ”¯æŒæµæ°´çº¿æ¶æ„çš„ä¸»ç¨‹åºå…¥å£
åè°ƒå„æ¨¡å—å®ŒæˆRSSæ–°é—»èšåˆæµç¨‹ï¼Œä½¿ç”¨å¼‚æ­¥æµæ°´çº¿
"""
import os
import sys

# Add project root to sys.path to allow imports from src package
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
import asyncio
from datetime import datetime
from typing import List, Optional, AsyncIterator

from src.config import Config
from src.models import NewsItem, RSSSource
from src.rss_fetcher import RSSFetcher
from src.batch_scorer import BatchScorer
from src.ai_cache import AICache
from src.markdown_generator import MarkdownGenerator
from src.rss_generator import RSSGenerator
from src.history_manager import HistoryManager
from src.monitoring import create_monitor, StageType, PerformanceMonitor
from src.pipeline import AsyncPipeline, PipelineConfig
from src.stages import create_default_pipeline_stages

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)


class RSSPipelineAggregator:
    """RSSæ–°é—»èšåˆå™¨ä¸»ç±»ï¼ˆæµæ°´çº¿ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, enable_monitoring: bool = True, use_pipeline: bool = True):
        self.config = Config()
        self.history = HistoryManager()
        self.use_pipeline = use_pipeline
        
        # åŸºç¡€ç»„ä»¶
        self.fetcher = None
        self.scorer = None
        self.ai_cache = None
        self.markdown_gen = None
        self.rss_gen = None
        
        # æ€§èƒ½ç›‘æ§å™¨
        self.monitor = None
        if enable_monitoring:
            self.monitor = create_monitor(
                output_dir="metrics",
                enable_logging=True,
                auto_save=True
            )
        
        # æµæ°´çº¿
        self.pipeline = None
        if use_pipeline:
            self._init_pipeline()
    
    def _init_pipeline(self):
        """åˆå§‹åŒ–æµæ°´çº¿"""
        logger.info("åˆå§‹åŒ–å¼‚æ­¥æµæ°´çº¿...")
        
        # åˆ›å»ºæµæ°´çº¿é…ç½®
        pipeline_config = PipelineConfig(
            max_queue_size=100,
            timeout=300.0,  # 5åˆ†é’Ÿè¶…æ—¶
            stop_on_critical_error=True
        )
        
        # åˆ›å»ºæµæ°´çº¿å®ä¾‹
        self.pipeline = AsyncPipeline(
            config=pipeline_config,
            monitor=self.monitor
        )
        
        logger.info("âœ“ æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    
    async def run(self) -> bool:
        """
        æ‰§è¡Œå®Œæ•´çš„æ–°é—»èšåˆæµç¨‹
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        start_time = datetime.now()
        logger.info("=" * 60)
        logger.info(f"ğŸš€ RSSæ–°é—»èšåˆå¼€å§‹ï¼ˆæµæ°´çº¿æ¨¡å¼ï¼‰ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60)
        
        try:
            # å¼€å§‹æ€§èƒ½ç›‘æ§
            if self.monitor:
                self.monitor.start()
            
            try:
                # 1. åˆå§‹åŒ–å„æ¨¡å—
                self._init_modules()
                
                if self.use_pipeline:
                    # 2. ä½¿ç”¨æµæ°´çº¿å¤„ç†
                    success = await self._run_with_pipeline()
                else:
                    # 2. ä¼ ç»Ÿæ–¹å¼å¤„ç†
                    success = await self._run_traditional()
                
                if not success:
                    return False
                
            except Exception as e:
                # è®°å½•é”™è¯¯
                if self.monitor:
                    self.monitor.increment('errors')
                raise
            
            finally:
                # ç»“æŸæ€§èƒ½ç›‘æ§
                if self.monitor:
                    self.monitor.end()
                    # æ‰“å°æ€§èƒ½æ‘˜è¦
                    self._print_performance_summary()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info("=" * 60)
            logger.info(f"âœ… RSSæ–°é—»èšåˆå®Œæˆ - è€—æ—¶: {duration:.1f}ç§’")
            logger.info("=" * 60)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return False
    
    def _init_modules(self):
        """åˆå§‹åŒ–å„æ¨¡å—"""
        logger.info("åˆå§‹åŒ–æ¨¡å—...")
        
        # RSSæŠ“å–å™¨
        self.fetcher = RSSFetcher(
            sources=self.config.rss_sources,
            output_config=self.config.output_config,
            filter_config=self.config.filter_config
        )
        
        # AIæ‰¹å¤„ç†è¯„åˆ†å™¨
        self.scorer = BatchScorer(
            config=self.config.ai_config
        )
        
        # AIç¼“å­˜
        self.ai_cache = AICache()
        
        # è¾“å‡ºç”Ÿæˆå™¨
        self.markdown_gen = MarkdownGenerator(
            output_dir="docs",
            archive_dir="archive"
        )
        
        self.rss_gen = RSSGenerator(
            feed_path="feed.xml",
            max_items=self.config.output_config.max_feed_items
        )
        
        logger.info(f"âœ“ å·²åŠ è½½ {len(self.config.rss_sources)} ä¸ªRSSæº")
        ai_config = self.config.ai_config
        current_provider = ai_config.provider
        provider_config = ai_config.providers_config[current_provider]
        logger.info(f"âœ“ AIæ¨¡å‹: {current_provider} ({provider_config.model})")
        
        # å¦‚æœä½¿ç”¨æµæ°´çº¿ï¼Œæ·»åŠ é˜¶æ®µ
        if self.use_pipeline and self.pipeline:
            self._add_pipeline_stages()
    
    def _add_pipeline_stages(self):
        """å‘æµæ°´çº¿æ·»åŠ é˜¶æ®µ"""
        # åˆ›å»ºé»˜è®¤é˜¶æ®µ
        stages = create_default_pipeline_stages(
            config=self.config,
            history=self.history,
            fetcher=self.fetcher,
            scorer=self.scorer,
            cache=self.ai_cache
        )
        
        # æ·»åŠ åˆ°æµæ°´çº¿
        for stage in stages:
            self.pipeline.add_stage(stage)
        
        logger.info(f"âœ“ æµæ°´çº¿å·²æ·»åŠ  {len(stages)} ä¸ªé˜¶æ®µ")
    
    async def _run_with_pipeline(self) -> bool:
        """ä½¿ç”¨æµæ°´çº¿è¿è¡Œ"""
        logger.info("ğŸš€ å¯åŠ¨å¼‚æ­¥æµæ°´çº¿å¤„ç†...")
        
        try:
            # åˆ›å»ºRSSæºè¿­ä»£å™¨
            async def rss_source_iterator() -> AsyncIterator[RSSSource]:
                """RSSæºå¼‚æ­¥è¿­ä»£å™¨"""
                for source in self.config.rss_sources:
                    if source.enabled:
                        logger.debug(f"å‘æµæ°´çº¿æä¾›æº: {source.name}")
                        yield source
                        await asyncio.sleep(0.1)  # å°å»¶è¿Ÿé¿å…é˜»å¡
            
            # è¿è¡Œæµæ°´çº¿
            results = []
            async for result in self.pipeline.run(rss_source_iterator()):
                results.append(result)
                logger.debug(f"æµæ°´çº¿äº§å‡ºç»“æœ: {result.get('item_count', 0)} æ¡æ–°é—»")
            
            # å¤„ç†ç»“æœ
            if results:
                final_result = results[-1]  # æœ€åä¸€ä¸ªç»“æœæ˜¯ç”Ÿæˆé˜¶æ®µçš„è¾“å‡º
                item_count = final_result.get('item_count', 0)
                
                if item_count > 0:
                    # æ›´æ–°ç»Ÿè®¡
                    self._update_pipeline_stats(results)
                    logger.info(f"âœ… æµæ°´çº¿å¤„ç†å®Œæˆ: ç”Ÿæˆ {item_count} æ¡æ–°é—»")
                    return True
                else:
                    logger.warning("âš ï¸ æµæ°´çº¿å¤„ç†å®Œæˆä½†æœªç”Ÿæˆæ–°é—»")
                    return False
            else:
                logger.warning("âš ï¸ æµæ°´çº¿æœªäº§å‡ºä»»ä½•ç»“æœ")
                return False
            
        except Exception as e:
            logger.error(f"âŒ æµæ°´çº¿è¿è¡Œå¤±è´¥: {e}", exc_info=True)
            return False
    
    async def _run_traditional(self) -> bool:
        """ä¼ ç»Ÿæ–¹å¼è¿è¡Œï¼ˆä¸ä½¿ç”¨æµæ°´çº¿ï¼‰"""
        logger.info("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼å¤„ç†...")
        
        try:
            # 1. è·å–RSSæ–°é—»
            with self.monitor.stage('rss_fetch', StageType.RSS_FETCH):
                all_items = self.fetcher.fetch_all()
                # è¿‡æ»¤å·²å¤„ç†çš„URL
                processed = self.history.get_processed_urls()
                news_items = [item for item in all_items if item.link not in processed]
            
            if not news_items:
                logger.warning("æœªè·å–åˆ°ä»»ä½•æ–°é—»")
                return False
            
            logger.info(f"ğŸ“¡ è·å– {len(news_items)} æ¡æ–°é—»")
            
            # 2. AIè¯„åˆ†å’Œç¿»è¯‘
            with self.monitor.stage('ai_scoring', StageType.AI_SCORING):
                scored_items = await self.scorer.score_all(news_items)
            
            logger.info(f"ğŸ¤– AIè¯„åˆ†å®Œæˆ: {len(scored_items)} æ¡")
            
            # 3. ç­›é€‰Top N
            threshold = self.config.filter_config.min_score_threshold
            filtered_items = [
                item for item in scored_items 
                if (item.ai_score or 0) >= threshold
            ]
            
            sorted_items = sorted(
                filtered_items,
                key=lambda x: (x.ai_score or 0, x.published_at),
                reverse=True
            )
            
            max_count = self.config.output_config.max_news_count
            top_items = sorted_items[:max_count]
            
            logger.info(f"ğŸ“‹ ç²¾é€‰Top {len(top_items)} æ¡æ–°é—»")
            
            # 4. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
            with self.monitor.stage('generate_output', StageType.GENERATE_OUTPUT):
                now = datetime.now()
                latest_path, archive_path = self.markdown_gen.generate(top_items, now)
                self.rss_gen.generate(top_items)
            
            logger.info(f"ğŸ“ è¾“å‡ºç”Ÿæˆå®Œæˆ: Markdown={latest_path}, RSS=feed.xml")
            
            # 5. æ›´æ–°ç»Ÿè®¡
            self._update_traditional_stats(news_items, top_items)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¼ ç»Ÿæ–¹å¼å¤„ç†å¤±è´¥: {e}", exc_info=True)
            return False
    
    def _update_pipeline_stats(self, results: List[dict]):
        """æ›´æ–°æµæ°´çº¿æ¨¡å¼çš„ç»Ÿè®¡"""
        # ä»ç»“æœä¸­æå–ä¿¡æ¯
        total_processed = 0
        generated_items = 0
        
        for result in results:
            if isinstance(result, dict):
                item_count = result.get('item_count', 0)
                if 'generated_at' in result:  # ç”Ÿæˆé˜¶æ®µçš„ç»“æœ
                    generated_items = item_count
                total_processed += item_count
        
        # æ›´æ–°å†å²
        run_time = datetime.now()
        
        # ç®€å•çš„æºç»Ÿè®¡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        source_stats = {source.name: 1 for source in self.config.rss_sources if source.enabled}
        
        self.history.update_stats(run_time, total_processed, source_stats)
        
        # ä¿å­˜
        self.history.save()
        
        # è¾“å‡ºç»Ÿè®¡
        stats = self.history.get_stats()
        logger.info(f"ğŸ“ˆ æ€»è¿è¡Œæ¬¡æ•°: {stats['total_runs']}")
        logger.info(f"ğŸ“ˆ æ€»å¤„ç†æ–°é—»: {stats['total_news_processed']}")
        logger.info(f"ğŸ“ˆ å¹³å‡æ¯æœŸ: {stats['avg_news_per_run']}")
        
        # æ‰“å°æµæ°´çº¿ç»Ÿè®¡
        if self.pipeline:
            self.pipeline.print_stats_summary()
    
    def _update_traditional_stats(self, all_items: List[NewsItem], selected_items: List[NewsItem]):
        """æ›´æ–°ä¼ ç»Ÿæ¨¡å¼çš„ç»Ÿè®¡"""
        # æºç»Ÿè®¡
        source_stats = {}
        for item in all_items:
            source_stats[item.source] = source_stats.get(item.source, 0) + 1
        
        # æ›´æ–°å†å²
        run_time = datetime.now()
        self.history.update_stats(run_time, len(all_items), source_stats)
        
        # è®°å½•å·²å¤„ç†çš„URL
        for item in all_items:
            self.history.add_processed(item.link)
        
        # æ›´æ–°æºé€‰ä¸­ç»Ÿè®¡
        for item in selected_items:
            self.history.update_source_selected(item.source, 1)
        
        # ä¿å­˜
        self.history.save()
        
        # è¾“å‡ºç»Ÿè®¡
        stats = self.history.get_stats()
        logger.info(f"ğŸ“ˆ æ€»è¿è¡Œæ¬¡æ•°: {stats['total_runs']}")
        logger.info(f"ğŸ“ˆ æ€»å¤„ç†æ–°é—»: {stats['total_news_processed']}")
        logger.info(f"ğŸ“ˆ å¹³å‡æ¯æœŸ: {stats['avg_news_per_run']}")
    
    def _print_performance_summary(self):
        """æ‰“å°æ€§èƒ½ç›‘æ§æ‘˜è¦"""
        if not self.monitor:
            return
        
        try:
            report = self.monitor.generate_report()
            
            logger.info("=" * 60)
            logger.info("ğŸ“Š æ€§èƒ½ç›‘æ§æ‘˜è¦")
            logger.info("=" * 60)
            logger.info(f"æ€»è€—æ—¶: {report['summary']['total_duration']:.2f}ç§’")
            logger.info(f"å¤„ç†æ–°é—»: {report['summary']['news_items_processed']}æ¡")
            logger.info(f"APIè°ƒç”¨: {report['summary']['total_api_calls']}æ¬¡")
            
            if report['summary']['total_tokens'] > 0:
                logger.info(f"Tokenä½¿ç”¨: {report['summary']['total_tokens']:,}")
            
            if report['summary']['cache_hits'] > 0 or report['summary']['cache_misses'] > 0:
                hit_rate = report['summary']['cache_hit_rate']
                hits = report['summary']['cache_hits']
                misses = report['summary']['cache_misses']
                logger.info(f"ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}% (å‘½ä¸­: {hits}, æœªå‘½ä¸­: {misses})")
            
            # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
            efficiency = report['efficiency']
            if efficiency['items_per_second'] > 0:
                logger.info(f"å¤„ç†é€Ÿåº¦: {efficiency['items_per_second']:.2f}æ¡/ç§’")
            
            if efficiency['api_calls_per_item'] > 0:
                logger.info(f"æ¯æ–°é—»APIè°ƒç”¨: {efficiency['api_calls_per_item']:.3f}")
            
            # é˜¶æ®µè€—æ—¶è¯¦æƒ…
            if report['stages']:
                logger.info(f"\né˜¶æ®µè€—æ—¶è¯¦æƒ…:")
                for name, data in report['stages'].items():
                    duration = data['total_duration']
                    logger.info(f"  {name}: {duration:.3f}ç§’ ({data['count']}æ¬¡)")
            
            logger.info("=" * 60)
            
        except Exception as e:
            logger.warning(f"æ€§èƒ½æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RSSæ–°é—»èšåˆå™¨ï¼ˆæµæ°´çº¿ç‰ˆæœ¬ï¼‰')
    parser.add_argument('--no-pipeline', action='store_true', 
                       help='ä¸ä½¿ç”¨æµæ°´çº¿ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼')
    parser.add_argument('--no-monitor', action='store_true',
                       help='ç¦ç”¨æ€§èƒ½ç›‘æ§')
    
    args = parser.parse_args()
    
    # åˆ›å»ºèšåˆå™¨
    aggregator = RSSPipelineAggregator(
        enable_monitoring=not args.no_monitor,
        use_pipeline=not args.no_pipeline
    )
    
    # è¿è¡Œ
    success = await aggregator.run()
    
    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)