"""
RSSè·å–æ¨¡å—
è´Ÿè´£ä»å¤šä¸ªRSSæºè·å–æ–°é—»å¹¶è§£æ
æ–°å¢ï¼šè¯­ä¹‰å»é‡(Semantic Deduplication)æ”¯æŒ
"""
import hashlib
import logging
import re
import socket
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import Optional

import feedparser
from dateutil import parser as date_parser

from src.models import NewsItem, RSSSource, OutputConfig, FilterConfig

logger = logging.getLogger(__name__)

# è®¾ç½®å…¨å±€socketè¶…æ—¶ï¼Œé˜²æ­¢RSSè·å–é˜»å¡ï¼ˆ10ç§’ï¼‰
socket.setdefaulttimeout(10)


class RSSFetcher:
    """RSSè·å–å™¨ - æ”¯æŒè¯­ä¹‰å»é‡"""
    
    def __init__(
        self, 
        sources: list[RSSSource], 
        output_config: OutputConfig, 
        filter_config: FilterConfig
    ):
        self.sources = sources
        self.output_config = output_config
        self.filter_config = filter_config
        self.time_window = timedelta(days=output_config.time_window_days)
        
        # è½»é‡çº§è¯­ä¹‰å»é‡é…ç½® (TF-IDFç‰ˆï¼ŒGitHub Actionså‹å¥½ï¼Œ~10MBå†…å­˜)
        self._semantic_dedup_enabled = getattr(filter_config, 'use_semantic_dedup', True)
        self._semantic_threshold = getattr(filter_config, 'semantic_similarity', 0.85)
        
        # TF-IDFå‘é‡åŒ–å™¨ (è½»é‡çº§æ›¿ä»£sentence-transformers)
        self._vectorizer = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.semantic_duplicates_removed = 0
    
    def _get_vectorizer(self):
        """å»¶è¿Ÿåˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨ (è½»é‡çº§ï¼ŒGitHub Actionså‹å¥½)"""
        if self._vectorizer is None and self._semantic_dedup_enabled:
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                
                logger.info("ğŸ“¦ åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨(è½»é‡çº§ï¼Œ~10MB)...")
                # è½»é‡çº§é…ç½®ï¼Œå†…å­˜å‹å¥½
                self._vectorizer = TfidfVectorizer(
                    max_features=500,       # é™åˆ¶ç‰¹å¾æ•°ï¼ŒèŠ‚çœå†…å­˜
                    ngram_range=(1, 2),     # å•è¯å’ŒåŒè¯ç»„åˆ
                    stop_words='english',   # ç§»é™¤è‹±æ–‡åœç”¨è¯
                    min_df=1,               # æœ€å°‘å‡ºç°1æ¬¡
                    max_df=0.95,            # å¿½ç•¥è¿‡äºå¸¸è§çš„è¯
                    lowercase=True,
                    strip_accents='unicode'
                )
                logger.info("âœ“ TF-IDFå‘é‡åŒ–å™¨åˆå§‹åŒ–å®Œæˆ (~10MB)")
            except Exception as e:
                logger.error(f"âŒ å‘é‡åŒ–å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œç¦ç”¨è¯­ä¹‰å»é‡: {e}")
                self._semantic_dedup_enabled = False
        
        return self._vectorizer
    
    def fetch_all(self) -> list[NewsItem]:
        """
        ä»æ‰€æœ‰æºè·å–æ–°é—»
        
        Returns:
            å»é‡åçš„æ–°é—»åˆ—è¡¨(æŒ‰å‘å¸ƒæ—¶é—´å€’åº)
        """
        all_items = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè·å–
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_source = {
                executor.submit(self._fetch_single, source): source 
                for source in self.sources
            }
            
            for future in as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    items = future.result(timeout=30)
                    all_items.extend(items)
                    logger.info(f"âœ“ æˆåŠŸä» {source.name} è·å– {len(items)} æ¡æ–°é—»")
                except Exception as e:
                    logger.error(f"âŒ ä» {source.name} è·å–å¤±è´¥: {e}")
        
        # å»é‡
        unique_items = self._deduplicate(all_items)
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åº(æœ€æ–°çš„åœ¨å‰)
        unique_items.sort(key=lambda x: x.published_at, reverse=True)
        
        logger.info(
            f"ğŸ“Š è·å–å®Œæˆ: åŸå§‹ {len(all_items)} æ¡ â†’ "
            f"å»é‡å {len(unique_items)} æ¡ "
            f"(è¯­ä¹‰å»é‡ {self.semantic_duplicates_removed} æ¡)"
        )
        return unique_items
    
    def _fetch_single(self, source: RSSSource, last_fetch_time: Optional[datetime] = None) -> List[NewsItem]:
        """
        è·å–å•ä¸ªRSSæºçš„æ–°é—»ï¼ˆæ”¯æŒåŸºäºæ—¶é—´èŠ‚ç‚¹çš„å¢é‡è·å–ï¼‰
        
        Args:
            source: RSSæºé…ç½®
            last_fetch_time: ä¸Šæ¬¡è·å–æ—¶é—´ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨time_window_daysä½œä¸ºfallback
        
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        items = []
        
        try:
            # è§£æRSS feed
            feed = feedparser.parse(source.url)
            
            if feed.bozo:  # è§£æè­¦å‘Š
                logger.warning(f"âš ï¸ {source.name} RSSè§£æè­¦å‘Š: {feed.bozo_exception}")
            
            # ç¡®å®šæ—¶é—´è¿‡æ»¤é˜ˆå€¼
            if last_fetch_time:
                # ä½¿ç”¨ä¸Šæ¬¡è·å–æ—¶é—´ï¼ˆå¢é‡æ¨¡å¼ï¼‰
                cutoff_time = last_fetch_time
                logger.info(f"â° {source.name} ä½¿ç”¨å¢é‡è·å–ï¼Œä¸Šæ¬¡æ—¶é—´: {cutoff_time}")
            else:
                # Fallback: ä½¿ç”¨time_window_daysï¼ˆå…¨é‡æ¨¡å¼ï¼‰
                cutoff_time = datetime.now() - self.time_window
                logger.info(f"â° {source.name} ä½¿ç”¨å…¨é‡è·å–ï¼Œæ—¶é—´çª—å£: {self.output_config.time_window_days}å¤©")
            
            for entry in feed.entries:
                try:
                    item = self._parse_entry(entry, source)
                    
                    # æ—¶é—´è¿‡æ»¤ï¼šåªä¿ç•™ cutoff_time ä¹‹åçš„æ–°é—»
                    if item.published_at > cutoff_time:
                        items.append(item)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£ææ¡ç›®å¤±è´¥: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"âŒ è·å–RSSæº {source.name} å¤±è´¥: {e}")
            raise
        
        return items
    
    def _parse_entry(self, entry, source: RSSSource) -> NewsItem:
        """å°†feedparser entryè§£æä¸ºNewsItem"""
        # è·å–æ ‡é¢˜
        title = entry.get('title', 'æ— æ ‡é¢˜').strip()
        
        # è·å–é“¾æ¥
        link = entry.get('link', '')
        if not link and 'links' in entry:
            for l in entry.links:
                if l.get('type') == 'text/html':
                    link = l.get('href', '')
                    break
        
        # è·å–å‘å¸ƒæ—¶é—´
        published = datetime.now()
        if 'published_parsed' in entry:
            published = datetime(*entry.published_parsed[:6])
        elif 'updated_parsed' in entry:
            published = datetime(*entry.updated_parsed[:6])
        elif 'published' in entry:
            try:
                published = date_parser.parse(entry.published)
            except:
                logger.debug(f"âš ï¸ {source.name} æ¡ç›®å‘å¸ƒæ—¶é—´è§£æå¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                pass
        
        # è¾¹ç•Œæƒ…å†µå¤„ç†ï¼šæ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨æœªæ¥
        if published > datetime.now():
            logger.warning(
                f"âš ï¸ {source.name} æ¡ç›®æ—¶é—´åœ¨æœªæ¥: {published}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´"
            )
            published = datetime.now()
        
        # è·å–æ‘˜è¦/å†…å®¹
        summary = entry.get('summary', '') or entry.get('description', '')
        content = entry.get('content', [{}])[0].get('value', '') if 'content' in entry else ''
        
        # ç”Ÿæˆå”¯ä¸€ID
        id_hash = hashlib.md5(f"{link}:{title}".encode()).hexdigest()[:12]
        
        return NewsItem(
            id=id_hash,
            title=title,
            link=link,
            source=source.name,
            category=source.category,
            published_at=published,
            summary=self._clean_html(summary),
            content=self._clean_html(content)
        )
    
    def _deduplicate(self, items: List[NewsItem]) -> List[NewsItem]:
        """
        çº¯è¯­ä¹‰å»é‡ - ç®€åŒ–æ¶æ„
        å»æ‰ä½æ•ˆçš„Levenshteinå­—ç¬¦çº§å»é‡ï¼Œä¿ç•™URLç²¾ç¡®å»é‡ + TF-IDFè¯­ä¹‰å»é‡
        ä¼˜åŠ¿ï¼šæ›´å¥½çš„ä¸­æ–‡æ”¯æŒã€æ›´é«˜çš„å»é‡å‡†ç¡®ç‡ã€æ›´ç®€æ´çš„ä»£ç 
        """
        import time
        start_time = time.time()
        
        if len(items) <= 1:
            return items
        
        # æ­¥éª¤1ï¼šURLç²¾ç¡®å»é‡ï¼ˆå¿«é€Ÿã€è½»é‡ã€å¿…è¦ï¼‰
        seen_urls = set()
        unique_by_url = []
        url_duplicates = 0
        
        for item in items:
            if item.link in seen_urls:
                url_duplicates += 1
                continue
            seen_urls.add(item.link)
            unique_by_url.append(item)
        
        if url_duplicates > 0:
            logger.debug(f"ğŸ”— URLå»é‡ç§»é™¤ {url_duplicates} æ¡")
        
        # æ­¥éª¤2ï¼šè¯­ä¹‰å»é‡ï¼ˆæ ¸å¿ƒé€»è¾‘ - ä½¿ç”¨TF-IDFå‘é‡åŒ– + ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        if len(unique_by_url) > 1:
            logger.info(f"ğŸ” è¯­ä¹‰å»é‡: {len(unique_by_url)} æ¡")
            
            # ç¡®ä¿å¯ç”¨è¯­ä¹‰å»é‡
            if not self._semantic_dedup_enabled:
                logger.warning("è¯­ä¹‰å»é‡è¢«ç¦ç”¨ï¼Œå¯ç”¨ä¸´æ—¶å‘é‡åŒ–å™¨")
                self._semantic_dedup_enabled = True
            
            final_items = self._semantic_deduplicate(unique_by_url)
        else:
            final_items = unique_by_url
        
        # æ€§èƒ½ç›‘æ§
        elapsed = time.time() - start_time
        total_removed = len(items) - len(final_items)
        logger.info(
            f"âœ… å»é‡å®Œæˆ: {len(items)}æ¡ â†’ {len(final_items)}æ¡ "
            f"(ç§»é™¤{total_removed}æ¡, è€—æ—¶{elapsed:.2f}ç§’)"
        )
        
        return final_items
    
    def _semantic_deduplicate(self, items: List[NewsItem]) -> List[NewsItem]:
        """
        è½»é‡çº§è¯­ä¹‰å»é‡ - ä½¿ç”¨TF-IDF (GitHub Actionså‹å¥½ï¼Œ~10MBå†…å­˜)
        è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼ä½†è¡¨è¿°ä¸åŒçš„æ ‡é¢˜
        """
        vectorizer = self._get_vectorizer()
        if vectorizer is None:
            return items
        
        try:
            from sklearn.metrics.pairwise import cosine_similarity
            import numpy as np
            
            # å‡†å¤‡æ–‡æœ¬ (æ ‡é¢˜ + æ‘˜è¦å‰100å­—)
            texts = []
            for item in items:
                text = f"{item.title} {item.summary[:100]}"
                texts.append(text.lower())
            
            # TF-IDFç¼–ç  (å†…å­˜å‹å¥½)
            logger.info(f"ğŸ§® TF-IDFç¼–ç  {len(texts)} æ¡æ–°é—»...")
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # èšç±»å»é‡
            unique_items = []
            processed_indices = set()
            semantic_duplicates = 0
            
            for i, item in enumerate(items):
                if i in processed_indices:
                    continue
                
                # æ‰¾åˆ°æ‰€æœ‰è¯­ä¹‰ç›¸ä¼¼çš„æ–°é—»
                similar_indices = [
                    j for j in range(len(items))
                    if similarity_matrix[i][j] > self._semantic_threshold
                    and j != i and j not in processed_indices
                ]
                
                if similar_indices:
                    logger.debug(
                        f"ğŸ¯ TF-IDFå»é‡: '{item.title[:40]}...' "
                        f"ä¸ {len(similar_indices)} æ¡ç›¸ä¼¼"
                    )
                    semantic_duplicates += len(similar_indices)
                
                # ä¿ç•™ç¬¬ä¸€æ¡ï¼Œæ ‡è®°å…¶ä½™ä¸ºé‡å¤
                unique_items.append(item)
                processed_indices.add(i)
                processed_indices.update(similar_indices)
            
            self.semantic_duplicates_removed = semantic_duplicates
            
            logger.info(
                f"âœ“ TF-IDFè¯­ä¹‰å»é‡å®Œæˆ: {len(items)} â†’ {len(unique_items)} æ¡ "
                f"(å»é™¤ {semantic_duplicates} æ¡è¯­ä¹‰é‡å¤)"
            )
            
            return unique_items
            
        except Exception as e:
            logger.error(f"âŒ TF-IDFè¯­ä¹‰å»é‡å¤±è´¥: {e}")
            return items  # å¤±è´¥æ—¶è¿”å›åŸå§‹åˆ—è¡¨
    
    def _clean_html(self, html: str) -> str:
        """ç®€å•æ¸…ç†HTMLæ ‡ç­¾"""
        if not html:
            return ""
        # ç§»é™¤scriptå’Œstyleæ ‡ç­¾åŠå…¶å†…å®¹
        html = re.sub(r'<(script|style)[^>]*>[^<]*</\1>', '', html, flags=re.DOTALL)
        # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾
        html = re.sub(r'<[^>]+>', '', html)
        # è§£ç HTMLå®ä½“
        html = html.replace('&lt;', '<').replace('&gt;', '>')
        html = html.replace('&amp;', '&').replace('&quot;', '"')
        html = html.replace('&#39;', "'").replace('&nbsp;', ' ')
        return html.strip()
    
    def get_stats(self) -> dict:
        """è·å–å»é‡ç»Ÿè®¡"""
        return {
            "semantic_dedup_enabled": self._semantic_dedup_enabled,
            "semantic_threshold": self._semantic_threshold,
            "semantic_duplicates_removed": self.semantic_duplicates_removed
        }
