"""
RSSè·å–æ¨¡å—
è´Ÿè´£ä»å¤šä¸ªRSSæºè·å–æ–°é—»å¹¶è§£æ
æ–°å¢ï¼šè¯­ä¹‰å»é‡(Semantic Deduplication)æ”¯æŒ
"""
import hashlib
import logging
import re
import socket
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import List, Optional

import feedparser
from dateutil import parser as date_parser

from src.models import NewsItem, RSSSource, OutputConfig, FilterConfig

logger = logging.getLogger(__name__)

# è®¾ç½®å…¨å±€socketè¶…æ—¶ï¼Œé˜²æ­¢RSSè·å–é˜»å¡ï¼ˆ10ç§’ï¼‰
socket.setdefaulttimeout(10)


class RSSFetcher:
    """RSSè·å–å™¨ - æ”¯æŒè¯­ä¹‰å»é‡"""
    
    def __init__(
        self, 
        sources: List[RSSSource], 
        output_config: OutputConfig, 
        filter_config: FilterConfig
    ):
        self.sources = sources
        self.output_config = output_config
        self.filter_config = filter_config
        self.time_window = timedelta(days=output_config.time_window_days)
        
        # è½»é‡çº§è¯­ä¹‰å»é‡é…ç½® (TF-IDFç‰ˆï¼ŒGitHub Actionså‹å¥½ï¼Œ~10MBå†…å­˜)
        self._semantic_dedup_enabled = getattr(filter_config, 'use_semantic_dedup', True)
        self._semantic_threshold = getattr(filter_config, 'semantic_similarity', 0.85)
        
        # TF-IDFå‘é‡åŒ–å™¨ (è½»é‡çº§æ›¿ä»£sentence-transformers)
        self._vectorizer = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.semantic_duplicates_removed = 0
    
    def _get_vectorizer(self):
        """å»¶è¿Ÿåˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨ (è½»é‡çº§ï¼ŒGitHub Actionså‹å¥½)"""
        if self._vectorizer is None and self._semantic_dedup_enabled:
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                
                logger.info("ğŸ“¦ åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨(è½»é‡çº§ï¼Œ~10MB)...")
                # è½»é‡çº§é…ç½®ï¼Œå†…å­˜å‹å¥½
                self._vectorizer = TfidfVectorizer(
                    max_features=500,       # é™åˆ¶ç‰¹å¾æ•°ï¼ŒèŠ‚çœå†…å­˜
                    ngram_range=(1, 2),     # å•è¯å’ŒåŒè¯ç»„åˆ
                    stop_words='english',   # ç§»é™¤è‹±æ–‡åœç”¨è¯
                    min_df=1,               # æœ€å°‘å‡ºç°1æ¬¡
                    max_df=0.95,            # å¿½ç•¥è¿‡äºå¸¸è§çš„è¯
                    lowercase=True,
                    strip_accents='unicode'
                )
                logger.info("âœ“ TF-IDFå‘é‡åŒ–å™¨åˆå§‹åŒ–å®Œæˆ (~10MB)")
            except Exception as e:
                logger.error(f"âŒ å‘é‡åŒ–å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œç¦ç”¨è¯­ä¹‰å»é‡: {e}")
                self._semantic_dedup_enabled = False
        
        return self._vectorizer
    
    def fetch_all(self) -> List[NewsItem]:
        """
        ä»æ‰€æœ‰æºè·å–æ–°é—»
        
        Returns:
            å»é‡åçš„æ–°é—»åˆ—è¡¨(æŒ‰å‘å¸ƒæ—¶é—´å€’åº)
        """
        all_items = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè·å–
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_source = {
                executor.submit(self._fetch_single, source): source 
                for source in self.sources
            }
            
            for future in as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    items = future.result(timeout=30)
                    all_items.extend(items)
                    logger.info(f"âœ“ æˆåŠŸä» {source.name} è·å– {len(items)} æ¡æ–°é—»")
                except Exception as e:
                    logger.error(f"âŒ ä» {source.name} è·å–å¤±è´¥: {e}")
        
        # å»é‡
        unique_items = self._deduplicate(all_items)
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åº(æœ€æ–°çš„åœ¨å‰)
        unique_items.sort(key=lambda x: x.published_at, reverse=True)
        
        logger.info(
            f"ğŸ“Š è·å–å®Œæˆ: åŸå§‹ {len(all_items)} æ¡ â†’ "
            f"å»é‡å {len(unique_items)} æ¡ "
            f"(è¯­ä¹‰å»é‡ {self.semantic_duplicates_removed} æ¡)"
        )
        return unique_items
    
    def _fetch_single(self, source: RSSSource) -> List[NewsItem]:
        """è·å–å•ä¸ªRSSæºçš„æ–°é—»"""
        items = []
        
        try:
            # è§£æRSS feed
            feed = feedparser.parse(source.url)
            
            if feed.bozo:  # è§£æè­¦å‘Š
                logger.warning(f"âš ï¸ {source.name} RSSè§£æè­¦å‘Š: {feed.bozo_exception}")
            
            # è·å–å½“å‰æ—¶é—´çª—å£
            cutoff_time = datetime.now() - self.time_window
            
            for entry in feed.entries:
                try:
                    item = self._parse_entry(entry, source)
                    
                    # åªä¿ç•™æ—¶é—´çª—å£å†…çš„æ–°é—»
                    if item.published_at > cutoff_time:
                        items.append(item)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£ææ¡ç›®å¤±è´¥: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"âŒ è·å–RSSæº {source.name} å¤±è´¥: {e}")
            raise
        
        return items
    
    def _parse_entry(self, entry, source: RSSSource) -> NewsItem:
        """å°†feedparser entryè§£æä¸ºNewsItem"""
        # è·å–æ ‡é¢˜
        title = entry.get('title', 'æ— æ ‡é¢˜').strip()
        
        # è·å–é“¾æ¥
        link = entry.get('link', '')
        if not link and 'links' in entry:
            for l in entry.links:
                if l.get('type') == 'text/html':
                    link = l.get('href', '')
                    break
        
        # è·å–å‘å¸ƒæ—¶é—´
        published = datetime.now()
        if 'published_parsed' in entry:
            published = datetime(*entry.published_parsed[:6])
        elif 'updated_parsed' in entry:
            published = datetime(*entry.updated_parsed[:6])
        elif 'published' in entry:
            try:
                published = date_parser.parse(entry.published)
            except:
                pass
        
        # è·å–æ‘˜è¦/å†…å®¹
        summary = entry.get('summary', '') or entry.get('description', '')
        content = entry.get('content', [{}])[0].get('value', '') if 'content' in entry else ''
        
        # ç”Ÿæˆå”¯ä¸€ID
        id_hash = hashlib.md5(f"{link}:{title}".encode()).hexdigest()[:12]
        
        return NewsItem(
            id=id_hash,
            title=title,
            link=link,
            source=source.name,
            category=source.category,
            published_at=published,
            summary=self._clean_html(summary),
            content=self._clean_html(content)
        )
    
    def _deduplicate(self, items: List[NewsItem]) -> List[NewsItem]:
        """
        å»é‡ - ä¸¤é˜¶æ®µå»é‡ç­–ç•¥
        é˜¶æ®µ1: URL + Levenshtein (å¿«é€Ÿå»é‡)
        é˜¶æ®µ2: è¯­ä¹‰ç›¸ä¼¼åº¦ (ç²¾å‡†å»é‡ï¼Œå¯é€‰)
        """
        # é˜¶æ®µ1: å¿«é€Ÿå»é‡
        unique_items = self._fast_dedup(items)
        
        # é˜¶æ®µ2: è¯­ä¹‰å»é‡ (å¦‚æœå¯ç”¨)
        if self._semantic_dedup_enabled and len(unique_items) > 1:
            logger.info(f"ğŸ” å¯åŠ¨è¯­ä¹‰å»é‡æ£€æŸ¥: {len(unique_items)} æ¡")
            unique_items = self._semantic_deduplicate(unique_items)
        
        return unique_items
    
    def _fast_dedup(self, items: List[NewsItem]) -> List[NewsItem]:
        """å¿«é€Ÿå»é‡ - åŸºäºURLå’ŒLevenshteinè·ç¦»"""
        seen_urls = set()
        seen_titles = []
        unique_items = []
        
        threshold = self.filter_config.dedup_similarity
        
        for item in items:
            # URLå»é‡
            if item.link in seen_urls:
                continue
            
            # æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡
            is_duplicate = False
            for seen_title in seen_titles:
                similarity = self._title_similarity(item.title, seen_title)
                if similarity >= threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                seen_urls.add(item.link)
                seen_titles.append(item.title)
                unique_items.append(item)
        
        return unique_items
    
    def _semantic_deduplicate(self, items: List[NewsItem]) -> List[NewsItem]:
        """
        è½»é‡çº§è¯­ä¹‰å»é‡ - ä½¿ç”¨TF-IDF (GitHub Actionså‹å¥½ï¼Œ~10MBå†…å­˜)
        è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼ä½†è¡¨è¿°ä¸åŒçš„æ ‡é¢˜
        """
        vectorizer = self._get_vectorizer()
        if vectorizer is None:
            return items
        
        try:
            from sklearn.metrics.pairwise import cosine_similarity
            import numpy as np
            
            # å‡†å¤‡æ–‡æœ¬ (æ ‡é¢˜ + æ‘˜è¦å‰100å­—)
            texts = []
            for item in items:
                text = f"{item.title} {item.summary[:100]}"
                texts.append(text.lower())
            
            # TF-IDFç¼–ç  (å†…å­˜å‹å¥½)
            logger.info(f"ğŸ§® TF-IDFç¼–ç  {len(texts)} æ¡æ–°é—»...")
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # èšç±»å»é‡
            unique_items = []
            processed_indices = set()
            semantic_duplicates = 0
            
            for i, item in enumerate(items):
                if i in processed_indices:
                    continue
                
                # æ‰¾åˆ°æ‰€æœ‰è¯­ä¹‰ç›¸ä¼¼çš„æ–°é—»
                similar_indices = [
                    j for j in range(len(items))
                    if similarity_matrix[i][j] > self._semantic_threshold
                    and j != i and j not in processed_indices
                ]
                
                if similar_indices:
                    logger.debug(
                        f"ğŸ¯ TF-IDFå»é‡: '{item.title[:40]}...' "
                        f"ä¸ {len(similar_indices)} æ¡ç›¸ä¼¼"
                    )
                    semantic_duplicates += len(similar_indices)
                
                # ä¿ç•™ç¬¬ä¸€æ¡ï¼Œæ ‡è®°å…¶ä½™ä¸ºé‡å¤
                unique_items.append(item)
                processed_indices.add(i)
                processed_indices.update(similar_indices)
            
            self.semantic_duplicates_removed = semantic_duplicates
            
            logger.info(
                f"âœ“ TF-IDFè¯­ä¹‰å»é‡å®Œæˆ: {len(items)} â†’ {len(unique_items)} æ¡ "
                f"(å»é™¤ {semantic_duplicates} æ¡è¯­ä¹‰é‡å¤)"
            )
            
            return unique_items
            
        except Exception as e:
            logger.error(f"âŒ TF-IDFè¯­ä¹‰å»é‡å¤±è´¥: {e}")
            return items  # å¤±è´¥æ—¶è¿”å›åŸå§‹åˆ—è¡¨
    
    def _title_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦(åŸºäºLevenshteinè·ç¦»)"""
        title1 = title1.lower().strip()
        title2 = title2.lower().strip()
        
        if title1 == title2:
            return 1.0
        
        len1, len2 = len(title1), len(title2)
        if len1 == 0 or len2 == 0:
            return 0.0
        
        max_len = max(len1, len2)
        distance = self._levenshtein_distance(title1, title2)
        similarity = 1 - (distance / max_len)
        
        return similarity
    
    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """è®¡ç®—Levenshteinç¼–è¾‘è·ç¦»"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    def _clean_html(self, html: str) -> str:
        """ç®€å•æ¸…ç†HTMLæ ‡ç­¾"""
        if not html:
            return ""
        # ç§»é™¤scriptå’Œstyleæ ‡ç­¾åŠå…¶å†…å®¹
        html = re.sub(r'<(script|style)[^>]*>[^<]*</\1>', '', html, flags=re.DOTALL)
        # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾
        html = re.sub(r'<[^>]+>', '', html)
        # è§£ç HTMLå®ä½“
        html = html.replace('&lt;', '<').replace('&gt;', '>')
        html = html.replace('&amp;', '&').replace('&quot;', '"')
        html = html.replace('&#39;', "'").replace('&nbsp;', ' ')
        return html.strip()
    
    def get_stats(self) -> dict:
        """è·å–å»é‡ç»Ÿè®¡"""
        return {
            "semantic_dedup_enabled": self._semantic_dedup_enabled,
            "semantic_threshold": self._semantic_threshold,
            "semantic_duplicates_removed": self.semantic_duplicates_removed
        }
