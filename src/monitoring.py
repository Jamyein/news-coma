"""
æ€§èƒ½ç›‘æ§ä¸æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ
ç”¨äºRSSæ–°é—»èšåˆé¡¹ç›®çš„æ€§èƒ½ç›‘æ§ï¼Œé‡åŒ–ä¼˜åŒ–å‰åçš„æ€§èƒ½æŒ‡æ ‡
"""
import json
import time
import asyncio
from pathlib import Path
from datetime import datetime, timedelta
from contextlib import contextmanager
from typing import Dict, List, Optional, Callable, Any, Union
import threading
from dataclasses import dataclass, field, asdict
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class StageType(Enum):
    """æ€§èƒ½ç›‘æ§é˜¶æ®µç±»å‹"""
    RSS_FETCH = "rss_fetch"
    AI_SCORING = "ai_scoring"
    GENERATE_OUTPUT = "generate_output"
    CACHE_LOOKUP = "cache_lookup"
    API_CALL = "api_call"
    CUSTOM = "custom"


@dataclass
class StageMetrics:
    """é˜¶æ®µæ€§èƒ½æŒ‡æ ‡"""
    name: str
    duration: float = 0.0
    start_time: float = 0.0
    end_time: float = 0.0
    success: bool = True
    error_message: Optional[str] = None
    custom_data: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        data = asdict(self)
        data['stage_type'] = self.name
        return data


@dataclass
class CounterMetrics:
    """è®¡æ•°å™¨æŒ‡æ ‡"""
    api_calls: int = 0
    tokens_input: int = 0
    tokens_output: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    errors: int = 0
    news_items_processed: int = 0
    
    def increment(self, counter_name: str, value: int = 1):
        """å¢åŠ è®¡æ•°å™¨å€¼"""
        if hasattr(self, counter_name):
            current = getattr(self, counter_name)
            setattr(self, counter_name, current + value)


class StageTimer:
    """é˜¶æ®µè®¡æ—¶å™¨ï¼Œæ”¯æŒé«˜ç²¾åº¦è®¡æ—¶"""
    
    def __init__(self, name: str, stage_type: StageType = StageType.CUSTOM):
        self.name = name
        self.stage_type = stage_type
        self.start_time: Optional[float] = None
        self.end_time: Optional[float] = None
        self.duration: Optional[float] = None
        self.error: Optional[str] = None
    
    def __enter__(self):
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end()
        if exc_val is not None:
            self.error = str(exc_val)
            return False
        return True
    
    async def __aenter__(self):
        self.start()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        self.end()
        if exc_val is not None:
            self.error = str(exc_val)
            return False
        return True
    
    def start(self):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.perf_counter()
        self.end_time = None
        self.duration = None
        self.error = None
    
    def end(self):
        """ç»“æŸè®¡æ—¶"""
        if self.start_time is not None:
            self.end_time = time.perf_counter()
            self.duration = self.end_time - self.start_time
    
    def get_metrics(self) -> StageMetrics:
        """è·å–é˜¶æ®µæŒ‡æ ‡"""
        return StageMetrics(
            name=self.name,
            duration=self.duration or 0.0,
            start_time=self.start_time or 0.0,
            end_time=self.end_time or 0.0,
            success=self.error is None,
            error_message=self.error,
            custom_data={'stage_type': self.stage_type.value}
        )


class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨ï¼Œæ”¯æŒçº¿ç¨‹å®‰å…¨æ“ä½œ"""
    
    def __init__(self):
        self._lock = threading.Lock()
        self._counters = CounterMetrics()
        self._stages: Dict[str, List[StageMetrics]] = {}
        self._custom_metrics: Dict[str, Any] = {}
    
    def increment_counter(self, name: str, value: int = 1):
        """å¢åŠ è®¡æ•°å™¨å€¼"""
        with self._lock:
            self._counters.increment(name, value)
    
    def add_stage_metrics(self, stage_metrics: StageMetrics):
        """æ·»åŠ é˜¶æ®µæŒ‡æ ‡"""
        with self._lock:
            if stage_metrics.name not in self._stages:
                self._stages[stage_metrics.name] = []
            self._stages[stage_metrics.name].append(stage_metrics)
    
    def set_custom_metric(self, name: str, value: Any):
        """è®¾ç½®è‡ªå®šä¹‰æŒ‡æ ‡"""
        with self._lock:
            self._custom_metrics[name] = value
    
    def get_counters(self) -> CounterMetrics:
        """è·å–è®¡æ•°å™¨æŒ‡æ ‡"""
        with self._lock:
            return self._counters
    
    def get_stages(self) -> Dict[str, List[StageMetrics]]:
        """è·å–æ‰€æœ‰é˜¶æ®µæŒ‡æ ‡"""
        with self._lock:
            return self._stages.copy()
    
    def get_custom_metrics(self) -> Dict[str, Any]:
        """è·å–è‡ªå®šä¹‰æŒ‡æ ‡"""
        with self._lock:
            return self._custom_metrics.copy()
    
    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰æŒ‡æ ‡"""
        with self._lock:
            self._counters = CounterMetrics()
            self._stages = {}
            self._custom_metrics = {}


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨ - ä¸»å…¥å£"""
    
    def __init__(
        self,
        output_dir: Union[str, Path] = "metrics",
        enable_logging: bool = True,
        auto_save: bool = True,
        callback: Optional[Callable[[Dict[str, Any]], None]] = None
    ):
        """
        åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            enable_logging: æ˜¯å¦å¯ç”¨æ—¥å¿—
            auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜æŠ¥å‘Š
            callback: å®æ—¶ç›‘æ§å›è°ƒå‡½æ•°
        """
        self.output_dir = Path(output_dir)
        self.enable_logging = enable_logging
        self.auto_save = auto_save
        self.callback = callback
        
        # åˆ›å»ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "history").mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨
        self.collector = MetricsCollector()
        
        # è¿è¡ŒçŠ¶æ€
        self._start_time: Optional[float] = None
        self._end_time: Optional[float] = None
        self._is_running = False
        
        if enable_logging:
            logger.info(f"æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def start(self):
        """å¼€å§‹ç›‘æ§"""
        self._start_time = time.perf_counter()
        self._end_time = None
        self._is_running = True
        
        if self.enable_logging:
            logger.info("ğŸ” æ€§èƒ½ç›‘æ§å¼€å§‹")
        
        return self
    
    def end(self):
        """ç»“æŸç›‘æ§"""
        if not self._is_running:
            return
        
        self._end_time = time.perf_counter()
        self._is_running = False
        
        if self.enable_logging:
            total_duration = self._end_time - self._start_time
            logger.info(f"ğŸ”š æ€§èƒ½ç›‘æ§ç»“æŸï¼Œæ€»è€—æ—¶: {total_duration:.2f}s")
        
        # è‡ªåŠ¨ä¿å­˜æŠ¥å‘Š
        if self.auto_save:
            self.save_report()
    
    @contextmanager
    def stage(self, name: str, stage_type: StageType = StageType.CUSTOM):
        """
        é˜¶æ®µè®¡æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        ç”¨æ³•:
            with monitor.stage('ai_scoring'):
                await scorer.score_all(items)
        
        Args:
            name: é˜¶æ®µåç§°
            stage_type: é˜¶æ®µç±»å‹
        """
        timer = StageTimer(name, stage_type)
        
        try:
            timer.start()
            if self.enable_logging:
                logger.debug(f"â±ï¸ å¼€å§‹é˜¶æ®µ: {name}")
            
            yield self
            
            timer.end()
            if self.enable_logging:
                logger.debug(f"âœ… ç»“æŸé˜¶æ®µ: {name}, è€—æ—¶: {timer.duration:.2f}s")
            
            # æ”¶é›†æŒ‡æ ‡
            self.collector.add_stage_metrics(timer.get_metrics())
            
        except Exception as e:
            timer.error = str(e)
            timer.end()
            
            # æ”¶é›†é”™è¯¯æŒ‡æ ‡
            self.collector.add_stage_metrics(timer.get_metrics())
            self.collector.increment_counter('errors')
            
            if self.enable_logging:
                logger.error(f"âŒ é˜¶æ®µå¤±è´¥: {name}, é”™è¯¯: {e}")
            
            raise
    
    async def astage(self, name: str, stage_type: StageType = StageType.CUSTOM):
        """å¼‚æ­¥é˜¶æ®µè®¡æ—¶å™¨"""
        timer = StageTimer(name, stage_type)
        
        try:
            timer.start()
            if self.enable_logging:
                logger.debug(f"â±ï¸ å¼€å§‹é˜¶æ®µ(å¼‚æ­¥): {name}")
            
            yield self
            
            timer.end()
            if self.enable_logging:
                logger.debug(f"âœ… ç»“æŸé˜¶æ®µ(å¼‚æ­¥): {name}, è€—æ—¶: {timer.duration:.2f}s")
            
            # æ”¶é›†æŒ‡æ ‡
            self.collector.add_stage_metrics(timer.get_metrics())
            
        except Exception as e:
            timer.error = str(e)
            timer.end()
            
            # æ”¶é›†é”™è¯¯æŒ‡æ ‡
            self.collector.add_stage_metrics(timer.get_metrics())
            self.collector.increment_counter('errors')
            
            if self.enable_logging:
                logger.error(f"âŒ é˜¶æ®µå¤±è´¥(å¼‚æ­¥): {name}, é”™è¯¯: {e}")
            
            raise
    
    def increment(self, counter: str, value: int = 1):
        """
        å¢åŠ è®¡æ•°å™¨
        
        Args:
            counter: è®¡æ•°å™¨åç§°
            value: å¢åŠ å€¼
        """
        self.collector.increment_counter(counter, value)
        
        # è§¦å‘å›è°ƒ
        if self.callback:
            self._trigger_callback({
                'type': 'counter_update',
                'counter': counter,
                'value': value,
                'timestamp': datetime.now().isoformat()
            })
    
    def record_api_call(self, tokens_input: int = 0, tokens_output: int = 0):
        """
        è®°å½•APIè°ƒç”¨
        
        Args:
            tokens_input: è¾“å…¥tokenæ•°
            tokens_output: è¾“å‡ºtokenæ•°
        """
        self.increment('api_calls')
        if tokens_input > 0:
            self.increment('tokens_input', tokens_input)
        if tokens_output > 0:
            self.increment('tokens_output', tokens_output)
    
    def record_cache_hit(self):
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        self.increment('cache_hits')
    
    def record_cache_miss(self):
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        self.increment('cache_misses')
    
    def set_custom_metric(self, name: str, value: Any):
        """
        è®¾ç½®è‡ªå®šä¹‰æŒ‡æ ‡
        
        Args:
            name: æŒ‡æ ‡åç§°
            value: æŒ‡æ ‡å€¼
        """
        self.collector.set_custom_metric(name, value)
        
        # è§¦å‘å›è°ƒ
        if self.callback:
            self._trigger_callback({
                'type': 'custom_metric',
                'name': name,
                'value': value,
                'timestamp': datetime.now().isoformat()
            })
    
    def generate_report(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆå®Œæ•´æ€§èƒ½æŠ¥å‘Š
        
        Returns:
            åŒ…å«æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        # è·å–æŒ‡æ ‡æ•°æ®
        counters = self.collector.get_counters()
        stages = self.collector.get_stages()
        custom_metrics = self.collector.get_custom_metrics()
        
        # è®¡ç®—æ€»è€—æ—¶
        total_duration = 0.0
        if self._start_time and self._end_time:
            total_duration = self._end_time - self._start_time
        
        # èšåˆé˜¶æ®µæ•°æ®
        stage_summary = {}
        for stage_name, stage_list in stages.items():
            if stage_list:
                total_stage_duration = sum(s.duration for s in stage_list)
                avg_duration = total_stage_duration / len(stage_list)
                success_rate = sum(1 for s in stage_list if s.success) / len(stage_list) * 100
                
                stage_summary[stage_name] = {
                    'count': len(stage_list),
                    'total_duration': round(total_stage_duration, 3),
                    'avg_duration': round(avg_duration, 3),
                    'min_duration': round(min(s.duration for s in stage_list), 3),
                    'max_duration': round(max(s.duration for s in stage_list), 3),
                    'success_rate': round(success_rate, 1)
                }
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        total_cache_operations = counters.cache_hits + counters.cache_misses
        cache_hit_rate = 0.0
        if total_cache_operations > 0:
            cache_hit_rate = counters.cache_hits / total_cache_operations
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        api_calls_per_item = 0.0
        if counters.news_items_processed > 0:
            api_calls_per_item = counters.api_calls / counters.news_items_processed
        
        tokens_per_api_call = 0.0
        if counters.api_calls > 0:
            tokens_per_api_call = (counters.tokens_input + counters.tokens_output) / counters.api_calls
        
        # æ„å»ºæŠ¥å‘Š
        report = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'start_time': datetime.fromtimestamp(self._start_time).isoformat() if self._start_time else None,
                'end_time': datetime.fromtimestamp(self._end_time).isoformat() if self._end_time else None,
                'total_duration': round(total_duration, 3)
            },
            'summary': {
                'total_duration': round(total_duration, 3),
                'total_stages': len(stages),
                'total_api_calls': counters.api_calls,
                'total_tokens': counters.tokens_input + counters.tokens_output,
                'tokens_input': counters.tokens_input,
                'tokens_output': counters.tokens_output,
                'cache_hit_rate': round(cache_hit_rate * 100, 2),
                'total_cache_operations': total_cache_operations,
                'cache_hits': counters.cache_hits,
                'cache_misses': counters.cache_misses,
                'error_count': counters.errors,
                'news_items_processed': counters.news_items_processed
            },
            'efficiency': {
                'api_calls_per_item': round(api_calls_per_item, 3),
                'tokens_per_api_call': round(tokens_per_api_call, 2),
                'items_per_second': round(counters.news_items_processed / total_duration, 2) if total_duration > 0 else 0,
                'tokens_per_second': round((counters.tokens_input + counters.tokens_output) / total_duration, 2) if total_duration > 0 else 0
            },
            'stages': stage_summary,
            'custom_metrics': custom_metrics
        }
        
        # è§¦å‘å›è°ƒ
        if self.callback:
            self._trigger_callback({
                'type': 'report_generated',
                'report': report,
                'timestamp': datetime.now().isoformat()
            })
        
        return report
    
    def save_report(self, filename: Optional[str] = None) -> Path:
        """
        ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            filename: è‡ªå®šä¹‰æ–‡ä»¶åï¼Œå¦‚ä¸æŒ‡å®šåˆ™ä½¿ç”¨æ—¶é—´æˆ³
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        report = self.generate_report()
        
        # ç”Ÿæˆæ–‡ä»¶å
        if filename is None:
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            filename = f"{timestamp}.json"
        
        # ä¿å­˜å†å²æŠ¥å‘Š
        history_path = self.output_dir / "history" / filename
        history_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # åŒæ—¶ä¿å­˜ä¸ºå½“å‰æŠ¥å‘Š
        latest_path = self.output_dir / "performance.json"
        with open(latest_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ›´æ–°è¶‹åŠ¿æ•°æ®
        self._update_trends(report)
        
        if self.enable_logging:
            logger.info(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {history_path}")
        
        return history_path
    
    def compare_with_history(self, history_file: Optional[str] = None) -> Dict[str, Any]:
        """
        ä¸å†å²æŠ¥å‘Šå¯¹æ¯”
        
        Args:
            history_file: å†å²æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼Œå¦‚ä¸æŒ‡å®šåˆ™æŸ¥æ‰¾æœ€è¿‘çš„
            
        Returns:
            å¯¹æ¯”ç»“æœ
        """
        if history_file is None:
            # æŸ¥æ‰¾æœ€è¿‘çš„å†å²æŠ¥å‘Š
            history_dir = self.output_dir / "history"
            if history_dir.exists():
                files = sorted(history_dir.glob("*.json"), reverse=True)
                if len(files) > 1:
                    history_file = files[1]  # å€’æ•°ç¬¬äºŒä¸ª(æœ€æ–°çš„æ˜¯å½“å‰)
        
        if not history_file or not Path(history_file).exists():
            return {"error": "æœªæ‰¾åˆ°å¯å¯¹æ¯”çš„å†å²æŠ¥å‘Šæ–‡ä»¶"}
        
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
        except Exception as e:
            return {"error": f"å†å²æŠ¥å‘Šè¯»å–å¤±è´¥: {e}"}
        
        current = self.generate_report()
        
        comparison = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'current_report': current['metadata']['timestamp'],
                'compared_with': Path(history_file).name,
                'comparison_date': history['metadata']['timestamp']
            },
            'improvements': {},
            'regressions': {},
            'unchanged': {}
        }
        
        # å¯¹æ¯”å…³é”®æŒ‡æ ‡
        metrics_to_compare = [
            ('total_duration', 'æ€»è€—æ—¶', 's', 'lower'),
            ('api_calls_per_item', 'æ¯æ–°é—»APIè°ƒç”¨æ•°', '', 'lower'),
            ('cache_hit_rate', 'ç¼“å­˜å‘½ä¸­ç‡', '%', 'higher'),
            ('items_per_second', 'æ¯ç§’å¤„ç†æ–°é—»æ•°', 'ä¸ª/s', 'higher')
        ]
        
        for key, label, unit, better in metrics_to_compare:
            curr_val = None
            hist_val = None
            
            # ä»ä¸åŒä½ç½®è·å–å€¼
            if key in current['summary']:
                curr_val = current['summary'][key]
                hist_val = history['summary'].get(key, 0)
            elif key in current['efficiency']:
                curr_val = current['efficiency'][key]
                hist_val = history['efficiency'].get(key, 0)
            
            if curr_val is None or hist_val is None:
                continue
            
            if curr_val == hist_val:
                comparison['unchanged'][key] = {
                    'label': label,
                    'value': f"{curr_val}{unit}",
                    'description': f"{label}ä¿æŒä¸å˜"
                }
            elif better == 'lower' and curr_val < hist_val:
                improvement = ((hist_val - curr_val) / hist_val * 100) if hist_val > 0 else 0
                comparison['improvements'][key] = {
                    'label': label,
                    'before': f"{hist_val}{unit}",
                    'after': f"{curr_val}{unit}",
                    'improvement': f"{improvement:.1f}%",
                    'description': f"{label}é™ä½{improvement:.1f}%ï¼Œæ€§èƒ½æå‡"
                }
            elif better == 'higher' and curr_val > hist_val:
                improvement = ((curr_val - hist_val) / hist_val * 100) if hist_val > 0 else 0
                comparison['improvements'][key] = {
                    'label': label,
                    'before': f"{hist_val}{unit}",
                    'after': f"{curr_val}{unit}",
                    'improvement': f"{improvement:.1f}%",
                    'description': f"{label}æå‡{improvement:.1f}%ï¼Œæ€§èƒ½æå‡"
                }
            else:
                if better == 'lower':
                    regression = ((curr_val - hist_val) / hist_val * 100) if hist_val > 0 else 0
                    description = f"{label}å¢åŠ {regression:.1f}%ï¼Œæ€§èƒ½ä¸‹é™"
                else:
                    regression = ((hist_val - curr_val) / hist_val * 100) if hist_val > 0 else 0
                    description = f"{label}é™ä½{regression:.1f}%ï¼Œæ€§èƒ½ä¸‹é™"
                
                comparison['regressions'][key] = {
                    'label': label,
                    'before': f"{hist_val}{unit}",
                    'after': f"{curr_val}{unit}",
                    'regression': f"{regression:.1f}%",
                    'description': description
                }
        
        # æ·»åŠ è¶‹åŠ¿åˆ†æ
        if self._load_trends():
            trends = self._load_trends()
            comparison['trend_analysis'] = self._analyze_trends(trends)
        
        return comparison
    
    def _trigger_callback(self, data: Dict[str, Any]):
        """è§¦å‘å›è°ƒå‡½æ•°"""
        try:
            if self.callback:
                self.callback(data)
        except Exception as e:
            logger.warning(f"æ€§èƒ½ç›‘æ§å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
    
    def _update_trends(self, report: Dict[str, Any]):
        """æ›´æ–°è¶‹åŠ¿æ•°æ®"""
        trends_path = self.output_dir / "trends.json"
        
        try:
            if trends_path.exists():
                with open(trends_path, 'r', encoding='utf-8') as f:
                    trends = json.load(f)
            else:
                trends = {
                    'records': [],
                    'summary': {
                        'total_runs': 0,
                        'average_duration': 0,
                        'best_duration': float('inf'),
                        'worst_duration': 0
                    }
                }
            
            # æ·»åŠ æ–°è®°å½•
            trends['records'].append({
                'timestamp': report['metadata']['timestamp'],
                'total_duration': report['summary']['total_duration'],
                'api_calls': report['summary']['total_api_calls'],
                'cache_hit_rate': report['summary']['cache_hit_rate'],
                'news_items': report['summary']['news_items_processed']
            })
            
            # é™åˆ¶è®°å½•æ•°é‡
            max_records = 100
            if len(trends['records']) > max_records:
                trends['records'] = trends['records'][-max_records:]
            
            # æ›´æ–°æ‘˜è¦
            durations = [r['total_duration'] for r in trends['records']]
            trends['summary'] = {
                'total_runs': len(trends['records']),
                'average_duration': round(sum(durations) / len(durations), 3) if durations else 0,
                'best_duration': round(min(durations), 3) if durations else 0,
                'worst_duration': round(max(durations), 3) if durations else 0,
                'improvement_rate': self._calculate_improvement_rate(trends['records'])
            }
            
            # ä¿å­˜è¶‹åŠ¿æ•°æ®
            with open(trends_path, 'w', encoding='utf-8') as f:
                json.dump(trends, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.warning(f"è¶‹åŠ¿æ•°æ®æ›´æ–°å¤±è´¥: {e}")
    
    def _load_trends(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½è¶‹åŠ¿æ•°æ®"""
        trends_path = self.output_dir / "trends.json"
        
        if not trends_path.exists():
            return None
        
        try:
            with open(trends_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"è¶‹åŠ¿æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _analyze_trends(self, trends: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¶‹åŠ¿æ•°æ®"""
        if not trends or 'records' not in trends or len(trends['records']) < 2:
            return {'has_enough_data': False}
        
        records = trends['records']
        
        # è®¡ç®—æœ€è¿‘çš„5æ¬¡è¿è¡Œçš„å¹³å‡å€¼
        recent_count = min(5, len(records))
        recent_records = records[-recent_count:]
        older_records = records[-recent_count*2:-recent_count] if len(records) >= recent_count*2 else []
        
        recent_avg = {
            'duration': sum(r['total_duration'] for r in recent_records) / recent_count,
            'api_calls': sum(r['api_calls'] for r in recent_records) / recent_count,
            'cache_hit_rate': sum(r['cache_hit_rate'] for r in recent_records) / recent_count
        }
        
        analysis = {
            'has_enough_data': True,
            'recent_performance': {
                'duration': round(recent_avg['duration'], 3),
                'api_calls': round(recent_avg['api_calls'], 1),
                'cache_hit_rate': round(recent_avg['cache_hit_rate'], 2)
            }
        }
        
        # å¦‚æœæœ‰æ›´æ—©çš„æ•°æ®ï¼Œè®¡ç®—å˜åŒ–è¶‹åŠ¿
        if older_records:
            older_avg = {
                'duration': sum(r['total_duration'] for r in older_records) / len(older_records),
                'api_calls': sum(r['api_calls'] for r in older_records) / len(older_records),
                'cache_hit_rate': sum(r['cache_hit_rate'] for r in older_records) / len(older_records)
            }
            
            changes = {}
            for key in ['duration', 'api_calls', 'cache_hit_rate']:
                if older_avg[key] > 0:
                    change_pct = ((recent_avg[key] - older_avg[key]) / older_avg[key]) * 100
                    changes[key] = round(change_pct, 1)
            
            analysis['trend_changes'] = changes
            
            # åˆ¤æ–­è¶‹åŠ¿æ–¹å‘
            trend_direction = []
            if changes.get('duration', 0) < -5:
                trend_direction.append('æ€§èƒ½æå‡ï¼ˆè€—æ—¶å‡å°‘ï¼‰')
            elif changes.get('duration', 0) > 5:
                trend_direction.append('æ€§èƒ½ä¸‹é™ï¼ˆè€—æ—¶å¢åŠ ï¼‰')
            
            if changes.get('cache_hit_rate', 0) > 5:
                trend_direction.append('ç¼“å­˜æ•ˆç‡æå‡')
            elif changes.get('cache_hit_rate', 0) < -5:
                trend_direction.append('ç¼“å­˜æ•ˆç‡ä¸‹é™')
            
            if trend_direction:
                analysis['trend_summary'] = '; '.join(trend_direction)
        
        return analysis
    
    def _calculate_improvement_rate(self, records: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ”¹è¿›ç‡"""
        if len(records) < 2:
            return 0.0
        
        # è®¡ç®—æœ€è¿‘5æ¬¡è¿è¡Œç›¸æ¯”æœ€æ—©çš„5æ¬¡è¿è¡Œçš„æ”¹è¿›ç‡
        recent_count = min(5, len(records))
        recent_avg = sum(r['total_duration'] for r in records[-recent_count:]) / recent_count
        
        older_count = min(5, len(records) - recent_count)
        if older_count == 0:
            return 0.0
        
        older_avg = sum(r['total_duration'] for r in records[:older_count]) / older_count
        
        if older_avg > 0:
            return round(((older_avg - recent_avg) / older_avg) * 100, 1)
        
        return 0.0


# ä¾¿æ·å‡½æ•°
def create_monitor(
    output_dir: str = "metrics",
    enable_logging: bool = True,
    auto_save: bool = True,
    callback: Optional[Callable[[Dict[str, Any]], None]] = None
) -> PerformanceMonitor:
    """
    å·¥å‚å‡½æ•°ï¼Œåˆ›å»ºç›‘æ§å™¨å®ä¾‹
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        enable_logging: æ˜¯å¦å¯ç”¨æ—¥å¿—
        auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜
        callback: å›è°ƒå‡½æ•°
    
    Returns:
        PerformanceMonitorå®ä¾‹
    """
    return PerformanceMonitor(
        output_dir=output_dir,
        enable_logging=enable_logging,
        auto_save=auto_save,
        callback=callback
    )


# ä½¿ç”¨ç¤ºä¾‹å’Œå¼‚æ­¥æ”¯æŒéªŒè¯
if __name__ == "__main__":
    import asyncio
    
    # å›è°ƒå‡½æ•°ç¤ºä¾‹
    def monitor_callback(data: Dict[str, Any]):
        """å®æ—¶ç›‘æ§å›è°ƒå‡½æ•°"""
        event_type = data.get('type', 'unknown')
        if event_type == 'counter_update':
            print(f"[å®æ—¶ç›‘æ§] è®¡æ•°å™¨æ›´æ–°: {data.get('counter')} += {data.get('value')}")
        elif event_type == 'stage_completed':
            print(f"[å®æ—¶ç›‘æ§] é˜¶æ®µå®Œæˆ: {data.get('stage_name')}, è€—æ—¶: {data.get('duration', 0):.2f}s")
        elif event_type == 'report_generated':
            print(f"[å®æ—¶ç›‘æ§] æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    async def example_sync():
        """åŒæ­¥ä½¿ç”¨ç¤ºä¾‹"""
        print("="*60)
        print("åŒæ­¥æ€§èƒ½ç›‘æ§ç¤ºä¾‹")
        print("="*60)
        
        # åˆ›å»ºç›‘æ§å™¨
        monitor = create_monitor(
            output_dir="metrics_example",
            enable_logging=True,
            auto_save=False,
            callback=monitor_callback
        )
        
        # å¼€å§‹ç›‘æ§
        monitor.start()
        
        # æ¨¡æ‹Ÿå„ä¸ªé˜¶æ®µ
        with monitor.stage('rss_fetch', StageType.RSS_FETCH):
            time.sleep(0.1)  # æ¨¡æ‹ŸRSSæŠ“å–
            monitor.increment('news_items_processed', 50)
            monitor.record_api_call(1000, 200)
        
        with monitor.stage('ai_scoring', StageType.AI_SCORING):
            time.sleep(0.3)  # æ¨¡æ‹ŸAIè¯„åˆ†
            monitor.increment('api_calls', 25)
            monitor.record_api_call(50000, 15000)
            monitor.record_cache_hit()
            monitor.record_cache_hit()
            monitor.record_cache_miss()
            monitor.record_cache_miss()
            monitor.record_cache_miss()
        
        with monitor.stage('generate_output', StageType.GENERATE_OUTPUT):
            time.sleep(0.05)  # æ¨¡æ‹Ÿè¾“å‡ºç”Ÿæˆ
        
        # è®¾ç½®è‡ªå®šä¹‰æŒ‡æ ‡
        monitor.set_custom_metric('total_news_sources', 15)
        monitor.set_custom_metric('avg_score', 7.8)
        
        # ç»“æŸç›‘æ§
        monitor.end()
        
        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        report = monitor.generate_report()
        report_path = monitor.save_report("example_report.json")
        print(f"ğŸ“Š æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ‰“å°æŠ¥å‘Šæ‘˜è¦
        print("\næ€§èƒ½æŠ¥å‘Šæ‘˜è¦:")
        print(f"æ€»è€—æ—¶: {report['summary']['total_duration']:.2f}s")
        print(f"APIè°ƒç”¨: {report['summary']['total_api_calls']}æ¬¡")
        print(f"Tokenä½¿ç”¨: {report['summary']['total_tokens']:,}")
        print(f"ç¼“å­˜å‘½ä¸­ç‡: {report['summary']['cache_hit_rate']:.1f}%")
        print(f"å¤„ç†æ–°é—»æ•°: {report['summary']['news_items_processed']}æ¡")
        
        # æ•ˆç‡æŒ‡æ ‡
        print(f"\næ•ˆç‡æŒ‡æ ‡:")
        print(f"æ¯æ–°é—»APIè°ƒç”¨: {report['efficiency']['api_calls_per_item']:.3f}")
        print(f"æ¯APIè°ƒç”¨Token: {report['efficiency']['tokens_per_api_call']:.0f}")
        print(f"æ¯ç§’å¤„ç†æ–°é—»: {report['efficiency']['items_per_second']:.2f}æ¡/s")
        
        # é˜¶æ®µè¯¦æƒ…
        print(f"\né˜¶æ®µè¯¦æƒ…:")
        for name, data in report['stages'].items():
            print(f"  {name}: {data['total_duration']:.3f}s (å¹³å‡{data['avg_duration']:.3f}s)")
    
    async def example_async():
        """å¼‚æ­¥ä½¿ç”¨ç¤ºä¾‹"""
        print("\n" + "="*60)
        print("å¼‚æ­¥æ€§èƒ½ç›‘æ§ç¤ºä¾‹")
        print("="*60)
        
        # åˆ›å»ºç›‘æ§å™¨
        monitor = create_monitor(
            output_dir="metrics_example_async",
            enable_logging=True,
            auto_save=False
        )
        
        # å¼€å§‹ç›‘æ§
        monitor.start()
        
        # æ¨¡æ‹Ÿå¼‚æ­¥é˜¶æ®µ
        async with monitor.astage('async_processing') as m:
            await asyncio.sleep(0.2)
            m.increment('api_calls', 10)
            m.record_api_call(20000, 5000)
            m.record_cache_hit()
            m.record_cache_miss()
        
        # å¤šä»»åŠ¡å¹¶å‘ç¤ºä¾‹
        async def process_item(item_id: int):
            """æ¨¡æ‹Ÿå•ä¸ªæ–°é—»å¤„ç†"""
            async with monitor.astage('item_processing') as m:
                await asyncio.sleep(0.02)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                m.increment('news_items_processed')
                if item_id % 3 == 0:  # 1/3å‘½ä¸­ç‡
                    m.record_cache_hit()
                else:
                    m.record_cache_miss()
                m.record_api_call(500, 200)
        
        # å¹¶å‘å¤„ç†10ä¸ªæ–°é—»
        tasks = [process_item(i) for i in range(10)]
        await asyncio.gather(*tasks)
        
        # ç»“æŸç›‘æ§
        monitor.end()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = monitor.generate_report()
        
        # æ‰“å°æ‘˜è¦
        print(f"å¼‚æ­¥å¤„ç†å®Œæˆ:")
        print(f"æ€»è€—æ—¶: {report['summary']['total_duration']:.2f}s")
        print(f"å¤„ç†æ–°é—»: {report['summary']['news_items_processed']}æ¡")
        print(f"ç¼“å­˜å‘½ä¸­: {report['summary']['cache_hits']}æ¬¡")
        print(f"ç¼“å­˜æœªå‘½ä¸­: {report['summary']['cache_misses']}æ¬¡")
        
        # å†å²å¯¹æ¯”ç¤ºä¾‹
        print("\nå†å²å¯¹æ¯”ç¤ºä¾‹:")
        comparison = monitor.compare_with_history()
        if 'error' in comparison:
            print(f"æ— å†å²æ•°æ®å¯å¯¹æ¯”: {comparison['error']}")
        else:
            if comparison.get('improvements'):
                print("æ”¹è¿›çš„æŒ‡æ ‡:")
                for key, data in comparison['improvements'].items():
                    print(f"  {data['label']}: {data['before']} â†’ {data['after']} ({data['improvement']})")
            
            if comparison.get('regressions'):
                print("ä¸‹é™çš„æŒ‡æ ‡:")
                for key, data in comparison['regressions'].items():
                    print(f"  {data['label']}: {data['before']} â†’ {data['after']}")
            
            if comparison.get('unchanged'):
                print(f"ä¿æŒä¸å˜çš„æŒ‡æ ‡: {len(comparison['unchanged'])}é¡¹")
    
    async def main():
        """ä¸»å‡½æ•°"""
        await example_sync()
        await example_async()
        
        print("\n" + "="*60)
        print("æ€§èƒ½ç›‘æ§ç³»ç»ŸéªŒè¯å®Œæˆ")
        print("="*60)
        
        # éªŒè¯çº¿ç¨‹å®‰å…¨
        print("\nçº¿ç¨‹å®‰å…¨æµ‹è¯•:")
        monitor = create_monitor(enable_logging=False)
        monitor.start()
        
        def worker(worker_id: int, iterations: int = 100):
            """å¤šçº¿ç¨‹å·¥ä½œå‡½æ•°"""
            for i in range(iterations):
                with monitor.stage(f'worker_{worker_id}_task_{i}'):
                    time.sleep(0.001)
                monitor.increment('news_items_processed')
                if i % 5 == 0:
                    monitor.record_cache_hit()
                else:
                    monitor.record_cache_miss()
        
        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹åŒæ—¶æ›´æ–°æŒ‡æ ‡
        threads = []
        for worker_id in range(5):
            t = threading.Thread(target=worker, args=(worker_id, 20))
            threads.append(t)
            t.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for t in threads:
            t.join()
        
        monitor.end()
        
        # æ£€æŸ¥è®¡æ•°å™¨ä¸€è‡´æ€§
        report = monitor.generate_report()
        expected_items = 5 * 20  # 5ä¸ªå·¥äºº * 20æ¬¡è¿­ä»£
        actual_items = report['summary']['news_items_processed']
        
        if actual_items == expected_items:
            print(f"âœ… çº¿ç¨‹å®‰å…¨æµ‹è¯•é€šè¿‡: é¢„æœŸ{expected_items}ï¼Œå®é™…{actual_items}")
        else:
            print(f"âŒ çº¿ç¨‹å®‰å…¨æµ‹è¯•å¤±è´¥: é¢„æœŸ{expected_items}ï¼Œå®é™…{actual_items}")
        
        print(f"æ€»é˜¶æ®µæ•°: {report['summary']['total_stages']}")
    
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())