"""
ä¸»ç¨‹åºå…¥å£
åè°ƒå„æ¨¡å—å®ŒæˆRSSæ–°é—»èšåˆæµç¨‹
"""
import os
import sys
import logging
import asyncio
from datetime import datetime
from typing import List

from src.config import Config
from src.models import NewsItem
from src.rss_fetcher import RSSFetcher
from src.ai_scorer import AIScorer
from src.markdown_generator import MarkdownGenerator
from src.rss_generator import RSSGenerator
from src.history_manager import HistoryManager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)


class RSSAggregator:
    """RSSæ–°é—»èšåˆå™¨ä¸»ç±»"""
    
    def __init__(self):
        self.config = Config()
        self.history = HistoryManager()
        self.fetcher = None
        self.scorer = None
        self.markdown_gen = None
        self.rss_gen = None
    
    async def run(self) -> bool:
        """
        æ‰§è¡Œå®Œæ•´çš„æ–°é—»èšåˆæµç¨‹
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        start_time = datetime.now()
        logger.info("=" * 50)
        logger.info(f"ğŸš€ RSSæ–°é—»èšåˆå¼€å§‹ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 50)
        
        try:
            # 1. åˆå§‹åŒ–å„æ¨¡å—
            self._init_modules()
            
            # 2. è·å–RSSæ–°é—»
            news_items = self._fetch_news()
            if not news_items:
                logger.warning("æœªè·å–åˆ°ä»»ä½•æ–°é—»")
                return False
            
            # 3. AIè¯„åˆ†å’Œç¿»è¯‘
            scored_items = await self._score_news(news_items)
            
            # 4. ç­›é€‰Top N
            top_items = self._select_top_news(scored_items)
            
            # 5. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
            self._generate_outputs(top_items)
            
            # 6. æ›´æ–°å†å²ç»Ÿè®¡
            self._update_stats(start_time, news_items, top_items)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info("=" * 50)
            logger.info(f"âœ… RSSæ–°é—»èšåˆå®Œæˆ - è€—æ—¶: {duration:.1f}ç§’")
            logger.info(f"ğŸ“Š æœ¬æ¬¡å¤„ç†: {len(news_items)}æ¡ â†’ ç²¾é€‰: {len(top_items)}æ¡")
            logger.info("=" * 50)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return False
    
    def _init_modules(self):
        """åˆå§‹åŒ–å„æ¨¡å—"""
        logger.info("åˆå§‹åŒ–æ¨¡å—...")
        
        self.fetcher = RSSFetcher(
            sources=self.config.rss_sources,
            output_config=self.config.output_config,
            filter_config=self.config.filter_config
        )
        
        self.scorer = AIScorer(config=self.config.ai_config)
        
        self.markdown_gen = MarkdownGenerator(
            output_dir="docs",
            archive_dir="archive"
        )
        
        self.rss_gen = RSSGenerator(
            feed_path="feed.xml",
            max_items=self.config.output_config.max_feed_items
        )
        
        logger.info(f"âœ“ å·²åŠ è½½ {len(self.config.rss_sources)} ä¸ªRSSæº")
        logger.info(f"âœ“ AIæ¨¡å‹: {self.config.ai_config.model}")
    
    def _fetch_news(self) -> List[NewsItem]:
        """è·å–æ–°é—»"""
        logger.info("ğŸ“¡ å¼€å§‹è·å–RSSæ–°é—»...")
        
        items = self.fetcher.fetch_all()
        
        # è¿‡æ»¤å·²å¤„ç†çš„URL
        processed = self.history.get_processed_urls()
        new_items = [item for item in items if item.link not in processed]
        
        logger.info(f"âœ“ è·å– {len(items)} æ¡ï¼Œå…¶ä¸­æ–°å†…å®¹ {len(new_items)} æ¡")
        
        return new_items if new_items else items  # å¦‚æœæ²¡æœ‰æ–°å†…å®¹ï¼Œä½¿ç”¨å…¨éƒ¨
    
    async def _score_news(self, items: List[NewsItem]) -> List[NewsItem]:
        """AIè¯„åˆ†"""
        logger.info(f"ğŸ¤– å¼€å§‹AIè¯„åˆ†(å…± {len(items)} æ¡)...")
        
        scored_items = await self.scorer.score_all(items)
        
        # è¿‡æ»¤ä½äºé˜ˆå€¼çš„
        threshold = self.config.filter_config.min_score_threshold
        filtered = [item for item in scored_items if (item.ai_score or 0) >= threshold]
        
        logger.info(f"âœ“ è¯„åˆ†å®Œæˆ: {len(scored_items)}æ¡ï¼Œâ‰¥{threshold}åˆ†: {len(filtered)}æ¡")
        
        return filtered
    
    def _select_top_news(self, items: List[NewsItem]) -> List[NewsItem]:
        """é€‰æ‹©Top Næ–°é—»"""
        # æŒ‰AIè¯„åˆ†æ’åº
        sorted_items = sorted(
            items, 
            key=lambda x: (x.ai_score or 0, x.published_at), 
            reverse=True
        )
        
        # å–å‰Næ¡
        max_count = self.config.output_config.max_news_count
        top_items = sorted_items[:max_count]
        
        logger.info(f"ğŸ“‹ ç²¾é€‰Top {len(top_items)} æ¡æ–°é—»")
        
        return top_items
    
    def _generate_outputs(self, items: List[NewsItem]):
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶"""
        logger.info("ğŸ“ ç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
        
        now = datetime.now()
        
        # ç”ŸæˆMarkdown
        latest_path, archive_path = self.markdown_gen.generate(items, now)
        logger.info(f"âœ“ Markdown: {latest_path}")
        
        # ç”ŸæˆRSS
        self.rss_gen.generate(items)
        logger.info(f"âœ“ RSS feed: feed.xml")
    
    def _update_stats(self, run_time: datetime, all_items: List[NewsItem], 
                      selected_items: List[NewsItem]):
        """æ›´æ–°ç»Ÿè®¡æ•°æ®"""
        # æºç»Ÿè®¡
        source_stats = {}
        for item in all_items:
            source_stats[item.source] = source_stats.get(item.source, 0) + 1
        
        # æ›´æ–°å†å²
        self.history.update_stats(run_time, len(all_items), source_stats)
        
        # è®°å½•å·²å¤„ç†çš„URL
        for item in all_items:
            self.history.add_processed(item.link)
        
        # æ›´æ–°æºé€‰ä¸­ç»Ÿè®¡
        for item in selected_items:
            self.history.update_source_selected(item.source, 1)
        
        # ä¿å­˜
        self.history.save()
        
        # è¾“å‡ºç»Ÿè®¡
        stats = self.history.get_stats()
        logger.info(f"ğŸ“ˆ æ€»è¿è¡Œæ¬¡æ•°: {stats['total_runs']}")
        logger.info(f"ğŸ“ˆ æ€»å¤„ç†æ–°é—»: {stats['total_news_processed']}")
        logger.info(f"ğŸ“ˆ å¹³å‡æ¯æœŸ: {stats['avg_news_per_run']}")


async def main():
    """ä¸»å…¥å£å‡½æ•°"""
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.error("âŒ ç¯å¢ƒå˜é‡ OPENAI_API_KEY æœªè®¾ç½®")
        logger.error("è¯·åœ¨GitHubä»“åº“Settings -> Secrets and variables -> Actionsä¸­è®¾ç½®")
        sys.exit(1)
    
    # åˆ›å»ºèšåˆå™¨å¹¶è¿è¡Œ
    aggregator = RSSAggregator()
    success = await aggregator.run()
    
    if not success:
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
