"""
æä¾›å•†ç®¡ç†å™¨ - LLMæä¾›å•†ç®¡ç†å’ŒAPIè°ƒç”¨

ç®¡ç†14å®¶LLMæä¾›å•†çš„åˆå§‹åŒ–ã€åˆ‡æ¢ã€å›é€€é€»è¾‘
"""
import asyncio
import logging
import time
from typing import List, Callable, Any, Optional
from openai import AsyncOpenAI, RateLimitError
from tenacity import (
    retry, stop_after_attempt, wait_random_exponential,
    retry_if_exception_type, before_sleep_log
)

from src.models import NewsItem, AIConfig, ProviderConfig
from .rate_limiter import SimpleRateLimiter
from .error_handler import ErrorHandler

logger = logging.getLogger(__name__)


def _split_into_batches(items: list, batch_size: int) -> list:
    """
    å°†åˆ—è¡¨åˆ†å‰²æˆå¤šä¸ªæ‰¹æ¬¡

    Args:
        items: å¾…åˆ†å‰²çš„åˆ—è¡¨
        batch_size: æ¯æ‰¹å¤§å°

    Returns:
        list: åˆ†å‰²åçš„æ‰¹æ¬¡åˆ—è¡¨
    """
    return [items[i:i+batch_size] for i in range(0, len(items), batch_size)]


class ProviderManager:
    """
    LLMæä¾›å•†ç®¡ç†å™¨
    
    ç®¡ç†14å®¶LLMæä¾›å•†çš„åˆå§‹åŒ–ã€åˆ‡æ¢ã€å›é€€é€»è¾‘
    æä¾›ç»Ÿä¸€çš„APIè°ƒç”¨æ¥å£
    """
    
    def __init__(self, config: AIConfig):
        """
        åˆå§‹åŒ–æä¾›å•†ç®¡ç†å™¨
        
        Args:
            config: AIé…ç½®å¯¹è±¡
        """
        self.config = config
        self.fallback = config.fallback
        self.providers_config = config.providers_config
        
        # å½“å‰æä¾›å•†çŠ¶æ€
        self._init_provider(config.provider)
        
        # APIè°ƒç”¨è®¡æ•°
        self.api_call_count = 0
    
    def _init_provider(self, provider_name: str):
        """
        åˆå§‹åŒ–æŒ‡å®šæä¾›å•†
        
        Args:
            provider_name: æä¾›å•†åç§°
            
        Raises:
            ValueError: æä¾›å•†ä¸å­˜åœ¨
        """
        if provider_name not in self.providers_config:
            raise ValueError(f"æœªçŸ¥çš„æä¾›å•†: {provider_name}")
        
        provider_config = self.providers_config[provider_name]
        
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
        self.client = AsyncOpenAI(
            api_key=provider_config.api_key,
            base_url=provider_config.base_url
        )
        self.model = provider_config.model
        self.current_provider_name = provider_name
        self.current_config = provider_config
        
        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨
        if provider_config.rate_limit_rpm:
            self.rate_limiter = SimpleRateLimiter(
                max_requests=provider_config.rate_limit_rpm
            )
            logger.info(
                f"[{provider_name}] å¯ç”¨é€Ÿç‡é™åˆ¶: "
                f"{provider_config.rate_limit_rpm} RPM"
            )
        else:
            self.rate_limiter = None
        
        logger.info(
            f"åˆå§‹åŒ–AIæä¾›å•†: {provider_name} "
            f"({self.model})"
        )
    
    def build_fallback_chain(self) -> List[str]:
        """
        æ„å»ºå›é€€é“¾ï¼ˆå»é‡ï¼‰
        
        Returns:
            List[str]: æä¾›å•†åç§°åˆ—è¡¨
        """
        chain = []
        seen = set()
        
        # 1. é¦–é€‰å½“å‰é…ç½®çš„ä¸»æä¾›å•†
        if (self.current_provider_name and 
            self.current_provider_name in self.providers_config):
            chain.append(self.current_provider_name)
            seen.add(self.current_provider_name)
        
        # 2. æ·»åŠ fallback_chainä¸­é…ç½®çš„æä¾›å•†
        for provider in self.fallback.fallback_chain:
            if (provider not in seen and 
                provider in self.providers_config):
                chain.append(provider)
                seen.add(provider)
        
        return chain
    
    async def execute_with_fallback(
        self,
        operation_name: str,
        operation_func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        å¸¦è‡ªåŠ¨å›é€€çš„æ‰§è¡Œ
        
        Args:
            operation_name: æ“ä½œåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            operation_func: æ“ä½œå‡½æ•°
            *args: ä½ç½®å‚æ•°
            **kwargs: å…³é”®å­—å‚æ•°
            
        Returns:
            Any: æ“ä½œç»“æœ
            
        Raises:
            Exception: æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥æ—¶æŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
        """
        if not self.fallback.enabled:
            return await operation_func(*args, **kwargs)
        
        fallback_chain = self.build_fallback_chain()
        last_exception = None
        
        for provider_name in fallback_chain:
            try:
                logger.info(
                    f"ğŸ”„ å°è¯•ä½¿ç”¨æä¾›å•†: {provider_name} "
                    f"({operation_name})"
                )
                self._init_provider(provider_name)
                result = await operation_func(*args, **kwargs)
                logger.info(f"âœ… æä¾›å•† {provider_name} è°ƒç”¨æˆåŠŸ")
                return result
                
            except Exception as e:
                logger.error(
                    f"âŒ æä¾›å•† {provider_name} å¤±è´¥: {e}"
                )
                last_exception = e
                continue
        
        logger.error(
            f"âŒ æ‰€æœ‰AIæä¾›å•†å‡å¤±è´¥ ({operation_name})"
        )
        raise last_exception
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_random_exponential(min=5, max=30),
        retry=retry_if_exception_type(RateLimitError),
        reraise=True,
        before_sleep=before_sleep_log(logger, logging.WARNING)
    )
    async def call_api(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.3,
        system_message: str = None,
        is_json: bool = True
    ) -> str:
        """
        è°ƒç”¨APIï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            prompt: ç”¨æˆ·Prompt
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            system_message: ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
            is_json: æ˜¯å¦éœ€è¦JSONå“åº”
            
        Returns:
            str: AIç”Ÿæˆçš„å“åº”å†…å®¹
        """
        # åº”ç”¨é€Ÿç‡é™åˆ¶
        if self.rate_limiter:
            await self.rate_limiter.acquire()
        
        # æ„å»ºæ¶ˆæ¯
        messages = []
        
        if system_message:
            messages.append({
                "role": "system",
                "content": system_message
            })
        else:
            messages.append({
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°é—»ç¼–è¾‘å’Œç­›é€‰å‘˜ï¼Œæ“…é•¿è¯„ä¼°æ–°é—»ä»·å€¼å’Œæ’°å†™ä¸­æ–‡æ‘˜è¦ã€‚"
            })
        
        messages.append({
            "role": "user",
            "content": prompt
        })
        
        # æ„å»ºå“åº”æ ¼å¼
        response_format = {"type": "json_object"} if is_json else None
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                response_format=response_format
            )
            
            self.api_call_count += 1
            
            return response.choices[0].message.content
            
        except Exception as e:
            # Special handling for 429 errors (rate limit)
            if hasattr(e, 'status') and e.status == 429:
                logger.warning(
                    f"âš ï¸ æ”¶åˆ°429é€Ÿç‡é™åˆ¶é”™è¯¯ ({self.current_provider_name})ï¼Œ"
                    f"é¢å¤–ç­‰å¾…10ç§’åé‡è¯•"
                )
                await asyncio.sleep(10)
            
            ErrorHandler.log_error(
                context=f"APIè°ƒç”¨ ({self.current_provider_name})",
                error=e,
                logger=logger
            )
            raise
    
    async def call_batch_api(
        self,
        prompt: str,
        max_tokens: int = 8000,
        temperature: float = 0.3
    ) -> str:
        """
        è°ƒç”¨æ‰¹é‡è¯„åˆ†API
        
        Args:
            prompt: æ‰¹é‡è¯„åˆ†Prompt
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            str: AIç”Ÿæˆçš„å“åº”å†…å®¹
        """
        return await self.call_api(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            system_message=(
                "ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°é—»ç¼–è¾‘å’Œç­›é€‰å‘˜ï¼Œæ“…é•¿è¯„ä¼°æ–°é—»ä»·å€¼"
                "å’Œæ’°å†™ä¸­æ–‡æ‘˜è¦ã€‚ä½ å¿…é¡»ä¸¥æ ¼è¿”å›JSONæ•°ç»„æ ¼å¼ã€‚"
            ),
            is_json=True
        )
    
    async def call_deep_analysis_api(
        self,
        prompt: str,
        max_tokens: int = 10000,
        temperature: float = 0.3
    ) -> str:
        """
        è°ƒç”¨æ·±åº¦åˆ†æAPI
        
        Args:
            prompt: æ·±åº¦åˆ†æPrompt
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            str: AIç”Ÿæˆçš„å“åº”å†…å®¹
        """
        return await self.call_api(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            system_message=(
                "ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°é—»åˆ†æå¸ˆï¼Œæ“…é•¿å¤šç»´åº¦æ·±åº¦åˆ†æã€‚"
                "ä½ å¿…é¡»ä¸¥æ ¼è¿”å›JSONæ•°ç»„æ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"
            ),
            is_json=True
        )
    
    async def call_single_scoring_api(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.3
    ) -> str:
        """
        è°ƒç”¨å•æ¡è¯„åˆ†API
        
        Args:
            prompt: å•æ¡è¯„åˆ†Prompt
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            str: AIç”Ÿæˆçš„å“åº”å†…å®¹
        """
        return await self.call_api(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            system_message="ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°é—»ç¼–è¾‘ï¼Œæ“…é•¿è¯„ä¼°æ–°é—»ä»·å€¼å’Œæ’°å†™ä¸­æ–‡æ‘˜è¦ã€‚",
            is_json=True
        )
    
    def get_provider_info(self) -> dict:
        """
        è·å–å½“å‰æä¾›å•†ä¿¡æ¯
        
        Returns:
            dict: æä¾›å•†ä¿¡æ¯
        """
        return {
            'name': self.current_provider_name,
            'model': self.model,
            'base_url': self.current_config.base_url,
            'temperature': self.current_config.temperature,
            'max_tokens': getattr(self.current_config, 'max_tokens', 1000),
        }
    
    def get_api_call_count(self) -> int:
        """
        è·å–APIè°ƒç”¨è®¡æ•°
        
        Returns:
            int: è°ƒç”¨æ¬¡æ•°
        """
        return self.api_call_count
    
    def reset_api_call_count(self):
        """é‡ç½®APIè°ƒç”¨è®¡æ•°"""
        self.api_call_count = 0
    
    def get_stats(self) -> dict:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            'api_call_count': self.api_call_count,
            'current_provider': self.current_provider_name,
            'current_model': self.model,
            'fallback_enabled': self.fallback.enabled,
            'fallback_chain': self.build_fallback_chain(),
        }
    
    def is_provider_available(self, provider_name: str) -> bool:
        """
        æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨
        
        Args:
            provider_name: æä¾›å•†åç§°
            
        Returns:
            bool: æ˜¯å¦å¯ç”¨
        """
        return provider_name in self.providers_config
    
    def get_available_providers(self) -> List[str]:
        """
        è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨
        
        Returns:
            List[str]: æä¾›å•†åç§°åˆ—è¡¨
        """
        return list(self.providers_config.keys())
    
    async def switch_provider(self, provider_name: str):
        """
        åˆ‡æ¢æä¾›å•†
        
        Args:
            provider_name: æä¾›å•†åç§°
            
        Raises:
            ValueError: æä¾›å•†ä¸å­˜åœ¨
        """
        if not self.is_provider_available(provider_name):
            raise ValueError(f"æä¾›å•†ä¸å¯ç”¨: {provider_name}")
        
        self._init_provider(provider_name)
        logger.info(f"åˆ‡æ¢åˆ°æä¾›å•†: {provider_name}")
    
    async def test_provider(self, provider_name: str) -> bool:
        """
        æµ‹è¯•æä¾›å•†æ˜¯å¦å¯ç”¨

        Args:
            provider_name: æä¾›å•†åç§°

        Returns:
            bool: æ˜¯å¦å¯ç”¨
        """
        try:
            original_provider = self.current_provider_name
            self._init_provider(provider_name)

            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": "test"}
                ],
                max_tokens=5
            )

            # æ¢å¤åŸæ¥çš„æä¾›å•†
            self._init_provider(original_provider)

            return True

        except Exception as e:
            logger.error(f"æµ‹è¯•æä¾›å•† {provider_name} å¤±è´¥: {e}")
            return False

    # ==================== çœŸæ‰¹å¤„ç†æ–¹æ³• ====================

    async def _process_batch_with_fallback(
        self,
        batch: List[NewsItem],
        batch_index: int,
        total_batches: int,
        call_batch_api_func: Callable,
        **api_kwargs
    ) -> tuple:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„APIè°ƒç”¨ï¼ˆå¸¦å›é€€æœºåˆ¶ï¼‰

        Args:
            batch: å½“å‰æ‰¹æ¬¡çš„æ–°é—»é¡¹åˆ—è¡¨
            batch_index: æ‰¹æ¬¡ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
            total_batches: æ€»æ‰¹æ¬¡æ•°
            call_batch_api_func: æ‰¹é‡APIè°ƒç”¨å‡½æ•°
            **api_kwargs: ä¼ é€’ç»™APIè°ƒç”¨å‡½æ•°çš„é¢å¤–å‚æ•°

        Returns:
            tuple: (æ‰¹æ¬¡ç´¢å¼•, APIå“åº”å†…å®¹æˆ–None, å¼‚å¸¸æˆ–None)
        """
        try:
            logger.info(
                f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_index}/{total_batches} "
                f"({len(batch)} æ¡æ–°é—»)"
            )

            # è°ƒç”¨æ‰¹é‡API
            response = await call_batch_api_func(**api_kwargs)
            self.api_call_count += 1

            return (batch_index, response, None)

        except Exception as e:
            logger.warning(
                f"âš ï¸ æ‰¹æ¬¡ {batch_index}/{total_batches} APIè°ƒç”¨å¤±è´¥: {e}"
            )
            return (batch_index, None, e)

    async def execute_batch_with_fallback(
        self,
        items: List[NewsItem],
        batch_size: int,
        call_batch_api_func: Callable,
        fallback_single_func: Callable = None,
        default_score: float = 5.0,
        **api_kwargs
    ) -> tuple:
        """
        æ‰§è¡Œå¸¦çœŸæ‰¹å¤„ç†å’Œå›é€€çš„æ‰¹é‡APIè°ƒç”¨

        é™çº§ç­–ç•¥ï¼š
        1. é¦–é€‰ï¼šæ‰¹é‡APIè°ƒç”¨ï¼ˆæŒ‰batch_sizeåˆ†æ‰¹ï¼‰
        2. é™çº§1ï¼šå•æ¡APIè°ƒç”¨
        3. é™çº§2ï¼šä½¿ç”¨é»˜è®¤åˆ†æ•°

        Args:
            items: æ–°é—»é¡¹åˆ—è¡¨
            batch_size: æ¯æ‰¹å¤§å°
            call_batch_api_func: æ‰¹é‡APIè°ƒç”¨å‡½æ•°
            fallback_single_func: å•æ¡å¤„ç†å‡½æ•°ï¼ˆå¯é€‰ï¼‰
            default_score: é»˜è®¤åˆ†æ•°
            **api_kwargs: ä¼ é€’ç»™APIè°ƒç”¨å‡½æ•°çš„é¢å¤–å‚æ•°

        Returns:
            tuple: (results: List[dict], api_call_count: int)
        """
        if not items:
            return [], 0

        original_count = len(items)
        logger.info(f"ğŸš€ å¼€å§‹çœŸæ‰¹å¤„ç†: {original_count} æ¡æ–°é—»")

        # 1. åˆ†å‰²æˆæ‰¹æ¬¡
        batches = _split_into_batches(items, batch_size)
        total_batches = len(batches)
        logger.info(f"ğŸ“¦ åˆ†å‰²ä¸º {total_batches} ä¸ªæ‰¹æ¬¡ (æ¯æ‰¹ â‰¤{batch_size})")

        # 2. å°è¯•æ‰¹é‡å¤„ç†
        batch_results = {}
        batch_failures = []

        for batch in batches:
            batch_idx = batches.index(batch) + 1
            batch_idx_outcome, response, error = (
                await self._process_batch_with_fallback(
                    batch=batch,
                    batch_index=batch_idx,
                    total_batches=total_batches,
                    call_batch_api_func=call_batch_api_func,
                    prompt=api_kwargs.get('prompt'),
                    max_tokens=api_kwargs.get('max_tokens', 8000),
                    temperature=api_kwargs.get('temperature', 0.3)
                )
            )

            if response:
                batch_results[batch_idx_outcome] = response
            else:
                batch_failures.append((batch_idx_outcome, batch, error))

        # 3. å¦‚æœæœ‰å¤±è´¥çš„æ‰¹æ¬¡ï¼Œå°è¯•å›é€€
        if batch_failures:
            logger.warning(
                f"âš ï¸ {len(batch_failures)}/{total_batches} æ‰¹æ¬¡å¤±è´¥ï¼Œå°è¯•å›é€€..."
            )

            for batch_idx, batch, error in batch_failures:
                if fallback_single_func:
                    # é™çº§1ï¼šå•æ¡å¤„ç†
                    logger.info(
                        f"ğŸ”„ æ‰¹æ¬¡ {batch_idx} é™çº§ä¸ºå•æ¡å¤„ç† "
                        f"({len(batch)} æ¡)"
                    )

                    for item in batch:
                        try:
                            result = await fallback_single_func(
                                item=item,
                                **api_kwargs
                            )
                            if result:
                                item.ai_score = result.get('score', default_score)
                            else:
                                item.ai_score = default_score
                        except Exception as e:
                            logger.error(
                                f"âŒ å•æ¡å¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°"
                            )
                            item.ai_score = default_score
                else:
                    # é™çº§2ï¼šä½¿ç”¨é»˜è®¤åˆ†æ•°
                    logger.warning(
                        f"âš ï¸ æ‰¹æ¬¡ {batch_idx} ä½¿ç”¨é»˜è®¤åˆ†æ•° {default_score}"
                    )
                    for item in batch:
                        item.ai_score = default_score

        # 4. åˆå¹¶ç»“æœ
        all_results = []
        for batch_idx in range(1, total_batches + 1):
            if batch_idx in batch_results:
                all_results.append(batch_results[batch_idx])

        total_api_calls = self.api_call_count

        logger.info(
            f"âœ… çœŸæ‰¹å¤„ç†å®Œæˆ: {original_count} æ¡ â†’ "
            f"{len(all_results)} æ‰¹æˆåŠŸ, {total_api_calls} æ¬¡APIè°ƒç”¨"
        )

        return all_results, total_api_calls
