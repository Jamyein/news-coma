"""
åˆ†æ‰¹å¤„ç†å™¨ - å¤„ç†å¤§æ‰¹é‡æ–°é—»çš„åˆ†æ‰¹APIè°ƒç”¨

è§£å†³å•æ¬¡APIè°ƒç”¨æ— æ³•å¤„ç†å¤§é‡æ–°é—»çš„é—®é¢˜
"""

import logging
from typing import List, Dict, Any, Callable, TypeVar
import asyncio

logger = logging.getLogger(__name__)

T = TypeVar('T')


class BatchProcessor:
    """
    åˆ†æ‰¹å¤„ç†å™¨
    
    å°†å¤§æ‰¹é‡æ•°æ®åˆ†æ‰¹å¤„ç†ï¼Œå¹¶åˆå¹¶ç»“æœ
    
    ç‰¹æ€§ï¼š
    1. è‡ªåŠ¨åˆ†æ‰¹ï¼šè¶…è¿‡é˜ˆå€¼è‡ªåŠ¨åˆ†æ‰¹æ¬¡å¤„ç†
    2. ç´¢å¼•æ˜ å°„ï¼šè‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å†…ç´¢å¼•åˆ°å…¨å±€ç´¢å¼•
    3. å¤±è´¥é‡è¯•ï¼šå•æ‰¹å¤±è´¥è‡ªåŠ¨é‡è¯•
    4. è¿›åº¦æ—¥å¿—ï¼šè®°å½•å¤„ç†è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
    """
    
    # é»˜è®¤é…ç½®
    DEFAULT_BATCH_SIZE = 100  # æ¯æ‰¹æœ€å¤§æ•°é‡
    DEFAULT_MAX_RETRIES = 2   # å•æ‰¹æœ€å¤§é‡è¯•æ¬¡æ•°
    DEFAULT_RETRY_DELAY = 1.0  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
    
    def __init__(
        self,
        batch_size: int = DEFAULT_BATCH_SIZE,
        max_retries: int = DEFAULT_MAX_RETRIES,
        retry_delay: float = DEFAULT_RETRY_DELAY,
        index_key: str = 'news_index'  # ç»“æœä¸­ç´¢å¼•å­—æ®µå
    ):
        """
        åˆå§‹åŒ–åˆ†æ‰¹å¤„ç†å™¨
        
        Args:
            batch_size: æ¯æ‰¹æœ€å¤§å¤„ç†æ•°é‡
            max_retries: å•æ‰¹å¤±è´¥é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
            index_key: ç»“æœå­—å…¸ä¸­ç´¢å¼•å­—æ®µçš„åç§°
        """
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.index_key = index_key
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_items': 0,
            'total_batches': 0,
            'successful_batches': 0,
            'failed_batches': 0,
            'retried_batches': 0,
            'total_results': 0,
            'missing_results': 0
        }
    
    async def process(
        self,
        items: List[T],
        process_func: Callable[[List[T]], List[Dict]],
        description: str = "å¤„ç†"
    ) -> List[Dict]:
        """
        åˆ†æ‰¹å¤„ç†æ•°æ®
        
        Args:
            items: å¾…å¤„ç†çš„æ•°æ®åˆ—è¡¨
            process_func: å¤„ç†å‡½æ•°ï¼Œæ¥æ”¶ä¸€æ‰¹æ•°æ®ï¼Œè¿”å›ç»“æœåˆ—è¡¨
            description: å¤„ç†æè¿°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            List[Dict]: åˆå¹¶åçš„æ‰€æœ‰ç»“æœ
            
        Raises:
            RuntimeError: å½“æ‰¹æ¬¡å¤„ç†å¤±è´¥ä¸”é‡è¯•è€—å°½æ—¶
        """
        if not items:
            return []
        
        self.stats['total_items'] = len(items)
        
        # å¦‚æœæ•°é‡åœ¨é˜ˆå€¼å†…ï¼Œç›´æ¥å¤„ç†
        if len(items) <= self.batch_size:
            logger.info(f"{description}: æ•°é‡{len(items)}åœ¨å•æ‰¹é˜ˆå€¼å†…ï¼Œç›´æ¥å¤„ç†")
            results = await self._process_single_batch(items, process_func, 1, 1)
            return results
        
        # è¶…è¿‡é˜ˆå€¼ï¼Œåˆ†æ‰¹å¤„ç†
        logger.info(f"ğŸ”„ {description}: æ•°é‡({len(items)})è¶…è¿‡å•æ‰¹é˜ˆå€¼({self.batch_size})ï¼Œå¯åŠ¨åˆ†æ‰¹å¤„ç†...")
        
        # åˆ†æ‰¹
        batches = [items[i:i+self.batch_size] for i in range(0, len(items), self.batch_size)]
        self.stats['total_batches'] = len(batches)
        
        all_results = []
        
        for batch_idx, batch in enumerate(batches, 1):
            logger.info(f"  {description} - ç¬¬{batch_idx}/{len(batches)}æ‰¹ ({len(batch)}æ¡)...")
            
            # å¤„ç†å•æ‰¹ï¼ˆå¸¦é‡è¯•ï¼‰
            try:
                batch_results = await self._process_single_batch(
                    batch, process_func, batch_idx, len(batches)
                )
                self.stats['successful_batches'] += 1
            except Exception as e:
                logger.error(f"  âŒ ç¬¬{batch_idx}æ‰¹å¤„ç†å¤±è´¥ï¼ˆå·²é‡è¯•{self.max_retries}æ¬¡ï¼‰: {e}")
                self.stats['failed_batches'] += 1
                # å¤±è´¥æ—¶å¡«å……é»˜è®¤å€¼
                batch_results = self._generate_default_results(batch, batch_idx)
            
            # ç´¢å¼•è°ƒæ•´ï¼šå°†æ‰¹æ¬¡å†…ç´¢å¼•æ˜ å°„åˆ°å…¨å±€ç´¢å¼•
            offset = (batch_idx - 1) * self.batch_size
            for result in batch_results:
                if self.index_key in result:
                    result[self.index_key] = result[self.index_key] + offset
            
            all_results.extend(batch_results)
            
            # æ£€æŸ¥æ‰¹æ¬¡å®Œæ•´æ€§
            expected_count = len(batch)
            actual_count = len(batch_results)
            if actual_count < expected_count:
                logger.warning(f"  âš ï¸ ç¬¬{batch_idx}æ‰¹ç»“æœä¸å®Œæ•´: {actual_count}/{expected_count}")
                self.stats['missing_results'] += (expected_count - actual_count)
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.stats['total_results'] = len(all_results)
        success_rate = (self.stats['successful_batches'] / self.stats['total_batches'] * 100) if self.stats['total_batches'] > 0 else 0
        
        logger.info(f"âœ… {description}å®Œæˆç»Ÿè®¡:")
        logger.info(f"   - æ€»æ–°é—»æ•°: {self.stats['total_items']}")
        logger.info(f"   - åˆ†æ‰¹æ•°: {self.stats['total_batches']}")
        logger.info(f"   - æˆåŠŸæ‰¹æ¬¡: {self.stats['successful_batches']} ({success_rate:.1f}%)")
        logger.info(f"   - å¤±è´¥æ‰¹æ¬¡: {self.stats['failed_batches']}")
        logger.info(f"   - é‡è¯•æ¬¡æ•°: {self.stats['retried_batches']}")
        logger.info(f"   - æ€»ç»“æœæ•°: {self.stats['total_results']}/{self.stats['total_items']}")
        
        return all_results
    
    async def _process_single_batch(
        self,
        batch: List[T],
        process_func: Callable[[List[T]], List[Dict]],
        batch_idx: int,
        total_batches: int
    ) -> List[Dict]:
        """
        å¤„ç†å•æ‰¹æ¬¡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®
            process_func: å¤„ç†å‡½æ•°
            batch_idx: å½“å‰æ‰¹æ¬¡ç´¢å¼•
            total_batches: æ€»æ‰¹æ¬¡æ•°
            
        Returns:
            List[Dict]: æ‰¹æ¬¡å¤„ç†ç»“æœ
            
        Raises:
            Exception: é‡è¯•è€—å°½åæŠ›å‡ºæœ€åä¸€æ¬¡å¼‚å¸¸
        """
        last_error = None
        
        for attempt in range(self.max_retries + 1):
            try:
                if attempt > 0:
                    logger.info(f"    ç¬¬{batch_idx}æ‰¹ç¬¬{attempt}æ¬¡é‡è¯•...")
                    self.stats['retried_batches'] += 1
                    await asyncio.sleep(self.retry_delay * attempt)  # é€’å¢å»¶è¿Ÿ
                
                # æ‰§è¡Œå¤„ç†
                results = await process_func(batch)
                
                # éªŒè¯ç»“æœ
                if not isinstance(results, list):
                    raise ValueError(f"å¤„ç†å‡½æ•°è¿”å›ç±»å‹é”™è¯¯: {type(results)}, æœŸæœ›list")
                
                return results
                
            except Exception as e:
                last_error = e
                logger.warning(f"    ç¬¬{batch_idx}æ‰¹å¤„ç†å¤±è´¥(å°è¯•{attempt+1}/{self.max_retries+1}): {e}")
                continue
        
        # æ‰€æœ‰é‡è¯•å¤±è´¥
        logger.error(f"    âŒ ç¬¬{batch_idx}æ‰¹å¤„ç†å¤±è´¥ï¼Œå·²é‡è¯•{self.max_retries}æ¬¡")
        raise last_error
    
    def _generate_default_results(self, batch: List[T], batch_idx: int) -> List[Dict]:
        """
        ç”Ÿæˆé»˜è®¤ç»“æœï¼ˆå½“æ‰¹æ¬¡å¤„ç†å®Œå…¨å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®
            batch_idx: æ‰¹æ¬¡ç´¢å¼•
            
        Returns:
            List[Dict]: é»˜è®¤ç»“æœåˆ—è¡¨
        """
        offset = (batch_idx - 1) * self.batch_size
        defaults = []
        
        for i, item in enumerate(batch, 1):
            defaults.append({
                self.index_key: i + offset,
                'category': 'ç¤¾ä¼šæ”¿æ²»',
                'category_confidence': 0.5,
                'total': 5.0,
                '_default': True,  # æ ‡è®°ä¸ºé»˜è®¤å€¼
                '_error': 'batch_processing_failed'
            })
        
        return defaults
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_items': 0,
            'total_batches': 0,
            'successful_batches': 0,
            'failed_batches': 0,
            'retried_batches': 0,
            'total_results': 0,
            'missing_results': 0
        }
