"""
AdaptiveBatchProcessor - æ™ºèƒ½åŠ¨æ€æ‰¹å¤„ç†å™¨

åŸºäºç›¸ä¼¼åº¦åˆ†ç»„å’ŒæˆåŠŸç‡å†å²çš„åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–
"""
import logging
from typing import List, Dict, Optional, Tuple, Any
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import re

from src.models import NewsItem

logger = logging.getLogger(__name__)


@dataclass
class BatchContext:
    """æ‰¹å¤„ç†ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    pass_number: int = 1  # Pass 1 æˆ– Pass 2
    category: str = ""  # æ–°é—»åˆ†ç±»
    priority_mode: str = "balanced"  # priority | balanced | similarity
    total_items: int = 0  # æ€»é¡¹ç›®æ•°
    time_window_hours: Optional[int] = None  # æ—¶é—´çª—å£


@dataclass
class BatchHistoryEntry:
    """æ‰¹å¤„ç†å†å²è®°å½•"""
    batch_size: int
    success: bool
    items_processed: int
    timestamp: datetime
    error_type: Optional[str] = None
    context: Dict[str, Any] = field(default_factory=dict)


class AdaptiveBatchProcessor:
    """
    è‡ªé€‚åº”æ‰¹å¤„ç†å™¨
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. åŠ¨æ€æ‰¹å¤§å°è°ƒæ•´ï¼šåŸºäºå†å²æˆåŠŸç‡è‡ªé€‚åº”è°ƒæ•´æ‰¹å¤§å°ï¼ˆ10-25ï¼‰
    2. ç›¸ä¼¼åº¦åˆ†ç»„ï¼šå°†ç›¸ä¼¼æ–°é—»åˆå¹¶å¤„ç†ä»¥å‡å°‘APIè°ƒç”¨
    3. ä¼˜å…ˆçº§æ„ŸçŸ¥ï¼šé«˜ä¼˜å…ˆçº§é¡¹ç›®ä¼˜å…ˆå¤„ç†
    4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼šè€ƒè™‘å¤„ç†ä¸Šä¸‹æ–‡ï¼ˆPassé˜¶æ®µã€åˆ†ç±»ç­‰ï¼‰
    
    ç›®æ ‡ï¼šåœ¨ä¿æŒè´¨é‡çš„åŒæ—¶æœ€å°åŒ–APIè°ƒç”¨æ¬¡æ•°
    """
    
    def __init__(
        self,
        min_size: int = 8,
        max_size: int = 25,
        target_success_rate: float = 0.85,
        history_window: int = 50
    ):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”æ‰¹å¤„ç†å™¨
        
        Args:
            min_size: æœ€å°æ‰¹å¤§å°
            max_size: æœ€å¤§æ‰¹å¤§å°
            target_success_rate: ç›®æ ‡æˆåŠŸç‡ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´ï¼‰
            history_window: å†å²è®°å½•çª—å£å¤§å°
        """
        self.min_size = max(5, min_size)  # è‡³å°‘5
        self.max_size = min(50, max_size)  # æœ€å¤š50
        self.target_success_rate = max(0.5, min(0.95, target_success_rate))
        self.history_window = history_window
        
        # å½“å‰æ‰¹å¤§å°ï¼ˆåˆå§‹ä¸ºä¸­é—´å€¼ï¼‰
        self.current_batch_size = (self.min_size + self.max_size) // 2
        
        # æ‰¹å¤„ç†å†å²è®°å½•
        self._batch_history: List[BatchHistoryEntry] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'total_batches': 0,
            'successful_batches': 0,
            'failed_batches': 0,
            'total_items_processed': 0,
            'avg_batch_size': 0.0,
            'similarity_groups_used': 0
        }
        
        logger.info(
            f"AdaptiveBatchProcessor åˆå§‹åŒ–: "
            f"min={self.min_size}, max={self.max_size}, "
            f"target_success={self.target_success_rate:.2f}"
        )
    
    def create_optimized_batches(
        self,
        items: List[NewsItem],
        context: Optional[BatchContext] = None
    ) -> List[List[NewsItem]]:
        """
        åˆ›å»ºä¼˜åŒ–çš„æ‰¹æ¬¡åˆ—è¡¨
        
        Args:
            items: æ–°é—»é¡¹åˆ—è¡¨
            context: æ‰¹å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            List[List[NewsItem]]: ä¼˜åŒ–åçš„æ‰¹æ¬¡åˆ—è¡¨
        """
        if not items:
            logger.debug("ç©ºè¾“å…¥ï¼Œè¿”å›ç©ºæ‰¹æ¬¡åˆ—è¡¨")
            return []
        
        if context is None:
            context = BatchContext(total_items=len(items))
        else:
            context.total_items = len(items)
        
        logger.info(
            f"ğŸ¯ åˆ›å»ºä¼˜åŒ–æ‰¹æ¬¡: {len(items)} æ¡æ–°é—» "
            f"(Pass {context.pass_number}, å½“å‰æ‰¹å¤§å°: {self.current_batch_size})"
        )
        
        # 1. ä¼˜å…ˆçº§æ’åº
        sorted_items = self._priority_sort(items, context)
        
        # 2. æ™ºèƒ½åˆ†ç»„
        batches = self._intelligent_grouping(
            sorted_items,
            target_size=self.current_batch_size,
            context=context
        )
        
        # 3. æ›´æ–°ç»Ÿè®¡
        self._stats['total_batches'] += len(batches)
        self._stats['total_items_processed'] += len(items)
        if len(batches) > 0:
            self._stats['avg_batch_size'] = (
                self._stats['avg_batch_size'] * 
                (self._stats['total_batches'] - len(batches)) +
                sum(len(batch) for batch in batches) / len(batches)
            ) / self._stats['total_batches']
        
        logger.info(
            f"âœ… åˆ›å»ºæ‰¹æ¬¡å®Œæˆ: {len(batches)} ä¸ªæ‰¹æ¬¡ "
            f"(å¹³å‡ {len(items)/len(batches):.1f} æ¡/æ‰¹æ¬¡)"
        )
        
        return batches
    
    def _priority_sort(
        self,
        items: List[NewsItem],
        context: BatchContext
    ) -> List[NewsItem]:
        """
        åŸºäºä¼˜å…ˆçº§å¯¹æ–°é—»é¡¹æ’åº
        
        Args:
            items: æ–°é—»é¡¹åˆ—è¡¨
            context: æ‰¹å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            List[NewsItem]: æ’åºåçš„æ–°é—»é¡¹åˆ—è¡¨
        """
        if context.priority_mode == "priority":
            # é«˜ä¼˜å…ˆçº§é¡¹ç›®ä¼˜å…ˆï¼šåŸºäºè¯„åˆ†å’Œç´§æ€¥æ€§
            def priority_score(item: NewsItem) -> float:
                score = 0.0
                
                # å·²æœ‰è¯„åˆ†çš„é¡¹ç›®ä¼˜å…ˆ
                if item.ai_score is not None:
                    score += item.ai_score
                
                # ç´§æ€¥å…³é”®è¯æå‡ä¼˜å…ˆçº§
                urgent_keywords = ['breaking', 'ç´§æ€¥', 'çªå‘', 'breaking news']
                if any(kw in item.title.lower() for kw in urgent_keywords):
                    score += 5.0
                
                # çŸ¥ååª’ä½“æºæå‡ä¼˜å…ˆçº§
                high_priority_sources = ['reuters', 'bloomberg', 'wsj', 'ft', 'æ–°å', 'äººæ°‘ç½‘']
                if any(src in item.source.lower() for src in high_priority_sources):
                    score += 2.0
                
                return score
            
            sorted_items = sorted(items, key=priority_score, reverse=True)
            logger.debug("ğŸ”¥ ä½¿ç”¨ä¼˜å…ˆçº§æ’åº")
        
        elif context.priority_mode == "similarity":
            # ç›¸ä¼¼åº¦åˆ†ç»„æ¨¡å¼ï¼šå…ˆæŒ‰åˆ†ç±»åˆ†ç»„
            category_groups = defaultdict(list)
            for item in items:
                cat = getattr(item, 'pre_category', item.category)
                category_groups[cat].append(item)
            
            # åœ¨æ¯ä¸ªåˆ†ç±»å†…æŒ‰æ—¶é—´æ’åº
            sorted_items = []
            for cat in sorted(category_groups.keys()):
                cat_items = sorted(
                    category_groups[cat],
                    key=lambda x: x.published_at,
                    reverse=True
                )
                sorted_items.extend(cat_items)
            
            logger.debug(f"ğŸ”— ä½¿ç”¨ç›¸ä¼¼åº¦æ’åº: {len(category_groups)} ä¸ªåˆ†ç±»")
        
        else:  # balanced
            # å¹³è¡¡æ¨¡å¼ï¼šæŒ‰æ—¶é—´æ’åºï¼Œä¿æŒæ–°é—»çš„æ—¶é—´é¡ºåº
            sorted_items = sorted(items, key=lambda x: x.published_at, reverse=True)
            logger.debug("âš–ï¸ ä½¿ç”¨å¹³è¡¡æ’åºï¼ˆæ—¶é—´é¡ºåºï¼‰")
        
        return sorted_items
    
    def _intelligent_grouping(
        self,
        items: List[NewsItem],
        target_size: int,
        context: BatchContext
    ) -> List[List[NewsItem]]:
        """
        æ™ºèƒ½åˆ†ç»„ï¼šç»“åˆç›¸ä¼¼åº¦å’Œç›®æ ‡æ‰¹å¤§å°
        
        Args:
            items: æ–°é—»é¡¹åˆ—è¡¨
            target_size: ç›®æ ‡æ‰¹å¤§å°
            context: æ‰¹å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            List[List[NewsItem]]: åˆ†ç»„åçš„æ‰¹æ¬¡åˆ—è¡¨
        """
        batches: List[List[NewsItem]] = []
        
        if not items:
            return batches
        
        # å¦‚æœé¡¹ç›®æ•°é‡å°äºç›®æ ‡æ‰¹å¤§å°ï¼Œç›´æ¥è¿”å›å•æ‰¹æ¬¡
        if len(items) <= target_size:
            batches.append(items)
            return batches
        
        # ç­–ç•¥1: ç›¸ä¼¼åº¦åˆ†ç»„ï¼ˆä¼˜å…ˆï¼‰
        if context.priority_mode == "similarity":
            batches = self._similarity_based_grouping(
                items, target_size, context
            )
            self._stats['similarity_groups_used'] += len(batches)
        else:
            # ç­–ç•¥2: æ··åˆåˆ†ç»„ï¼ˆç›¸ä¼¼åº¦ + å¤§å°æ§åˆ¶ï¼‰
            batches = self._hybrid_grouping(items, target_size, context)
        
        # ç¡®ä¿æ‰€æœ‰é¡¹ç›®éƒ½è¢«åˆ†ç»„
        if len(sum(batches, [])) != len(items):
            logger.warning(
                f"åˆ†ç»„å¼‚å¸¸: è¾“å…¥{len(items)}æ¡, è¾“å‡º{len(sum(batches, []))}æ¡"
            )
        
        return batches
    
    def _similarity_based_grouping(
        self,
        items: List[NewsItem],
        target_size: int,
        context: BatchContext
    ) -> List[List[NewsItem]]:
        """
        åŸºäºç›¸ä¼¼åº¦çš„åˆ†ç»„
        
        å°†ç›¸ä¼¼æ–°é—»åˆå¹¶åˆ°åŒä¸€æ‰¹æ¬¡ä¸­
        
        Args:
            items: æ–°é—»é¡¹åˆ—è¡¨
            target_size: ç›®æ ‡æ‰¹å¤§å°
            context: æ‰¹å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            List[List[NewsItem]]: åˆ†ç»„åçš„æ‰¹æ¬¡åˆ—è¡¨
        """
        batches: List[List[NewsItem]] = []
        remaining_items = items.copy()
        
        similarity_threshold = 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
        while remaining_items:
            # å–ç¬¬ä¸€ä¸ªæœªåˆ†é…çš„é¡¹ç›®
            current_item = remaining_items[0]
            current_batch = [current_item]
            remaining_items = remaining_items[1:]
            
            # å¯»æ‰¾ç›¸ä¼¼é¡¹ç›®
            i = 0
            while i < len(remaining_items) and len(current_batch) < target_size:
                candidate_item = remaining_items[i]
                similarity = self._calculate_similarity(current_item, candidate_item)
                
                if similarity >= similarity_threshold:
                    current_batch.append(candidate_item)
                    remaining_items.pop(i)
                    # ä¸å¢åŠ iï¼Œå› ä¸ºé¡¹ç›®è¢«ç§»é™¤äº†
                else:
                    i += 1
            
            # å¦‚æœæ‰¹æ¬¡å¤ªå°ï¼Œæ·»åŠ éç›¸ä¼¼é¡¹ç›®ä»¥æ¥è¿‘ç›®æ ‡å¤§å°
            while remaining_items and len(current_batch) < max(self.min_size, target_size // 2):
                current_batch.append(remaining_items.pop(0))
            
            batches.append(current_batch)
        
        logger.debug(
            f"ğŸ”— ç›¸ä¼¼åº¦åˆ†ç»„: {len(batches)} ä¸ªæ‰¹æ¬¡ "
            f"(é˜ˆå€¼: {similarity_threshold})"
        )
        
        return batches
    
    def _hybrid_grouping(
        self,
        items: List[NewsItem],
        target_size: int,
        context: BatchContext
    ) -> List[List[NewsItem]]:
        """
        æ··åˆåˆ†ç»„ç­–ç•¥
        
        ç»“åˆç›¸ä¼¼åº¦åˆ†ç»„å’Œå›ºå®šå¤§å°åˆ†ç»„çš„ä¼˜ç‚¹
        
        Args:
            items: æ–°é—»é¡¹åˆ—è¡¨
            target_size: ç›®æ ‡æ‰¹å¤§å°
            context: æ‰¹å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            List[List[NewsItem]]: åˆ†ç»„åçš„æ‰¹æ¬¡åˆ—è¡¨
        """
        batches: List[List[NewsItem]] = []
        
        # é¦–å…ˆæŒ‰åˆ†ç±»åˆ†ç»„
        category_groups = defaultdict(list)
        for item in items:
            cat = getattr(item, 'pre_category', item.category)
            category_groups[cat].append(item)
        
        # å¯¹æ¯ä¸ªåˆ†ç±»è¿›è¡Œæ™ºèƒ½åˆ†æ‰¹
        for category, cat_items in category_groups.items():
            if len(cat_items) <= target_size:
                # å°åˆ†ç±»ç›´æ¥ä½œä¸ºä¸€ä¸ªæ‰¹æ¬¡
                batches.append(cat_items)
            else:
                # å¤§åˆ†ç±»åˆ†å‰²ä¸ºå¤šä¸ªæ‰¹æ¬¡
                # ç­–ç•¥ï¼šå…ˆå°è¯•ç›¸ä¼¼åº¦åˆ†ç»„ï¼Œå†æŒ‰å¤§å°åˆ†å‰²
                similar_groups = self._find_similarity_clusters(cat_items)
                
                for group in similar_groups:
                    if len(group) <= target_size:
                        batches.append(group)
                    else:
                        # åˆ†å‰²å¤§ç»„
                        for i in range(0, len(group), target_size):
                            batch = group[i:i + target_size]
                            if batch:
                                batches.append(batch)
        
        logger.debug(
            f"ğŸ¯ æ··åˆåˆ†ç»„: {len(batches)} ä¸ªæ‰¹æ¬¡ "
            f"(åˆ†ç±»æ•°: {len(category_groups)})"
        )
        
        return batches
    
    def _find_similarity_clusters(
        self,
        items: List[NewsItem]
    ) -> List[List[NewsItem]]:
        """
        å‘ç°ç›¸ä¼¼åº¦èšç±»
        
        ä½¿ç”¨è´ªå¿ƒç®—æ³•å°†ç›¸ä¼¼é¡¹ç›®èšç±»åœ¨ä¸€èµ·
        
        Args:
            items: æ–°é—»é¡¹åˆ—è¡¨
            
        Returns:
            List[List[NewsItem]]: èšç±»ç»“æœ
        """
        if not items:
            return []
        
        clusters = []
        unassigned = items.copy()
        similarity_threshold = 0.6  # èšç±»ç›¸ä¼¼åº¦é˜ˆå€¼
        
        while unassigned:
            # åˆ›å»ºæ–°èšç±»
            cluster = [unassigned.pop(0)]
            
            # å°è¯•å°†ç›¸ä¼¼é¡¹ç›®åŠ å…¥èšç±»
            changed = True
            while changed and unassigned:
                changed = False
                i = 0
                while i < len(unassigned):
                    # æ£€æŸ¥æ˜¯å¦ä¸èšç±»ä¸­çš„ä»»ä½•é¡¹ç›®ç›¸ä¼¼
                    is_similar = any(
                        self._calculate_similarity(cluster_item, unassigned[i]) >= similarity_threshold
                        for cluster_item in cluster
                    )
                    
                    if is_similar:
                        cluster.append(unassigned.pop(i))
                        changed = True
                    else:
                        i += 1
            
            clusters.append(cluster)
        
        return clusters
    
    def _calculate_similarity(
        self,
        item1: NewsItem,
        item2: NewsItem
    ) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–°é—»é¡¹çš„ç›¸ä¼¼åº¦
        
        åŸºäºå¤šä¸ªç»´åº¦çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼š
        1. æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆå…³é”®è¯é‡å ï¼‰
        2. æ¥æºç›¸ä¼¼åº¦
        3. åˆ†ç±»ç›¸ä¼¼åº¦
        4. æ—¶é—´ç›¸ä¼¼åº¦
        
        Args:
            item1: æ–°é—»é¡¹1
            item2: æ–°é—»é¡¹2
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        similarity_score = 0.0
        
        # 1. æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆæœ€é‡è¦ï¼‰
        title_sim = self._calculate_title_similarity(item1.title, item2.title)
        similarity_score += title_sim * 0.6
        
        # 2. æ¥æºç›¸ä¼¼åº¦
        source_sim = self._calculate_source_similarity(item1.source, item2.source)
        similarity_score += source_sim * 0.2
        
        # 3. åˆ†ç±»ç›¸ä¼¼åº¦
        category_sim = self._calculate_category_similarity(item1, item2)
        similarity_score += category_sim * 0.15
        
        # 4. æ—¶é—´ç›¸ä¼¼åº¦ï¼ˆåŒä¸€æ–°é—»äº‹ä»¶é€šå¸¸åœ¨åŒä¸€æ—¶é—´çª—å£å†…ï¼‰
        time_sim = self._calculate_time_similarity(item1.published_at, item2.published_at)
        similarity_score += time_sim * 0.05
        
        return min(1.0, similarity_score)
    
    def _calculate_title_similarity(self, title1: str, title2: str) -> float:
        """
        è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆåŸºäºå…³é”®è¯é‡å ï¼‰
        
        Args:
            title1: æ ‡é¢˜1
            title2: æ ‡é¢˜2
            
        Returns:
            float: ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶å°å†™
        title1_clean = re.sub(r'[^\w\s]', '', title1.lower())
        title2_clean = re.sub(r'[^\w\s]', '', title2.lower())
        
        # åˆ†è¯
        words1 = set(title1_clean.split())
        words2 = set(title2_clean.split())
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        jaccard = intersection / union if union > 0 else 0.0
        
        # å¯¹å®Œå…¨ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„æ ‡é¢˜ç»™äºˆé¢å¤–åŠ åˆ†
        if title1_clean == title2_clean:
            return 1.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ï¼ˆä¸€ä¸ªæ ‡é¢˜åŒ…å«å¦ä¸€ä¸ªï¼‰
        if title1_clean in title2_clean or title2_clean in title1_clean:
            return 0.9
        
        return jaccard
    
    def _calculate_source_similarity(self, source1: str, source2: str) -> float:
        """
        è®¡ç®—æ¥æºç›¸ä¼¼åº¦
        
        Args:
            source1: æ¥æº1
            source2: æ¥æº2
            
        Returns:
            float: ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        source1_lower = source1.lower()
        source2_lower = source2.lower()
        
        if source1_lower == source2_lower:
            return 1.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒåŸŸå
        if source1_lower in source2_lower or source2_lower in source1_lower:
            return 0.8
        
        return 0.0
    
    def _calculate_category_similarity(self, item1: NewsItem, item2: NewsItem) -> float:
        """
        è®¡ç®—åˆ†ç±»ç›¸ä¼¼åº¦
        
        Args:
            item1: æ–°é—»é¡¹1
            item2: æ–°é—»é¡¹2
            
        Returns:
            float: ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        # ä½¿ç”¨é¢„åˆ†ç±»ï¼ˆå¦‚æœæœ‰ï¼‰æˆ–åŸå§‹åˆ†ç±»
        cat1 = getattr(item1, 'pre_category', item1.category)
        cat2 = getattr(item2, 'pre_category', item2.category)
        
        if cat1 == cat2:
            return 1.0
        
        # æ£€æŸ¥æ˜¯å¦ç›¸å…³åˆ†ç±»
        related_pairs = [
            ('è´¢ç»', 'ç§‘æŠ€'),  # é‡‘èç§‘æŠ€
            ('ç¤¾ä¼šæ”¿æ²»', 'è´¢ç»'),  # æ”¿ç­–å½±å“é‡‘è
            ('ç§‘æŠ€', 'ç¤¾ä¼šæ”¿æ²»'),  # ç§‘æŠ€æ”¿ç­–
        ]
        
        for pair in related_pairs:
            if (cat1 in pair[0] and cat2 in pair[1]) or (cat2 in pair[0] and cat1 in pair[1]):
                return 0.5
        
        return 0.0
    
    def _calculate_time_similarity(
        self,
        time1: datetime,
        time2: datetime
    ) -> float:
        """
        è®¡ç®—æ—¶é—´ç›¸ä¼¼åº¦
        
        Args:
            time1: æ—¶é—´1
            time2: æ—¶é—´2
            
        Returns:
            float: ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        # è®¡ç®—æ—¶é—´å·®ï¼ˆå°æ—¶ï¼‰
        time_diff = abs((time1 - time2).total_seconds()) / 3600
        
        # 24å°æ—¶å†…çš„æ–°é—»å¯èƒ½æ˜¯åŒä¸€äº‹ä»¶
        if time_diff < 24:
            return 1.0 - (time_diff / 24)
        elif time_diff < 72:
            return 0.5 * (1 - (time_diff - 24) / 48)
        else:
            return 0.0
    
    def record_batch_result(
        self,
        batch_size: int,
        success: bool,
        items_processed: int,
        error_type: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ):
        """
        è®°å½•æ‰¹å¤„ç†ç»“æœ
        
        Args:
            batch_size: æ‰¹å¤§å°
            success: æ˜¯å¦æˆåŠŸ
            items_processed: å¤„ç†çš„é¡¹ç›®æ•°
            error_type: é”™è¯¯ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        entry = BatchHistoryEntry(
            batch_size=batch_size,
            success=success,
            items_processed=items_processed,
            timestamp=datetime.now(),
            error_type=error_type,
            context=context or {}
        )
        
        self._batch_history.append(entry)
        
        # æ›´æ–°ç»Ÿè®¡
        self._stats['total_batches'] += 1
        if success:
            self._stats['successful_batches'] += 1
        else:
            self._stats['failed_batches'] += 1
        
        # ä¿æŒå†å²çª—å£å¤§å°
        if len(self._batch_history) > self.history_window:
            self._batch_history.pop(0)
        
        # è§¦å‘åŠ¨æ€è°ƒæ•´
        self._dynamic_batch_size_adjustment()
    
    def _dynamic_batch_size_adjustment(self):
        """
        åŸºäºå†å²è®°å½•åŠ¨æ€è°ƒæ•´æ‰¹å¤§å°
        
        è°ƒæ•´ç­–ç•¥ï¼š
        1. è®¡ç®—æœ€è¿‘çš„æˆåŠŸç‡
        2. å¦‚æœæˆåŠŸç‡é«˜äºç›®æ ‡ï¼Œå¢åŠ æ‰¹å¤§å°
        3. å¦‚æœæˆåŠŸç‡ä½äºç›®æ ‡ï¼Œå‡å°‘æ‰¹å¤§å°
        4. è°ƒæ•´å¹…åº¦åŸºäºåå·®ç¨‹åº¦
        """
        if len(self._batch_history) < 10:
            # å†å²è®°å½•ä¸è¶³ï¼Œä¸è°ƒæ•´
            return
        
        # è®¡ç®—æœ€è¿‘çš„æˆåŠŸç‡
        recent_history = self._batch_history[-20:]  # æœ€è¿‘20æ¬¡
        success_count = sum(1 for entry in recent_history if entry.success)
        current_success_rate = success_count / len(recent_history)
        
        old_batch_size = self.current_batch_size
        
        if current_success_rate > self.target_success_rate:
            # æˆåŠŸç‡é«˜ï¼Œå¯ä»¥å¢åŠ æ‰¹å¤§å°
            adjustment_factor = min(
                0.2,
                (current_success_rate - self.target_success_rate) * 2
            )
            new_batch_size = int(
                self.current_batch_size * (1 + adjustment_factor)
            )
            self.current_batch_size = min(new_batch_size, self.max_size)
        
        elif current_success_rate < self.target_success_rate:
            # æˆåŠŸç‡ä½ï¼Œå‡å°‘æ‰¹å¤§å°
            adjustment_factor = min(
            0.3,
                (self.target_success_rate - current_success_rate) * 3
            )
            new_batch_size = int(
                self.current_batch_size * (1 - adjustment_factor)
            )
            self.current_batch_size = max(new_batch_size, self.min_size)
        
        if self.current_batch_size != old_batch_size:
            logger.info(
                f"ğŸ“Š æ‰¹å¤§å°åŠ¨æ€è°ƒæ•´: {old_batch_size} -> {self.current_batch_size} "
                f"(æˆåŠŸç‡: {current_success_rate:.2%}, ç›®æ ‡: {self.target_success_rate:.2%})"
            )
    
    def get_current_batch_size(self) -> int:
        """
        è·å–å½“å‰æ‰¹å¤§å°
        
        Returns:
            int: å½“å‰æ‰¹å¤§å°
        """
        return self.current_batch_size
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # è®¡ç®—å½“å‰æˆåŠŸç‡
        if self._batch_history:
            recent_history = self._batch_history[-20:]
            success_count = sum(1 for entry in recent_history if entry.success)
            current_success_rate = success_count / len(recent_history)
        else:
            current_success_rate = 0.0
        
        return {
            'current_batch_size': self.current_batch_size,
            'min_size': self.min_size,
            'max_size': self.max_size,
            'target_success_rate': self.target_success_rate,
            'current_success_rate': current_success_rate,
            'total_batches': self._stats['total_batches'],
            'successful_batches': self._stats['successful_batches'],
            'failed_batches': self._stats['failed_batches'],
            'total_items_processed': self._stats['total_items_processed'],
            'avg_batch_size': self._stats['avg_batch_size'],
            'similarity_groups_used': self._stats['similarity_groups_used'],
            'history_size': len(self._batch_history)
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self._batch_history.clear()
        self._stats = {
            'total_batches': 0,
            'successful_batches': 0,
            'failed_batches': 0,
            'total_items_processed': 0,
            'avg_batch_size': 0.0,
            'similarity_groups_used': 0
        }
        self.current_batch_size = (self.min_size + self.max_size) // 2
        logger.info("ğŸ”„ ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
