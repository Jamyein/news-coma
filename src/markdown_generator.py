"""
Markdownç”Ÿæˆæ¨¡å—
è´Ÿè´£ç”Ÿæˆç»“æ„åŒ–Markdownæ–‡æ¡£
"""
import logging
from datetime import datetime
from typing import List, Tuple, Dict
from pathlib import Path

from src.models import NewsItem

logger = logging.getLogger(__name__)


class MarkdownGenerator:
    """Markdownç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "docs", archive_dir: str = "archive"):
        self.output_dir = Path(output_dir)
        self.archive_dir = Path(archive_dir)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.archive_dir.mkdir(parents=True, exist_ok=True)
    
    def generate(self, items: List[NewsItem], timestamp: datetime) -> Tuple[str, str]:
        """
        ç”ŸæˆMarkdownæ–‡ä»¶
        
        Args:
            items: æ–°é—»åˆ—è¡¨(å·²æ’åº)
            timestamp: ç”Ÿæˆæ—¶é—´æˆ³
            
        Returns:
            (latest_path, archive_path)
        """
        content = self._build_content(items, timestamp)
        
        # æ›´æ–°latest.md
        latest_path = self.output_dir / "latest.md"
        self._write_file(latest_path, content)
        logger.info(f"å·²æ›´æ–°: {latest_path}")
        
        # åˆ›å»ºå½’æ¡£
        archive_filename = timestamp.strftime("%Y-%m-%d") + ".md"
        archive_path = self.archive_dir / archive_filename
        
        # å¦‚æœå½’æ¡£æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ åˆ°ç°æœ‰å†…å®¹
        if archive_path.exists():
            existing_content = archive_path.read_text(encoding='utf-8')
            # åˆå¹¶å†…å®¹(å»é‡)
            content = self._merge_archive_content(existing_content, content)
        
        self._write_file(archive_path, content)
        logger.info(f"å·²å½’æ¡£: {archive_path}")
        
        return str(latest_path), str(archive_path)
    
    def _build_content(self, items: List[NewsItem], timestamp: datetime) -> str:
        """æ„å»ºMarkdownå†…å®¹ï¼ˆä¸‰æ¿å—åˆ†åŒºå¸ƒå±€ï¼‰"""
        from datetime import timedelta
        beijing_time = timestamp + timedelta(hours=8)

        # æŒ‰ ai_category åˆ†ç»„
        finance_items = [item for item in items if item.ai_category == "è´¢ç»"]
        tech_items = [item for item in items if item.ai_category == "ç§‘æŠ€"]
        politics_items = [item for item in items if item.ai_category == "ç¤¾ä¼šæ”¿æ²»"]

        # è®¡ç®—å„æ¿å—ç²¾é€‰æ•°é‡
        total_count = len(items)

        header = f"""# ğŸ“° æ–°é—»ç²¾é€‰

> ğŸ• æ›´æ–°æ—¶é—´: {beijing_time.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")}
> ğŸ“Š æœ¬æœŸç²¾é€‰ **{total_count}** æ¡é«˜è´¨é‡æ–°é—»
> ğŸ¤– ç”± AI è‡ªåŠ¨åˆ†ç±»ã€ç­›é€‰ã€ç¿»è¯‘å’Œæ€»ç»“

---

"""

        # æ„å»ºä¸‰æ¿å—å†…å®¹
        body = ""

        # è´¢ç»æ¿å—
        body += self._build_section("ğŸ’° è´¢ç»æ–°é—»", finance_items, "è´¢ç»")

        # ç§‘æŠ€æ¿å—
        body += self._build_section("ğŸ”¬ ç§‘æŠ€æ–°é—»", tech_items, "ç§‘æŠ€")

        # ç¤¾ä¼šæ”¿æ²»æ¿å—
        body += self._build_section("ğŸ›ï¸ ç¤¾ä¼šæ”¿æ²»", politics_items, "ç¤¾ä¼šæ”¿æ²»")

        # é¡µè„š
        footer = """## ğŸ“® è®¢é˜…

- **RSSè®¢é˜…**: [feed.xml](https://{username}.github.io/{repo}/feed.xml)

---

*æœ¬é¡¹ç›®è‡ªåŠ¨èšåˆæ–°é—»ï¼Œç”±AIæ™ºèƒ½åˆ†ç±»ç­›é€‰æœ€æœ‰ä»·å€¼çš„å†…å®¹*
"""

        return header + body + footer

    def _build_section(self, title: str, items: List[NewsItem], category: str) -> str:
        """æ„å»ºå•ä¸ªæ¿å—çš„å†…å®¹"""
        if not items:
            return f"""## {title} (0æ¡)

*æš‚æ— {category}æ¿å—æ–°é—»*

---

"""

        # æŒ‰AIè¯„åˆ†æ’åº
        sorted_items = sorted(items, key=lambda x: (x.ai_score or 0, x.published_at), reverse=True)

        section = f"""## {title} ({len(items)}æ¡)

ç²¾é€‰ **{len(sorted_items)}** æ¡{category}æ–°é—»

"""

        for i, item in enumerate(sorted_items, 1):
            key_points_str = "\n".join([f"- {point}" for point in (item.key_points or ["æš‚æ— è¦ç‚¹"])])

            section += f"""### {i}. {item.translated_title or item.title}

**ğŸ“Œ æ¥æº**: {item.source} | **ğŸï¸ AIåˆ†ç±»**: {item.ai_category} | **â­ è¯„åˆ†**: {item.ai_score or 'N/A'}/10

**ğŸ“ æ‘˜è¦**:
{item.ai_summary or 'æš‚æ— æ‘˜è¦'}

**ğŸ’¡ å…³é”®è¦ç‚¹**:
{key_points_str}

**ğŸ”— åŸæ–‡é“¾æ¥**: [{item.title}]({item.link})

---

"""

        return section
    

    def _merge_archive_content(self, existing: str, new: str) -> str:
        """
        åˆå¹¶å½’æ¡£å†…å®¹ï¼ŒåŸºäºé“¾æ¥å»é‡
        
        è§£æç°æœ‰å’Œæ–°å†…å®¹ä¸­çš„æ–°é—»æ¡ç›®ï¼ŒåŸºäºé“¾æ¥URLå»é‡ï¼Œ
        åˆå¹¶åé‡æ–°ç¼–å·ï¼Œä¿æŒMarkdownæ ¼å¼
        """
        import re

        # è¾¹ç•Œæƒ…å†µï¼šå†…å®¹ä¸ºç©ºæˆ–ç›¸åŒ
        if not existing:
            return new
        if existing == new:
            return new

        try:
            # è§£ææ¡ç›®ï¼šè¿”å› {url: (title, full_entry_content)}
            def parse_entries(content: str) -> dict:
                entries = {}
                # åŒ¹é…æ¡ç›®ï¼šä» ### N. å¼€å§‹åˆ° --- ç»“æŸ
                # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ª ### æˆ–æ–‡ä»¶ç»“æŸ
                entry_pattern = r'###\s+\d+\.\s+(.*?)(?=###\s+\d+\.\s+|\Z)'
                # é“¾æ¥æ¨¡å¼ï¼š**ğŸ”— åŸæ–‡é“¾æ¥**: [æ ‡é¢˜](URL)
                link_pattern = r'\*\*ğŸ”— åŸæ–‡é“¾æ¥\*\*:\s*\[.*?\]\((.+?)\)'

                for match in re.finditer(entry_pattern, content, re.DOTALL):
                    entry_content = match.group(0)
                    # æå–é“¾æ¥
                    link_match = re.search(link_pattern, entry_content)
                    if link_match:
                        url = link_match.group(1)
                        # æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œï¼‰
                        title_match = re.match(r'###\s+\d+\.\s+(.+?)\n', entry_content)
                        title = title_match.group(1) if title_match else ""
                        entries[url] = (title, entry_content)
                return entries

            # æå–headerï¼ˆç¬¬ä¸€ä¸ª ### ä¹‹å‰çš„å†…å®¹ï¼‰
            def extract_header(content: str) -> str:
                first_entry_match = re.search(r'###\s+\d+\.', content)
                if first_entry_match:
                    return content[:first_entry_match.start()]
                return ""

            # æå–footerï¼ˆæœ€åä¸€ä¸ª --- ä¹‹åçš„å†…å®¹ï¼‰
            def extract_footer(content: str) -> str:
                # æŸ¥æ‰¾è®¢é˜…éƒ¨åˆ†ï¼ˆé€šå¸¸æ˜¯æœ€åä¸€éƒ¨åˆ†ï¼‰
                footer_match = re.search(r'##\s+ğŸ“®\s+è®¢é˜…', content)
                if footer_match:
                    return content[footer_match.start():]
                return ""

            # è§£æç°æœ‰å’Œæ–°å†…å®¹çš„æ¡ç›®
            existing_entries = parse_entries(existing)
            new_entries = parse_entries(new)

            # åˆå¹¶æ¡ç›®ï¼šæ–°æ¡ç›®è¦†ç›–æˆ–è¿½åŠ ï¼ˆä¿ç•™æœ€æ–°ï¼‰
            merged_entries = {**existing_entries, **new_entries}

            # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•æ¡ç›®ï¼Œä½¿ç”¨ä¿å®ˆç­–ç•¥
            if not merged_entries:
                return existing + '\n\n' + new

            # æå–headerå’Œfooterï¼ˆä½¿ç”¨æ–°å†…å®¹çš„headerå’Œfooterï¼‰
            header = extract_header(new)
            footer = extract_footer(new)

            # é‡æ–°ç”Ÿæˆæ¡ç›®å†…å®¹ï¼Œé‡æ–°ç¼–å·
            body_parts = []
            for idx, (url, (title, entry_content)) in enumerate(merged_entries.items(), 1):
                # æ›¿æ¢æ¡ç›®ç¼–å·
                renumbered_entry = re.sub(
                    r'^###\s+\d+\.\s+',
                    f'### {idx}. ',
                    entry_content,
                    count=1
                )
                body_parts.append(renumbered_entry)

            # ç»„è£…æœ€ç»ˆå†…å®¹
            result = header + ''.join(body_parts)
            if footer:
                # ç§»é™¤bodyæœ«å°¾çš„---ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œç„¶åæ·»åŠ footer
                result = result.rstrip()
                if result.endswith('---'):
                    result = result[:-3].rstrip()
                result = result + '\n\n' + footer

            return result

        except Exception as e:
            # è§£æå¤±è´¥ï¼Œä¿å®ˆç­–ç•¥ï¼šç›´æ¥è¿½åŠ 
            logger.warning(f"åˆå¹¶å½’æ¡£å†…å®¹æ—¶è§£æå¤±è´¥ï¼Œä½¿ç”¨ä¿å®ˆè¿½åŠ ç­–ç•¥: {e}")
            return existing + '\n\n---\n\n' + new
    
    def _write_file(self, path: Path, content: str):
        """å†™å…¥æ–‡ä»¶"""
        try:
            path.write_text(content, encoding='utf-8')
        except Exception as e:
            logger.error(f"å†™å…¥æ–‡ä»¶å¤±è´¥ {path}: {e}")
            raise
