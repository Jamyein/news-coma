"""SmartScorer - 1-Pass AI æ–°é—»è¯„åˆ†æ ¸å¿ƒåè°ƒå™¨"""

import asyncio
import logging
from typing import List, Dict
from datetime import datetime
from collections import defaultdict

from src.models import NewsItem, AIConfig
from src.exceptions import ContentFilterError
from .batch_provider import BatchProvider
from .prompt_engine import PromptEngine
from .result_processor import ResultProcessor

logger = logging.getLogger(__name__)


class SmartScorer:
    """æ™ºèƒ½è¯„åˆ†å™¨ - 1-passå®Œæˆåˆ†ç±»+è¯„åˆ†+ç­›é€‰"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self.batch_provider = BatchProvider(config)
        self.prompt_engine = PromptEngine(config)
        self.result_processor = ResultProcessor(config)
        self._stats = {
            'total_processed': 0,
            'total_api_calls': 0,
            'avg_processing_time': 0.0,
            'success_rate': 1.0
        }
        logger.info(f"SmartScoreråˆå§‹åŒ–å®Œæˆ (batch_size={config.batch_size})")
    
    async def score_news(self, items: List[NewsItem]) -> List[NewsItem]:
        """1-passè¯„åˆ†å…¥å£"""
        if not items:
            return []
        
        start_time = datetime.now()
        logger.info(f"SmartScorerå¼€å§‹å¤„ç† {len(items)} æ¡æ–°é—»")
        
        batches = self._create_batches(items)
        scored_items = await self._process_batches(batches)
        final_items = self._select_top_items(scored_items)
        
        duration = (datetime.now() - start_time).total_seconds()
        self._update_stats(len(items), len(final_items), duration)
        
        logger.info(f"SmartScorerå®Œæˆ: {len(items)} â†’ {len(final_items)} æ¡ ({duration:.1f}s)")
        return final_items
    
    def _create_batches(self, items: List[NewsItem]) -> List[List[NewsItem]]:
        """å°†æ–°é—»åˆ†æ‰¹å¤„ç†"""
        return [
            items[i:i + self.config.batch_size]
            for i in range(0, len(items), self.config.batch_size)
        ]

    async def _process_single_batch(
        self,
        batch: List[NewsItem],
        batch_id: str
    ) -> List[NewsItem]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ˆç”¨äºå¹¶è¡Œï¼‰

        Args:
            batch: æ–°é—»æ‰¹æ¬¡
            batch_id: æ‰¹æ¬¡æ ‡è¯†ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            è¯„åˆ†åçš„æ–°é—»æ‰¹æ¬¡ï¼Œå¤±è´¥æ—¶è¿”å›å¸¦é»˜è®¤åˆ†æ•°çš„æ‰¹æ¬¡
        """
        try:
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_id}: {len(batch)} æ¡æ–°é—»")
            prompt = self.prompt_engine.build_1pass_prompt(batch)

            # ä½¿ç”¨æ”¯æŒfallbackçš„æ–°API
            response = await self.batch_provider.call_batch_api_with_fallback(
                prompt=prompt,
                items=batch,
                prompt_template=None,  # ä¼šä»promptè‡ªåŠ¨æå–
                max_tokens=None,  # ä½¿ç”¨é…ç½®é»˜è®¤å€¼
                temperature=None
            )

            scored_batch = self.result_processor.parse_1pass_response(batch, response)
            logger.info(f"æ‰¹æ¬¡ {batch_id} å¤„ç†å®Œæˆ: {len(scored_batch)} æ¡")
            return scored_batch

        except ContentFilterError as e:
            logger.error(f"æ‰¹æ¬¡ {batch_id} å†…å®¹è¿‡æ»¤ä¸”Gemini fallbackå¤±è´¥: {e}")
            # ä¸ºæ•´ä¸ªæ‰¹æ¬¡èµ‹äºˆé»˜è®¤ä½åˆ†
            for item in batch:
                item.ai_score = self.config.default_score_on_error
                item.ai_category = "ç¤¾ä¼šæ”¿æ²»"
                item.ai_summary = f"å†…å®¹è¿‡æ»¤fallbackå¤±è´¥: {str(e)[:self.config.max_error_message_length]}"
            return batch

        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ {batch_id} å¤„ç†å¤±è´¥: {e}")
            # ä¸ºæ•´ä¸ªæ‰¹æ¬¡èµ‹äºˆé»˜è®¤ä½åˆ†
            for item in batch:
                item.ai_score = self.config.default_score_on_error
                item.ai_category = "ç¤¾ä¼šæ”¿æ²»"
                item.ai_summary = f"å¤„ç†å¤±è´¥: {str(e)[:self.config.max_error_message_length]}"
            return batch

    async def _process_batches(self, batches: List[List[NewsItem]]) -> List[NewsItem]:
        """å¹¶è¡Œæ‰¹é‡å¤„ç†

        ä½¿ç”¨ asyncio.gather() å®ç°çœŸæ­£çš„å¹¶è¡Œå¤„ç†ï¼Œ
        ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é¿å…APIè¿‡è½½ã€‚

        Args:
            batches: æ–°é—»æ‰¹æ¬¡åˆ—è¡¨

        Returns:
            æ‰€æœ‰æ‰¹æ¬¡çš„è¯„åˆ†ç»“æœ
        """
        if not batches:
            return []

        total_batches = len(batches)

        # é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…APIè¿‡è½½ï¼ˆä½¿ç”¨é…ç½®çš„ max_concurrentï¼Œæœ€å¤§5ï¼‰
        max_concurrent = min(getattr(self.config, 'max_concurrent', 3), 5)

        # å¦‚æœåªæœ‰1ä¸ªæ‰¹æ¬¡æˆ–ç¦ç”¨å¹¶è¡Œï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†
        if total_batches == 1 or max_concurrent == 1:
            logger.info(f"ä¸²è¡Œå¤„ç† {total_batches} ä¸ªæ‰¹æ¬¡")
            all_scored = []
            for batch_idx, batch in enumerate(batches, 1):
                batch_id = f"{batch_idx}/{total_batches}"
                scored = await self._process_single_batch(batch, batch_id)
                all_scored.extend(scored)
            logger.info(f"ä¸²è¡Œå¤„ç†å®Œæˆ: å…± {len(all_scored)} æ¡")
            return all_scored

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(max_concurrent)

        async def process_with_semaphore(batch_idx: int, batch: List[NewsItem]) -> List[NewsItem]:
            """å¸¦ä¿¡å·é‡æ§åˆ¶çš„æ‰¹æ¬¡å¤„ç†"""
            async with semaphore:
                batch_id = f"{batch_idx}/{total_batches}"
                return await self._process_single_batch(batch, batch_id)

        logger.info(f"ğŸš€ å¹¶è¡Œå¤„ç† {total_batches} ä¸ªæ‰¹æ¬¡ (å¹¶å‘: {max_concurrent})")

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
        tasks = [
            process_with_semaphore(batch_idx, batch)
            for batch_idx, batch in enumerate(batches, 1)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # åˆå¹¶ç»“æœï¼Œå¤„ç†å¼‚å¸¸
        all_scored = []
        exception_count = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                exception_count += 1
                logger.error(f"âŒ æ‰¹æ¬¡ {i+1}/{total_batches} å¤„ç†å¼‚å¸¸: {result}")
                # ä½¿ç”¨é»˜è®¤åˆ†æ•°ï¼ˆ_process_single_batchå†…éƒ¨å·²ç»å¤„ç†ï¼‰
                all_scored.extend(batches[i])
            else:
                all_scored.extend(result)

        success_count = total_batches - exception_count
        logger.info(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{total_batches} æ‰¹æ¬¡, å¤±è´¥ {exception_count} æ‰¹æ¬¡, å…± {len(all_scored)} æ¡")

        return all_scored

    def _select_top_items(self, items: List[NewsItem]) -> List[NewsItem]:
        """ç­›é€‰Topæ–°é—»ï¼ˆæŒ‰åˆ†æ•°+å¤šæ ·æ€§ï¼‰"""
        sorted_items = sorted(items, key=lambda x: x.ai_score or 0, reverse=True)
        return self._ensure_diversity(sorted_items)
    
    def _ensure_diversity(self, items: List[NewsItem]) -> List[NewsItem]:
        """ç¡®ä¿åˆ†ç±»å¤šæ ·æ€§"""
        if not items:
            return []

        max_items = self.config.max_output_items

        # æŒ‰åˆ†ç±»åˆ†ç»„
        by_category = defaultdict(list)
        for item in items:
            category = getattr(item, 'ai_category', 'æœªåˆ†ç±»')
            by_category[category].append(item)

        # ç­–ç•¥ï¼šæ¯ä¸ªåˆ†ç±»å…ˆå–1æ¡ï¼Œç„¶åè¡¥å……é«˜åˆ†æ–°é—»
        selected = []
        for cat_items in by_category.values():
            if cat_items and len(selected) < max_items:
                selected.append(cat_items[0])

        for item in items:
            if item not in selected and len(selected) < max_items:
                selected.append(item)

        selected.sort(key=lambda x: x.ai_score or 0, reverse=True)
        return selected
    
    def _update_stats(self, input_count: int, output_count: int, duration: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self._stats['total_processed'] += input_count
        provider_stats = self.batch_provider.get_stats()
        self._stats['total_api_calls'] = provider_stats.get('api_call_count', 0)

        if self._stats['total_processed'] > 0:
            current_avg = self._stats['avg_processing_time']
            self._stats['avg_processing_time'] = (
                current_avg * (self._stats['total_processed'] - input_count) + duration
            ) / self._stats['total_processed']

    def get_stats(self) -> Dict:
        return self._stats.copy()

